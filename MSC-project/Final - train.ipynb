{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luyuh\\AppData\\Local\\Temp\\ipykernel_3560\\2959114804.py:6: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "C:\\Users\\luyuh\\Downloads\\Anaconda\\Anaconda\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import MultiIndex, Int64Index\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas.testing as tm\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "from memory_profiler import profile\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import interp\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, roc_curve, auc, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    uss_info = process.memory_full_info()\n",
    "    return uss_info.uss / (1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_1(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        uss_before = process_memory()\n",
    "        start_time   = time.perf_counter()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time    = time.perf_counter()\n",
    "        uss_after = process_memory()\n",
    "        \n",
    "        run_time   = end_time - start_time\n",
    "        run_memory = abs(uss_after - uss_before)\n",
    "        \n",
    "        print(f'Running Memory: {(run_memory):.2f} MB') \n",
    "        print(f'Running Time: {(run_time):.2f} Seconds') \n",
    "        return run_time,run_memory\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_2(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        uss_before = process_memory()\n",
    "        start_time   = time.perf_counter()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        end_time    = time.perf_counter()\n",
    "        uss_after = process_memory()\n",
    "        \n",
    "        run_time   = end_time - start_time\n",
    "        run_memory = abs(uss_after - uss_before)\n",
    "        \n",
    "        print(f'Running Memory: {(run_memory):.2f} MB') \n",
    "        print(f'Running Time: {(run_time):.2f} Seconds') \n",
    "        return result,run_time,run_memory\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\luyuh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\luyuh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\luyuh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\luyuh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\luyuh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_txt(path, list=None):\n",
    "    if list != None:\n",
    "        file = open(path, 'w', encoding='utf-8')\n",
    "        file.write(str(list))\n",
    "        file.close()\n",
    "        return None\n",
    "    else:\n",
    "        file = open(path, 'r', encoding='utf-8')\n",
    "        rdlist = eval(file.read())\n",
    "        file.close()\n",
    "        return rdlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_final_df = pd.read_csv('DS1_final_df.csv')\n",
    "DS2_final_df = pd.read_csv('DS2_final_df.csv')\n",
    "DS3_final_df = pd.read_csv('DS3_final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_corpus = []\n",
    "DS2_corpus = []\n",
    "DS3_corpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_corpus = list_txt('corpus_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_corpus = list_txt('corpus_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS3_corpus = list_txt('corpus_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words + N-gram (N=1,2)&emsp;/&emsp;TF-IDF + N-gram (N=1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile_2\n",
    "def feature_extraction(df,corpus,method,n):\n",
    "    \n",
    "    if method=='bow':\n",
    "        if n==1 or n==2 or n==3 or n==4:\n",
    "            vectorizer = CountVectorizer(max_features = 40, ngram_range=(n,n))\n",
    "            x = vectorizer.fit_transform(corpus).toarray()\n",
    "            y = df.iloc[0:df.shape[0], 1].values\n",
    "            feature_df = pd.DataFrame(x, columns=vectorizer.get_feature_names_out())\n",
    "            return x,y,vectorizer,feature_df\n",
    "        else: \n",
    "            return print('N is wrong!')\n",
    "        \n",
    "    elif method=='tf-idf':\n",
    "        if n==1 or n==2 or n==3 or n==4:\n",
    "            vectorizer = TfidfVectorizer(max_features = 40, ngram_range=(n,n))\n",
    "            x = vectorizer.fit_transform(corpus).toarray()\n",
    "            y = df.iloc[0:df.shape[0], 1].values\n",
    "            feature_df = pd.DataFrame(x, columns=vectorizer.get_feature_names_out())\n",
    "            return x,y,vectorizer,feature_df\n",
    "        else: \n",
    "            return print('N is wrong!')\n",
    "    else: return print('Wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 45.55 MB\n",
      "Running Time: 13.10 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS1_bow_1_res, DS1_bow_1_time, DS1_bow_1_mem = feature_extraction(DS1_final_df,DS1_corpus,'bow',1)\n",
    "\n",
    "DS1_bow_1_x = DS1_bow_1_res[0]\n",
    "DS1_bow_1_y = DS1_bow_1_res[1]\n",
    "DS1_bow_1_vectorizer = DS1_bow_1_res[2]\n",
    "DS1_bow_1_feature_df = DS1_bow_1_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 91.86 MB\n",
      "Running Time: 50.45 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS1_bow_2_res, DS1_bow_2_time, DS1_bow_2_mem = feature_extraction(DS1_final_df,DS1_corpus,'bow',2)\n",
    "\n",
    "DS1_bow_2_x = DS1_bow_2_res[0]\n",
    "DS1_bow_2_y = DS1_bow_2_res[1]\n",
    "DS1_bow_2_vectorizer = DS1_bow_2_res[2]\n",
    "DS1_bow_2_feature_df = DS1_bow_2_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 615.93 MB\n",
      "Running Time: 99.74 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS1_bow_3_res, DS1_bow_3_time, DS1_bow_3_mem = feature_extraction(DS1_final_df,DS1_corpus,'bow',3)\n",
    "\n",
    "DS1_bow_3_x = DS1_bow_3_res[0]\n",
    "DS1_bow_3_y = DS1_bow_3_res[1]\n",
    "DS1_bow_3_vectorizer = DS1_bow_3_res[2]\n",
    "DS1_bow_3_feature_df = DS1_bow_3_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 150.41 MB\n",
      "Running Time: 12.65 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS1_tfidf_1_res, DS1_tfidf_1_time, DS1_tfidf_1_mem = feature_extraction(DS1_final_df,DS1_corpus,'tf-idf',1)\n",
    "\n",
    "DS1_tfidf_1_x = DS1_tfidf_1_res[0]\n",
    "DS1_tfidf_1_y = DS1_tfidf_1_res[1]\n",
    "DS1_tfidf_1_vectorizer = DS1_tfidf_1_res[2]\n",
    "DS1_tfidf_1_feature_df = DS1_tfidf_1_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 349.79 MB\n",
      "Running Time: 37.05 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS1_tfidf_2_res, DS1_tfidf_2_time, DS1_tfidf_2_mem = feature_extraction(DS1_final_df,DS1_corpus,'tf-idf',2)\n",
    "\n",
    "DS1_tfidf_2_x = DS1_tfidf_2_res[0]\n",
    "DS1_tfidf_2_y = DS1_tfidf_2_res[1]\n",
    "DS1_tfidf_2_vectorizer = DS1_tfidf_2_res[2]\n",
    "DS1_tfidf_2_feature_df = DS1_tfidf_2_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 335.27 MB\n",
      "Running Time: 69.72 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS1_tfidf_3_res, DS1_tfidf_3_time, DS1_tfidf_3_mem = feature_extraction(DS1_final_df,DS1_corpus,'tf-idf',3)\n",
    "\n",
    "DS1_tfidf_3_x = DS1_tfidf_3_res[0]\n",
    "DS1_tfidf_3_y = DS1_tfidf_3_res[1]\n",
    "DS1_tfidf_3_vectorizer = DS1_tfidf_3_res[2]\n",
    "DS1_tfidf_3_feature_df = DS1_tfidf_3_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_feature_mem  = [DS1_bow_1_mem, DS1_bow_2_mem, DS1_bow_3_mem, DS1_tfidf_1_mem,  DS1_tfidf_2_mem,  DS1_tfidf_3_mem]\n",
    "DS1_feature_time = [DS1_bow_1_time, DS1_bow_2_time, DS1_bow_3_time, DS1_tfidf_1_time, DS1_tfidf_2_time, DS1_tfidf_3_time]\n",
    "\n",
    "this_df_1 = pd.DataFrame()\n",
    "this_df_1['Time (S)']=DS1_feature_time\n",
    "this_df_1['Memory (MB)']=DS1_feature_mem\n",
    "this_df_1.index=['BoW(n=1)','BoW(n=2)','BoW(n=3)','TF-IDF(n=1)','TF-IDF(n=2)','TF-IDF(n=3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (S)</th>\n",
       "      <th>Memory (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BoW(n=1)</th>\n",
       "      <td>13.10</td>\n",
       "      <td>45.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoW(n=2)</th>\n",
       "      <td>50.45</td>\n",
       "      <td>91.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoW(n=3)</th>\n",
       "      <td>99.74</td>\n",
       "      <td>615.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF(n=1)</th>\n",
       "      <td>12.65</td>\n",
       "      <td>150.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF(n=2)</th>\n",
       "      <td>37.05</td>\n",
       "      <td>349.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF(n=3)</th>\n",
       "      <td>69.72</td>\n",
       "      <td>335.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time (S)  Memory (MB)\n",
       "BoW(n=1)        13.10        45.55\n",
       "BoW(n=2)        50.45        91.86\n",
       "BoW(n=3)        99.74       615.93\n",
       "TF-IDF(n=1)     12.65       150.41\n",
       "TF-IDF(n=2)     37.05       349.79\n",
       "TF-IDF(n=3)     69.72       335.27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_df_1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features(df, color, n, a, b, s):\n",
    "    features_name_list  = list(df.columns)\n",
    "    \n",
    "    features_num_list   = []\n",
    "    for i in features_name_list:\n",
    "        features_num_list.append(max(df[i]))\n",
    "        \n",
    "    features_final_dict = {}\n",
    "    for i in range(0,len(features_name_list)):\n",
    "        features_final_dict[features_name_list[i]]=features_num_list[i]\n",
    "\n",
    "    features_final_dict = sorted(features_final_dict.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "    features_final_name_list = []\n",
    "    features_final_num_list = []\n",
    "    \n",
    "    for i in features_final_dict:\n",
    "        features_final_name_list.append(i[0])\n",
    "        features_final_num_list.append(i[1])\n",
    "    \n",
    "    fig = plt.figure(figsize=(a,b))\n",
    "    plt.barh(features_final_name_list[40-n:], features_final_num_list[40-n:], height=0.4,color=sns.color_palette(color,n), edgecolor='black')\n",
    "    plt.xlabel(s,fontsize=20)\n",
    "    plt.ylabel('Feature',fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_features(DS1_bow_1_feature_df, 'Blues', 15, 6, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS1_bow_2_feature_df, 'Blues', 15, 6, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS1_bow_3_feature_df, 'Blues', 15, 3, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS1_tfidf_1_feature_df, 'Oranges', 15, 6, 6, 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS1_tfidf_2_feature_df, 'Oranges', 15, 6, 6, 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS1_tfidf_3_feature_df, 'Oranges', 15, 3, 6, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 28.55 MB\n",
      "Running Time: 26.35 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS2_bow_1_res, DS2_bow_1_time, DS2_bow_1_mem = feature_extraction(DS2_final_df,DS2_corpus,'bow',1)\n",
    "\n",
    "DS2_bow_1_x = DS2_bow_1_res[0]\n",
    "DS2_bow_1_y = DS2_bow_1_res[1]\n",
    "DS2_bow_1_vectorizer = DS2_bow_1_res[2]\n",
    "DS2_bow_1_feature_df = DS2_bow_1_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 498.06 MB\n",
      "Running Time: 39.32 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS2_bow_2_res, DS2_bow_2_time, DS2_bow_2_mem = feature_extraction(DS2_final_df,DS2_corpus,'bow',2)\n",
    "\n",
    "DS2_bow_2_x = DS2_bow_2_res[0]\n",
    "DS2_bow_2_y = DS2_bow_2_res[1]\n",
    "DS2_bow_2_vectorizer = DS2_bow_2_res[2]\n",
    "DS2_bow_2_feature_df = DS2_bow_2_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 504.11 MB\n",
      "Running Time: 72.01 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS2_bow_3_res, DS2_bow_3_time, DS2_bow_3_mem = feature_extraction(DS2_final_df,DS2_corpus,'bow',3)\n",
    "\n",
    "DS2_bow_3_x = DS2_bow_3_res[0]\n",
    "DS2_bow_3_y = DS2_bow_3_res[1]\n",
    "DS2_bow_3_vectorizer = DS2_bow_3_res[2]\n",
    "DS2_bow_3_feature_df = DS2_bow_3_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 158.53 MB\n",
      "Running Time: 12.00 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS2_tfidf_1_res, DS2_tfidf_1_time, DS2_tfidf_1_mem = feature_extraction(DS2_final_df,DS2_corpus,'tf-idf',1)\n",
    "\n",
    "DS2_tfidf_1_x = DS2_tfidf_1_res[0]\n",
    "DS2_tfidf_1_y = DS2_tfidf_1_res[1]\n",
    "DS2_tfidf_1_vectorizer = DS2_tfidf_1_res[2]\n",
    "DS2_tfidf_1_feature_df = DS2_tfidf_1_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 156.34 MB\n",
      "Running Time: 42.45 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS2_tfidf_2_res, DS2_tfidf_2_time, DS2_tfidf_2_mem = feature_extraction(DS2_final_df,DS2_corpus,'tf-idf',2)\n",
    "\n",
    "DS2_tfidf_2_x = DS2_tfidf_2_res[0]\n",
    "DS2_tfidf_2_y = DS2_tfidf_2_res[1]\n",
    "DS2_tfidf_2_vectorizer = DS2_tfidf_2_res[2]\n",
    "DS2_tfidf_2_feature_df = DS2_tfidf_2_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 800.36 MB\n",
      "Running Time: 77.08 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS2_tfidf_3_res, DS2_tfidf_3_time, DS2_tfidf_3_mem = feature_extraction(DS2_final_df,DS2_corpus,'tf-idf',3)\n",
    "\n",
    "DS2_tfidf_3_x = DS2_tfidf_3_res[0]\n",
    "DS2_tfidf_3_y = DS2_tfidf_3_res[1]\n",
    "DS2_tfidf_3_vectorizer = DS2_tfidf_3_res[2]\n",
    "DS2_tfidf_3_feature_df = DS2_tfidf_3_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_feature_mem  = [DS2_bow_1_mem, DS2_bow_2_mem, DS2_bow_3_mem, DS2_tfidf_1_mem,  DS2_tfidf_2_mem,  DS2_tfidf_3_mem]\n",
    "DS2_feature_time = [DS2_bow_1_time, DS2_bow_2_time, DS2_bow_3_time, DS2_tfidf_1_time, DS2_tfidf_2_time, DS2_tfidf_3_time]\n",
    "\n",
    "this_df_2 = pd.DataFrame()\n",
    "this_df_2['Time (S)']    = DS2_feature_time\n",
    "this_df_2['Memory (MB)'] = DS2_feature_mem\n",
    "this_df_2.index = ['BoW(n=1)','BoW(n=2)','BoW(n=3)','TF-IDF(n=1)','TF-IDF(n=2)','TF-IDF(n=3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_features(DS2_bow_1_feature_df, 'Blues', 15, 6, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS2_bow_2_feature_df, 'Blues', 15, 6, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS2_bow_3_feature_df, 'Blues', 15, 3, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS2_tfidf_1_feature_df, 'Oranges', 15, 6, 6, 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS2_tfidf_2_feature_df, 'Oranges', 15, 6, 6, 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS2_tfidf_3_feature_df, 'Oranges', 15, 3, 6, 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 7.24 MB\n",
      "Running Time: 2.98 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS3_bow_1_res, DS3_bow_1_time, DS3_bow_1_mem = feature_extraction(DS3_final_df,DS3_corpus,'bow',1)\n",
    "\n",
    "DS3_bow_1_x = DS3_bow_1_res[0]\n",
    "DS3_bow_1_y = DS3_bow_1_res[1]\n",
    "DS3_bow_1_vectorizer = DS3_bow_1_res[2]\n",
    "DS3_bow_1_feature_df = DS3_bow_1_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 88.00 MB\n",
      "Running Time: 7.58 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS3_bow_2_res, DS3_bow_2_time, DS3_bow_2_mem = feature_extraction(DS3_final_df,DS3_corpus,'bow',2)\n",
    "\n",
    "DS3_bow_2_x = DS3_bow_2_res[0]\n",
    "DS3_bow_2_y = DS3_bow_2_res[1]\n",
    "DS3_bow_2_vectorizer = DS3_bow_2_res[2]\n",
    "DS3_bow_2_feature_df = DS3_bow_2_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 140.24 MB\n",
      "Running Time: 9.06 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS3_bow_3_res, DS3_bow_3_time, DS3_bow_3_mem = feature_extraction(DS3_final_df,DS3_corpus,'bow',3)\n",
    "\n",
    "DS3_bow_3_x = DS3_bow_3_res[0]\n",
    "DS3_bow_3_y = DS3_bow_3_res[1]\n",
    "DS3_bow_3_vectorizer = DS3_bow_3_res[2]\n",
    "DS3_bow_3_feature_df = DS3_bow_3_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 4.48 MB\n",
      "Running Time: 2.54 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS3_tfidf_1_res, DS3_tfidf_1_time, DS3_tfidf_1_mem = feature_extraction(DS3_final_df,DS3_corpus,'tf-idf',1)\n",
    "\n",
    "DS3_tfidf_1_x = DS3_tfidf_1_res[0]\n",
    "DS3_tfidf_1_y = DS3_tfidf_1_res[1]\n",
    "DS3_tfidf_1_vectorizer = DS3_tfidf_1_res[2]\n",
    "DS3_tfidf_1_feature_df = DS3_tfidf_1_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 67.93 MB\n",
      "Running Time: 7.55 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS3_tfidf_2_res, DS3_tfidf_2_time, DS3_tfidf_2_mem = feature_extraction(DS3_final_df,DS3_corpus,'tf-idf',2)\n",
    "\n",
    "DS3_tfidf_2_x = DS3_tfidf_2_res[0]\n",
    "DS3_tfidf_2_y = DS3_tfidf_2_res[1]\n",
    "DS3_tfidf_2_vectorizer = DS3_tfidf_2_res[2]\n",
    "DS3_tfidf_2_feature_df = DS3_tfidf_2_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Memory: 138.73 MB\n",
      "Running Time: 8.97 Seconds\n"
     ]
    }
   ],
   "source": [
    "DS3_tfidf_3_res, DS3_tfidf_3_time, DS3_tfidf_3_mem = feature_extraction(DS3_final_df,DS3_corpus,'tf-idf',3)\n",
    "\n",
    "DS3_tfidf_3_x = DS3_tfidf_3_res[0]\n",
    "DS3_tfidf_3_y = DS3_tfidf_3_res[1]\n",
    "DS3_tfidf_3_vectorizer = DS3_tfidf_3_res[2]\n",
    "DS3_tfidf_3_feature_df = DS3_tfidf_3_res[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (S)</th>\n",
       "      <th>Memory (MB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BoW(n=1)</th>\n",
       "      <td>2.58</td>\n",
       "      <td>8.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoW(n=2)</th>\n",
       "      <td>7.58</td>\n",
       "      <td>88.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BoW(n=3)</th>\n",
       "      <td>9.06</td>\n",
       "      <td>140.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF(n=1)</th>\n",
       "      <td>2.54</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF(n=2)</th>\n",
       "      <td>7.55</td>\n",
       "      <td>67.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TF-IDF(n=3)</th>\n",
       "      <td>8.97</td>\n",
       "      <td>138.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Time (S)  Memory (MB)\n",
       "BoW(n=1)         2.58         8.73\n",
       "BoW(n=2)         7.58        88.00\n",
       "BoW(n=3)         9.06       140.24\n",
       "TF-IDF(n=1)      2.54         4.48\n",
       "TF-IDF(n=2)      7.55        67.93\n",
       "TF-IDF(n=3)      8.97       138.73"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS3_feature_mem  = [DS3_bow_1_mem, DS3_bow_2_mem, DS3_bow_3_mem, DS3_tfidf_1_mem,  DS3_tfidf_2_mem,  DS3_tfidf_3_mem]\n",
    "DS3_feature_time = [DS3_bow_1_time, DS3_bow_2_time, DS3_bow_3_time, DS3_tfidf_1_time, DS3_tfidf_2_time, DS3_tfidf_3_time]\n",
    "\n",
    "this_df_3 = pd.DataFrame()\n",
    "this_df_3['Time (S)'] = DS3_feature_time\n",
    "this_df_3['Memory (MB)'] = DS3_feature_mem\n",
    "this_df_3.index = ['BoW(n=1)','BoW(n=2)','BoW(n=3)','TF-IDF(n=1)','TF-IDF(n=2)','TF-IDF(n=3)']\n",
    "this_df_3.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_features(DS3_bow_1_feature_df, 'Blues', 15, 6, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS3_bow_2_feature_df, 'Blues', 15, 6, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS3_bow_3_feature_df, 'Blues', 15, 3, 6, 'Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS3_tfidf_1_feature_df, 'Oranges', 15, 6, 6, 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS3_tfidf_2_feature_df, 'Oranges', 15, 6, 6'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_features(DS3_tfidf_3_feature_df, 'Oranges', 15, 3, 6'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 20)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_bow_1_x_train, DS1_bow_1_x_test, DS1_bow_1_y_train, DS1_bow_1_y_test = split(DS1_bow_1_x,DS1_bow_1_y)\n",
    "DS1_bow_2_x_train, DS1_bow_2_x_test, DS1_bow_2_y_train, DS1_bow_2_y_test = split(DS1_bow_2_x,DS1_bow_2_y)\n",
    "DS1_bow_3_x_train, DS1_bow_3_x_test, DS1_bow_3_y_train, DS1_bow_3_y_test = split(DS1_bow_3_x,DS1_bow_3_y)\n",
    "DS1_tfidf_1_x_train, DS1_tfidf_1_x_test, DS1_tfidf_1_y_train, DS1_tfidf_1_y_test = split(DS1_tfidf_1_x,DS1_tfidf_1_y)\n",
    "DS1_tfidf_2_x_train, DS1_tfidf_2_x_test, DS1_tfidf_2_y_train, DS1_tfidf_2_y_test = split(DS1_tfidf_2_x,DS1_tfidf_2_y)\n",
    "DS1_tfidf_3_x_train, DS1_tfidf_3_x_test, DS1_tfidf_3_y_train, DS1_tfidf_3_y_test = split(DS1_tfidf_3_x,DS1_tfidf_3_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_bow_1_x_train, DS2_bow_1_x_test, DS2_bow_1_y_train, DS2_bow_1_y_test = split(DS2_bow_1_x,DS2_bow_1_y)\n",
    "DS2_bow_2_x_train, DS2_bow_2_x_test, DS2_bow_2_y_train, DS2_bow_2_y_test = split(DS2_bow_2_x,DS2_bow_2_y)\n",
    "DS2_bow_3_x_train, DS2_bow_3_x_test, DS2_bow_3_y_train, DS2_bow_3_y_test = split(DS2_bow_3_x,DS2_bow_3_y)\n",
    "DS2_tfidf_1_x_train, DS2_tfidf_1_x_test, DS2_tfidf_1_y_train, DS2_tfidf_1_y_test = split(DS2_tfidf_1_x,DS2_tfidf_1_y)\n",
    "DS2_tfidf_2_x_train, DS2_tfidf_2_x_test, DS2_tfidf_2_y_train, DS2_tfidf_2_y_test = split(DS2_tfidf_2_x,DS2_tfidf_2_y)\n",
    "#DS2_tfidf_3_x_train, DS2_tfidf_3_x_test, DS2_tfidf_3_y_train, DS2_tfidf_3_y_test = split(DS2_tfidf_3_x,DS2_tfidf_3_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_bow_1_x_train, DS2_bow_1_x_test, DS2_bow_1_y_train, DS2_bow_1_y_test = split(DS2_bow_1_x,DS2_bow_1_y)\n",
    "DS2_bow_2_x_train, DS2_bow_2_x_test, DS2_bow_2_y_train, DS2_bow_2_y_test = split(DS2_bow_2_x,DS2_bow_2_y)\n",
    "DS2_bow_3_x_train, DS2_bow_3_x_test, DS2_bow_3_y_train, DS2_bow_3_y_test = split(DS2_bow_3_x,DS2_bow_3_y)\n",
    "DS2_tfidf_1_x_train, DS2_tfidf_1_x_test, DS2_tfidf_1_y_train, DS2_tfidf_1_y_test = split(DS2_tfidf_1_x,DS2_tfidf_1_y)\n",
    "DS2_tfidf_2_x_train, DS2_tfidf_2_x_test, DS2_tfidf_2_y_train, DS2_tfidf_2_y_test = split(DS2_tfidf_2_x,DS2_tfidf_2_y)\n",
    "DS2_tfidf_3_x_train, DS2_tfidf_3_x_test, DS2_tfidf_3_y_train, DS2_tfidf_3_y_test = split(DS2_tfidf_3_x,DS2_tfidf_3_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS3_bow_1_x_train, DS3_bow_1_x_test, DS3_bow_1_y_train, DS3_bow_1_y_test = split(DS3_bow_1_x,DS3_bow_1_y)\n",
    "DS3_bow_2_x_train, DS3_bow_2_x_test, DS3_bow_2_y_train, DS3_bow_2_y_test = split(DS3_bow_2_x,DS3_bow_2_y)\n",
    "DS3_bow_3_x_train, DS3_bow_3_x_test, DS3_bow_3_y_train, DS3_bow_3_y_test = split(DS3_bow_3_x,DS3_bow_3_y)\n",
    "DS3_tfidf_1_x_train, DS3_tfidf_1_x_test, DS3_tfidf_1_y_train, DS3_tfidf_1_y_test = split(DS3_tfidf_1_x,DS3_tfidf_1_y)\n",
    "DS3_tfidf_2_x_train, DS3_tfidf_2_x_test, DS3_tfidf_2_y_train, DS3_tfidf_2_y_test = split(DS3_tfidf_2_x,DS3_tfidf_2_y)\n",
    "DS3_tfidf_3_x_train, DS3_tfidf_3_x_test, DS3_tfidf_3_y_train, DS3_tfidf_3_y_test = split(DS3_tfidf_3_x,DS3_tfidf_3_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKF = StratifiedKFold(n_splits=10, random_state=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile_1\n",
    "def train(x_train, y_train, classifier, SKF):\n",
    "    model  = classifier.fit(x_train, y_train)\n",
    "    scores = cross_val_score(model, x_train, y_train, scoring='accuracy', cv=SKF) \n",
    "    print(\"Train Accuracy: %.4f\" % (np.mean(scores)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, y_test, classifier):\n",
    "    y_pred = classifier.predict(x_test)\n",
    "   \n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    pre = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(\"Test  Accuracy: %.4f\" % acc)\n",
    "    print(\"Test  Presion:  %.4f\" % pre)\n",
    "    print(\"Test  Recall:   %.4f\" % rec)\n",
    "    print(\"Test  F1-score: %.4f\" % f1)\n",
    "    return cm, acc, pre, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_maxtrix(cm, model, method):\n",
    "    new_cm = pd.DataFrame(cm)\n",
    "    ax = sns.heatmap(new_cm,cmap= 'Blues', annot = True, fmt='', annot_kws={\"fontsize\":14})\n",
    "    plt.xlabel(\"Actual\",fontsize=14)\n",
    "    plt.ylabel(\"Predicted\",fontsize=14)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=13)\n",
    "    plt.title('Confusion matrix: '+ model+method,fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1  = ' LR'\n",
    "model_2  = ' LSVM'\n",
    "model_3  = ' MLP'\n",
    "model_4  = ' KNN'\n",
    "model_5  = ' RF'\n",
    "model_6  = ' Voting 1' #(RF, LR, KNN)\n",
    "model_7  = ' Voting 2' #(LR, LSVM, CART)\n",
    "model_8  = ' Bagging'\n",
    "model_9  = ' AdaBoost'\n",
    "model_10 = ' XGBoost'\n",
    "\n",
    "method_1 = ' (BoW) (n=1)'\n",
    "method_2 = ' (BoW) (n=2)'\n",
    "method_3 = ' (BoW) (n=3)'\n",
    "method_4 = ' (TF-IDF) (n=1)'\n",
    "method_5 = ' (TF-IDF) (n=2)'\n",
    "method_6 = ' (TF-IDF) (n=3)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_LR = LogisticRegression(random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_LSVM = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_MLP = MLPClassifier(solver='sgd', activation='relu', alpha=1e-4, hidden_layer_sizes=(50, 50), random_state=20,\n",
    "                    max_iter=50, verbose=True, learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_KNN = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_RF = RandomForestClassifier(random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf  =  RandomForestClassifier(random_state=20)\n",
    "lr  =  LogisticRegression(random_state=20)\n",
    "knn =  KNeighborsClassifier()\n",
    "lsvm =  LinearSVC()\n",
    "cart =  DecisionTreeClassifier(random_state=20)\n",
    "\n",
    "classifier_VOT_1 = VotingClassifier(estimators=[('RF',rf), ('LR',lr), ('KNN',knn)],voting='hard')\n",
    "classifier_VOT_2 = VotingClassifier(estimators=[ ('LR',lr), ('LSVM',lsvm), ('CART',cart)],voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_BAGG = BaggingClassifier(n_estimators=100, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_ADA = AdaBoostClassifier(random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_XG = XGBClassifier(eval_metric=['logloss','auc','error'], random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9922\n",
      "Running Memory: 1.54 MB\n",
      "Running Time: 1.94 Seconds\n",
      "Test  Accuracy: 0.9921\n",
      "Test  Presion:  0.9970\n",
      "Test  Recall:   0.9878\n",
      "Test  F1-score: 0.9924\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_LR_DS1_bow_1, memory_LR_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS1_bow_1, acc_LR_DS1_bow_1, pre_LR_DS1_bow_1, rec_LR_DS1_bow_1, f1_LR_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS1_bow_1, model_1, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8728\n",
      "Running Memory: 27.10 MB\n",
      "Running Time: 4.08 Seconds\n",
      "Test  Accuracy: 0.8679\n",
      "Test  Presion:  0.8399\n",
      "Test  Recall:   0.9196\n",
      "Test  F1-score: 0.8779\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_LR_DS1_bow_2, memory_LR_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS1_bow_2, acc_LR_DS1_bow_2, pre_LR_DS1_bow_2, rec_LR_DS1_bow_2, f1_LR_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS1_bow_2, model_1, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7270\n",
      "Running Memory: 27.02 MB\n",
      "Running Time: 2.93 Seconds\n",
      "Test  Accuracy: 0.7285\n",
      "Test  Presion:  0.9591\n",
      "Test  Recall:   0.4958\n",
      "Test  F1-score: 0.6537\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_LR_DS1_bow_3, memory_LR_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS1_bow_3, acc_LR_DS1_bow_3, pre_LR_DS1_bow_3, rec_LR_DS1_bow_3, f1_LR_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS1_bow_3, model_1, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9807\n",
      "Running Memory: 27.80 MB\n",
      "Running Time: 1.72 Seconds\n",
      "Test  Accuracy: 0.9805\n",
      "Test  Presion:  0.9779\n",
      "Test  Recall:   0.9846\n",
      "Test  F1-score: 0.9812\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_LR_DS1_tfidf_1, memory_LR_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS1_tfidf_1, acc_LR_DS1_tfidf_1, pre_LR_DS1_tfidf_1, rec_LR_DS1_tfidf_1, f1_LR_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS1_tfidf_1, model_1, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8744\n",
      "Running Memory: 27.02 MB\n",
      "Running Time: 3.17 Seconds\n",
      "Test  Accuracy: 0.8706\n",
      "Test  Presion:  0.8574\n",
      "Test  Recall:   0.8992\n",
      "Test  F1-score: 0.8778\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_LR_DS1_tfidf_2, memory_LR_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS1_tfidf_2, acc_LR_DS1_tfidf_2, pre_LR_DS1_tfidf_2, rec_LR_DS1_tfidf_2, f1_LR_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS1_tfidf_2, model_1, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7252\n",
      "Running Memory: 27.00 MB\n",
      "Running Time: 2.69 Seconds\n",
      "Test  Accuracy: 0.7276\n",
      "Test  Presion:  0.9608\n",
      "Test  Recall:   0.4930\n",
      "Test  F1-score: 0.6517\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_LR_DS1_tfidf_3, memory_LR_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS1_tfidf_3, acc_LR_DS1_tfidf_3, pre_LR_DS1_tfidf_3, rec_LR_DS1_tfidf_3, f1_LR_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS1_tfidf_3, model_1, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9926\n",
      "Running Memory: 18.81 MB\n",
      "Running Time: 34.06 Seconds\n",
      "Test  Accuracy: 0.9916\n",
      "Test  Presion:  0.9975\n",
      "Test  Recall:   0.9862\n",
      "Test  F1-score: 0.9918\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS1_bow_1, memory_LSVM_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS1_bow_1, acc_LSVM_DS1_bow_1, pre_LSVM_DS1_bow_1, rec_LSVM_DS1_bow_1, f1_LSVM_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS1_bow_1, model_2, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8716\n",
      "Running Memory: 9.88 MB\n",
      "Running Time: 9.22 Seconds\n",
      "Test  Accuracy: 0.8673\n",
      "Test  Presion:  0.8379\n",
      "Test  Recall:   0.9214\n",
      "Test  F1-score: 0.8777\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS1_bow_2, memory_LSVM_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS1_bow_2, acc_LSVM_DS1_bow_2, pre_LSVM_DS1_bow_2, rec_LSVM_DS1_bow_2, f1_LSVM_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS1_bow_2, model_2, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7272\n",
      "Running Memory: 9.85 MB\n",
      "Running Time: 8.16 Seconds\n",
      "Test  Accuracy: 0.7284\n",
      "Test  Presion:  0.9591\n",
      "Test  Recall:   0.4956\n",
      "Test  F1-score: 0.6535\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS1_bow_3, memory_LSVM_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS1_bow_3, acc_LSVM_DS1_bow_3, pre_LSVM_DS1_bow_3, rec_LSVM_DS1_bow_3, f1_LSVM_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS1_bow_3, model_2, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9880\n",
      "Running Memory: 9.84 MB\n",
      "Running Time: 1.83 Seconds\n",
      "Test  Accuracy: 0.9867\n",
      "Test  Presion:  0.9858\n",
      "Test  Recall:   0.9885\n",
      "Test  F1-score: 0.9872\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS1_tfidf_1, memory_LSVM_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS1_tfidf_1, acc_LSVM_DS1_tfidf_1, pre_LSVM_DS1_tfidf_1, rec_LSVM_DS1_tfidf_1, f1_LSVM_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS1_tfidf_1, model_2, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8753\n",
      "Running Memory: 9.60 MB\n",
      "Running Time: 1.06 Seconds\n",
      "Test  Accuracy: 0.8708\n",
      "Test  Presion:  0.8577\n",
      "Test  Recall:   0.8993\n",
      "Test  F1-score: 0.8780\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS1_tfidf_2, memory_LSVM_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS1_tfidf_2, acc_LSVM_DS1_tfidf_2, pre_LSVM_DS1_tfidf_2, rec_LSVM_DS1_tfidf_2, f1_LSVM_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS1_tfidf_2, model_2, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7243\n",
      "Running Memory: 9.61 MB\n",
      "Running Time: 0.93 Seconds\n",
      "Test  Accuracy: 0.7276\n",
      "Test  Presion:  0.9608\n",
      "Test  Recall:   0.4930\n",
      "Test  F1-score: 0.6517\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS1_tfidf_3, memory_LSVM_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS1_tfidf_3, acc_LSVM_DS1_tfidf_3, pre_LSVM_DS1_tfidf_3, rec_LSVM_DS1_tfidf_3, f1_LSVM_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS1_tfidf_3, model_2, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.13128579\n",
      "Iteration 2, loss = 0.04193800\n",
      "Iteration 3, loss = 0.03898869\n",
      "Iteration 4, loss = 0.03523456\n",
      "Iteration 5, loss = 0.03326478\n",
      "Iteration 6, loss = 0.02735376\n",
      "Iteration 7, loss = 0.02834849\n",
      "Iteration 8, loss = 0.02450371\n",
      "Iteration 9, loss = 0.02537590\n",
      "Iteration 10, loss = 0.02309595\n",
      "Iteration 11, loss = 0.02168359\n",
      "Iteration 12, loss = 0.02108971\n",
      "Iteration 13, loss = 0.02697142\n",
      "Iteration 14, loss = 0.02152942\n",
      "Iteration 15, loss = 0.02231202\n",
      "Iteration 16, loss = 0.01863254\n",
      "Iteration 17, loss = 0.01909768\n",
      "Iteration 18, loss = 0.01670793\n",
      "Iteration 19, loss = 0.01576123\n",
      "Iteration 20, loss = 0.01496746\n",
      "Iteration 21, loss = 0.01463127\n",
      "Iteration 22, loss = 0.01432350\n",
      "Iteration 23, loss = 0.01496416\n",
      "Iteration 24, loss = 0.01792678\n",
      "Iteration 25, loss = 0.02401827\n",
      "Iteration 26, loss = 0.01690089\n",
      "Iteration 27, loss = 0.01430959\n",
      "Iteration 28, loss = 0.01746135\n",
      "Iteration 29, loss = 0.01445215\n",
      "Iteration 30, loss = 0.01639260\n",
      "Iteration 31, loss = 0.01271915\n",
      "Iteration 32, loss = 0.01011564\n",
      "Iteration 33, loss = 0.00948184\n",
      "Iteration 34, loss = 0.00904012\n",
      "Iteration 35, loss = 0.00923182\n",
      "Iteration 36, loss = 0.01246943\n",
      "Iteration 37, loss = 0.01013361\n",
      "Iteration 38, loss = 0.01190043\n",
      "Iteration 39, loss = 0.00977563\n",
      "Iteration 40, loss = 0.01429378\n",
      "Iteration 41, loss = 0.00672986\n",
      "Iteration 42, loss = 0.00560989\n",
      "Iteration 43, loss = 0.00486172\n",
      "Iteration 44, loss = 0.00509903\n",
      "Iteration 45, loss = 0.00818780\n",
      "Iteration 46, loss = 0.00709070\n",
      "Iteration 47, loss = 0.00496287\n",
      "Iteration 48, loss = 0.00467029\n",
      "Iteration 49, loss = 0.00764514\n",
      "Iteration 50, loss = 0.01044821\n",
      "Iteration 1, loss = 0.14447295\n",
      "Iteration 2, loss = 0.04388970\n",
      "Iteration 3, loss = 0.03813617\n",
      "Iteration 4, loss = 0.03494241\n",
      "Iteration 5, loss = 0.03369286\n",
      "Iteration 6, loss = 0.02667453\n",
      "Iteration 7, loss = 0.02754612\n",
      "Iteration 8, loss = 0.02944849\n",
      "Iteration 9, loss = 0.02528206\n",
      "Iteration 10, loss = 0.02337228\n",
      "Iteration 11, loss = 0.02205296\n",
      "Iteration 12, loss = 0.02120928\n",
      "Iteration 13, loss = 0.02456191\n",
      "Iteration 14, loss = 0.01926879\n",
      "Iteration 15, loss = 0.02014885\n",
      "Iteration 16, loss = 0.01803523\n",
      "Iteration 17, loss = 0.01653864\n",
      "Iteration 18, loss = 0.02089741\n",
      "Iteration 19, loss = 0.01975827\n",
      "Iteration 20, loss = 0.01421114\n",
      "Iteration 21, loss = 0.01487064\n",
      "Iteration 22, loss = 0.01863706\n",
      "Iteration 23, loss = 0.01296098\n",
      "Iteration 24, loss = 0.01165038\n",
      "Iteration 25, loss = 0.02159830\n",
      "Iteration 26, loss = 0.01639670\n",
      "Iteration 27, loss = 0.01326134\n",
      "Iteration 28, loss = 0.01291018\n",
      "Iteration 29, loss = 0.01077819\n",
      "Iteration 30, loss = 0.00983765\n",
      "Iteration 31, loss = 0.01164592\n",
      "Iteration 32, loss = 0.01118685\n",
      "Iteration 33, loss = 0.01904620\n",
      "Iteration 34, loss = 0.01086655\n",
      "Iteration 35, loss = 0.00944130\n",
      "Iteration 36, loss = 0.00863001\n",
      "Iteration 37, loss = 0.00792503\n",
      "Iteration 38, loss = 0.00642680\n",
      "Iteration 39, loss = 0.00774813\n",
      "Iteration 40, loss = 0.00860429\n",
      "Iteration 41, loss = 0.00624445\n",
      "Iteration 42, loss = 0.00863603\n",
      "Iteration 43, loss = 0.01137537\n",
      "Iteration 44, loss = 0.01185633\n",
      "Iteration 45, loss = 0.01681321\n",
      "Iteration 46, loss = 0.01208226\n",
      "Iteration 47, loss = 0.00620655\n",
      "Iteration 48, loss = 0.00513152\n",
      "Iteration 49, loss = 0.00491020\n",
      "Iteration 50, loss = 0.00423093\n",
      "Iteration 1, loss = 0.16176463\n",
      "Iteration 2, loss = 0.04691260\n",
      "Iteration 3, loss = 0.04433324\n",
      "Iteration 4, loss = 0.03553479\n",
      "Iteration 5, loss = 0.03173599\n",
      "Iteration 6, loss = 0.03445110\n",
      "Iteration 7, loss = 0.02927918\n",
      "Iteration 8, loss = 0.02595532\n",
      "Iteration 9, loss = 0.02634216\n",
      "Iteration 10, loss = 0.02412299\n",
      "Iteration 11, loss = 0.02294302\n",
      "Iteration 12, loss = 0.03007091\n",
      "Iteration 13, loss = 0.02387032\n",
      "Iteration 14, loss = 0.02210428\n",
      "Iteration 15, loss = 0.02009255\n",
      "Iteration 16, loss = 0.01963490\n",
      "Iteration 17, loss = 0.02530021\n",
      "Iteration 18, loss = 0.02318390\n",
      "Iteration 19, loss = 0.02076221\n",
      "Iteration 20, loss = 0.02556329\n",
      "Iteration 21, loss = 0.01984632\n",
      "Iteration 22, loss = 0.01811212\n",
      "Iteration 23, loss = 0.01878472\n",
      "Iteration 24, loss = 0.01567001\n",
      "Iteration 25, loss = 0.01651210\n",
      "Iteration 26, loss = 0.01446557\n",
      "Iteration 27, loss = 0.01656971\n",
      "Iteration 28, loss = 0.01359261\n",
      "Iteration 29, loss = 0.01282741\n",
      "Iteration 30, loss = 0.01168977\n",
      "Iteration 31, loss = 0.01196433\n",
      "Iteration 32, loss = 0.01134641\n",
      "Iteration 33, loss = 0.01384492\n",
      "Iteration 34, loss = 0.02420643\n",
      "Iteration 35, loss = 0.01627506\n",
      "Iteration 36, loss = 0.01563681\n",
      "Iteration 37, loss = 0.01172237\n",
      "Iteration 38, loss = 0.01325282\n",
      "Iteration 39, loss = 0.00964712\n",
      "Iteration 40, loss = 0.00880221\n",
      "Iteration 41, loss = 0.00927796\n",
      "Iteration 42, loss = 0.00999996\n",
      "Iteration 43, loss = 0.01137038\n",
      "Iteration 44, loss = 0.00928972\n",
      "Iteration 45, loss = 0.00737901\n",
      "Iteration 46, loss = 0.00680006\n",
      "Iteration 47, loss = 0.00766187\n",
      "Iteration 48, loss = 0.00918807\n",
      "Iteration 49, loss = 0.00689574\n",
      "Iteration 50, loss = 0.00778822\n",
      "Iteration 1, loss = 0.13940040\n",
      "Iteration 2, loss = 0.04430789\n",
      "Iteration 3, loss = 0.04441707\n",
      "Iteration 4, loss = 0.03529215\n",
      "Iteration 5, loss = 0.03201667\n",
      "Iteration 6, loss = 0.02754384\n",
      "Iteration 7, loss = 0.02950221\n",
      "Iteration 8, loss = 0.02571698\n",
      "Iteration 9, loss = 0.02422024\n",
      "Iteration 10, loss = 0.02391243\n",
      "Iteration 11, loss = 0.02161465\n",
      "Iteration 12, loss = 0.01925868\n",
      "Iteration 13, loss = 0.02267089\n",
      "Iteration 14, loss = 0.02069135\n",
      "Iteration 15, loss = 0.02014336\n",
      "Iteration 16, loss = 0.01881141\n",
      "Iteration 17, loss = 0.01751637\n",
      "Iteration 18, loss = 0.01574438\n",
      "Iteration 19, loss = 0.01740004\n",
      "Iteration 20, loss = 0.01476861\n",
      "Iteration 21, loss = 0.01788122\n",
      "Iteration 22, loss = 0.01803414\n",
      "Iteration 23, loss = 0.01365768\n",
      "Iteration 24, loss = 0.01484939\n",
      "Iteration 25, loss = 0.01351084\n",
      "Iteration 26, loss = 0.01372174\n",
      "Iteration 27, loss = 0.01217044\n",
      "Iteration 28, loss = 0.01592558\n",
      "Iteration 29, loss = 0.01367556\n",
      "Iteration 30, loss = 0.01197863\n",
      "Iteration 31, loss = 0.01590464\n",
      "Iteration 32, loss = 0.02191976\n",
      "Iteration 33, loss = 0.01458311\n",
      "Iteration 34, loss = 0.01001512\n",
      "Iteration 35, loss = 0.01091406\n",
      "Iteration 36, loss = 0.00915513\n",
      "Iteration 37, loss = 0.00993637\n",
      "Iteration 38, loss = 0.00984738\n",
      "Iteration 39, loss = 0.00897062\n",
      "Iteration 40, loss = 0.00777007\n",
      "Iteration 41, loss = 0.01425912\n",
      "Iteration 42, loss = 0.01427129\n",
      "Iteration 43, loss = 0.00942734\n",
      "Iteration 44, loss = 0.00608887\n",
      "Iteration 45, loss = 0.01049663\n",
      "Iteration 46, loss = 0.00601514\n",
      "Iteration 47, loss = 0.00998141\n",
      "Iteration 48, loss = 0.00926132\n",
      "Iteration 49, loss = 0.00654098\n",
      "Iteration 50, loss = 0.00604339\n",
      "Iteration 1, loss = 0.18048536\n",
      "Iteration 2, loss = 0.04524352\n",
      "Iteration 3, loss = 0.03917960\n",
      "Iteration 4, loss = 0.03583339\n",
      "Iteration 5, loss = 0.03422941\n",
      "Iteration 6, loss = 0.03279771\n",
      "Iteration 7, loss = 0.02771958\n",
      "Iteration 8, loss = 0.02970589\n",
      "Iteration 9, loss = 0.02764223\n",
      "Iteration 10, loss = 0.02556959\n",
      "Iteration 11, loss = 0.02533328\n",
      "Iteration 12, loss = 0.02557737\n",
      "Iteration 13, loss = 0.02307433\n",
      "Iteration 14, loss = 0.02370133\n",
      "Iteration 15, loss = 0.02157835\n",
      "Iteration 16, loss = 0.01911288\n",
      "Iteration 17, loss = 0.01941810\n",
      "Iteration 18, loss = 0.02080124\n",
      "Iteration 19, loss = 0.01909163\n",
      "Iteration 20, loss = 0.01629353\n",
      "Iteration 21, loss = 0.01720559\n",
      "Iteration 22, loss = 0.01983568\n",
      "Iteration 23, loss = 0.02263082\n",
      "Iteration 24, loss = 0.01778409\n",
      "Iteration 25, loss = 0.01631718\n",
      "Iteration 26, loss = 0.01416697\n",
      "Iteration 27, loss = 0.01358271\n",
      "Iteration 28, loss = 0.01816971\n",
      "Iteration 29, loss = 0.01382914\n",
      "Iteration 30, loss = 0.01597076\n",
      "Iteration 31, loss = 0.02130175\n",
      "Iteration 32, loss = 0.02370876\n",
      "Iteration 33, loss = 0.01609558\n",
      "Iteration 34, loss = 0.01599883\n",
      "Iteration 35, loss = 0.01279882\n",
      "Iteration 36, loss = 0.01113009\n",
      "Iteration 37, loss = 0.00953834\n",
      "Iteration 38, loss = 0.00896151\n",
      "Iteration 39, loss = 0.00916492\n",
      "Iteration 40, loss = 0.01749762\n",
      "Iteration 41, loss = 0.02791767\n",
      "Iteration 42, loss = 0.01406453\n",
      "Iteration 43, loss = 0.01254134\n",
      "Iteration 44, loss = 0.01101345\n",
      "Iteration 45, loss = 0.01088510\n",
      "Iteration 46, loss = 0.00905876\n",
      "Iteration 47, loss = 0.01306197\n",
      "Iteration 48, loss = 0.00967467\n",
      "Iteration 49, loss = 0.00871391\n",
      "Iteration 50, loss = 0.00880731\n",
      "Iteration 1, loss = 0.13737985\n",
      "Iteration 2, loss = 0.04736666\n",
      "Iteration 3, loss = 0.04306082\n",
      "Iteration 4, loss = 0.03309173\n",
      "Iteration 5, loss = 0.03436853\n",
      "Iteration 6, loss = 0.02911062\n",
      "Iteration 7, loss = 0.02512054\n",
      "Iteration 8, loss = 0.02350317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.02234888\n",
      "Iteration 10, loss = 0.02217296\n",
      "Iteration 11, loss = 0.02177720\n",
      "Iteration 12, loss = 0.02545649\n",
      "Iteration 13, loss = 0.02502377\n",
      "Iteration 14, loss = 0.02112000\n",
      "Iteration 15, loss = 0.01834870\n",
      "Iteration 16, loss = 0.01932374\n",
      "Iteration 17, loss = 0.01785858\n",
      "Iteration 18, loss = 0.01765026\n",
      "Iteration 19, loss = 0.01735738\n",
      "Iteration 20, loss = 0.01884261\n",
      "Iteration 21, loss = 0.01476676\n",
      "Iteration 22, loss = 0.01380486\n",
      "Iteration 23, loss = 0.01283584\n",
      "Iteration 24, loss = 0.02088722\n",
      "Iteration 25, loss = 0.01263294\n",
      "Iteration 26, loss = 0.01082623\n",
      "Iteration 27, loss = 0.01340142\n",
      "Iteration 28, loss = 0.01064761\n",
      "Iteration 29, loss = 0.01876806\n",
      "Iteration 30, loss = 0.01150994\n",
      "Iteration 31, loss = 0.00860295\n",
      "Iteration 32, loss = 0.00832678\n",
      "Iteration 33, loss = 0.00671369\n",
      "Iteration 34, loss = 0.00800747\n",
      "Iteration 35, loss = 0.00834345\n",
      "Iteration 36, loss = 0.00614933\n",
      "Iteration 37, loss = 0.01494068\n",
      "Iteration 38, loss = 0.00953336\n",
      "Iteration 39, loss = 0.02636042\n",
      "Iteration 40, loss = 0.01131612\n",
      "Iteration 41, loss = 0.00926091\n",
      "Iteration 42, loss = 0.00762577\n",
      "Iteration 43, loss = 0.00546541\n",
      "Iteration 44, loss = 0.00705484\n",
      "Iteration 45, loss = 0.00504204\n",
      "Iteration 46, loss = 0.01004756\n",
      "Iteration 47, loss = 0.00824202\n",
      "Iteration 48, loss = 0.00652217\n",
      "Iteration 49, loss = 0.01750751\n",
      "Iteration 50, loss = 0.01649818\n",
      "Iteration 1, loss = 0.14465480\n",
      "Iteration 2, loss = 0.04451996\n",
      "Iteration 3, loss = 0.03730034\n",
      "Iteration 4, loss = 0.03137050\n",
      "Iteration 5, loss = 0.03047016\n",
      "Iteration 6, loss = 0.03686319\n",
      "Iteration 7, loss = 0.02480334\n",
      "Iteration 8, loss = 0.02488542\n",
      "Iteration 9, loss = 0.02536243\n",
      "Iteration 10, loss = 0.02205536\n",
      "Iteration 11, loss = 0.02442996\n",
      "Iteration 12, loss = 0.01910368\n",
      "Iteration 13, loss = 0.01785638\n",
      "Iteration 14, loss = 0.01768839\n",
      "Iteration 15, loss = 0.01622540\n",
      "Iteration 16, loss = 0.01815966\n",
      "Iteration 17, loss = 0.01987383\n",
      "Iteration 18, loss = 0.01639434\n",
      "Iteration 19, loss = 0.01348651\n",
      "Iteration 20, loss = 0.01286231\n",
      "Iteration 21, loss = 0.01452191\n",
      "Iteration 22, loss = 0.01671758\n",
      "Iteration 23, loss = 0.01470869\n",
      "Iteration 24, loss = 0.01378499\n",
      "Iteration 25, loss = 0.01481925\n",
      "Iteration 26, loss = 0.01500368\n",
      "Iteration 27, loss = 0.01089573\n",
      "Iteration 28, loss = 0.01060324\n",
      "Iteration 29, loss = 0.02330821\n",
      "Iteration 30, loss = 0.01122292\n",
      "Iteration 31, loss = 0.00986376\n",
      "Iteration 32, loss = 0.00851470\n",
      "Iteration 33, loss = 0.00957594\n",
      "Iteration 34, loss = 0.01164680\n",
      "Iteration 35, loss = 0.01599622\n",
      "Iteration 36, loss = 0.01619594\n",
      "Iteration 37, loss = 0.01260717\n",
      "Iteration 38, loss = 0.00889054\n",
      "Iteration 39, loss = 0.00869723\n",
      "Iteration 40, loss = 0.00706884\n",
      "Iteration 41, loss = 0.00639743\n",
      "Iteration 42, loss = 0.00697819\n",
      "Iteration 43, loss = 0.00553237\n",
      "Iteration 44, loss = 0.00418586\n",
      "Iteration 45, loss = 0.00496651\n",
      "Iteration 46, loss = 0.00481712\n",
      "Iteration 47, loss = 0.00477978\n",
      "Iteration 48, loss = 0.00420987\n",
      "Iteration 49, loss = 0.00447056\n",
      "Iteration 50, loss = 0.01640109\n",
      "Iteration 1, loss = 0.15938382\n",
      "Iteration 2, loss = 0.04734314\n",
      "Iteration 3, loss = 0.04311531\n",
      "Iteration 4, loss = 0.03536408\n",
      "Iteration 5, loss = 0.03034105\n",
      "Iteration 6, loss = 0.03267053\n",
      "Iteration 7, loss = 0.02668899\n",
      "Iteration 8, loss = 0.02753354\n",
      "Iteration 9, loss = 0.02404329\n",
      "Iteration 10, loss = 0.02602041\n",
      "Iteration 11, loss = 0.02310675\n",
      "Iteration 12, loss = 0.02194751\n",
      "Iteration 13, loss = 0.02364716\n",
      "Iteration 14, loss = 0.01908340\n",
      "Iteration 15, loss = 0.01934560\n",
      "Iteration 16, loss = 0.01932788\n",
      "Iteration 17, loss = 0.01783563\n",
      "Iteration 18, loss = 0.02061194\n",
      "Iteration 19, loss = 0.01932402\n",
      "Iteration 20, loss = 0.01824444\n",
      "Iteration 21, loss = 0.01796253\n",
      "Iteration 22, loss = 0.01984756\n",
      "Iteration 23, loss = 0.01655891\n",
      "Iteration 24, loss = 0.01488240\n",
      "Iteration 25, loss = 0.01691049\n",
      "Iteration 26, loss = 0.01667403\n",
      "Iteration 27, loss = 0.01553376\n",
      "Iteration 28, loss = 0.01991809\n",
      "Iteration 29, loss = 0.01543495\n",
      "Iteration 30, loss = 0.01252711\n",
      "Iteration 31, loss = 0.01063284\n",
      "Iteration 32, loss = 0.01554611\n",
      "Iteration 33, loss = 0.01142376\n",
      "Iteration 34, loss = 0.00935706\n",
      "Iteration 35, loss = 0.01087855\n",
      "Iteration 36, loss = 0.02425809\n",
      "Iteration 37, loss = 0.01787358\n",
      "Iteration 38, loss = 0.01353861\n",
      "Iteration 39, loss = 0.01197489\n",
      "Iteration 40, loss = 0.01032434\n",
      "Iteration 41, loss = 0.01078444\n",
      "Iteration 42, loss = 0.00794395\n",
      "Iteration 43, loss = 0.00716631\n",
      "Iteration 44, loss = 0.00722247\n",
      "Iteration 45, loss = 0.00743164\n",
      "Iteration 46, loss = 0.00718821\n",
      "Iteration 47, loss = 0.00590146\n",
      "Iteration 48, loss = 0.01635561\n",
      "Iteration 49, loss = 0.01776759\n",
      "Iteration 50, loss = 0.01261627\n",
      "Iteration 1, loss = 0.13871797\n",
      "Iteration 2, loss = 0.04769928\n",
      "Iteration 3, loss = 0.04033125\n",
      "Iteration 4, loss = 0.03605059\n",
      "Iteration 5, loss = 0.03259277\n",
      "Iteration 6, loss = 0.03070970\n",
      "Iteration 7, loss = 0.02690602\n",
      "Iteration 8, loss = 0.02564916\n",
      "Iteration 9, loss = 0.02351612\n",
      "Iteration 10, loss = 0.02166438\n",
      "Iteration 11, loss = 0.02037938\n",
      "Iteration 12, loss = 0.02005347\n",
      "Iteration 13, loss = 0.02043314\n",
      "Iteration 14, loss = 0.01839946\n",
      "Iteration 15, loss = 0.01692687\n",
      "Iteration 16, loss = 0.01944990\n",
      "Iteration 17, loss = 0.01723606\n",
      "Iteration 18, loss = 0.01565994\n",
      "Iteration 19, loss = 0.01461627\n",
      "Iteration 20, loss = 0.01780330\n",
      "Iteration 21, loss = 0.02207129\n",
      "Iteration 22, loss = 0.01418314\n",
      "Iteration 23, loss = 0.01207932\n",
      "Iteration 24, loss = 0.01841160\n",
      "Iteration 25, loss = 0.01447317\n",
      "Iteration 26, loss = 0.01412778\n",
      "Iteration 27, loss = 0.01183565\n",
      "Iteration 28, loss = 0.00902935\n",
      "Iteration 29, loss = 0.00981040\n",
      "Iteration 30, loss = 0.01056873\n",
      "Iteration 31, loss = 0.01100165\n",
      "Iteration 32, loss = 0.00801910\n",
      "Iteration 33, loss = 0.00702819\n",
      "Iteration 34, loss = 0.01140796\n",
      "Iteration 35, loss = 0.01269037\n",
      "Iteration 36, loss = 0.01330995\n",
      "Iteration 37, loss = 0.00676935\n",
      "Iteration 38, loss = 0.00684571\n",
      "Iteration 39, loss = 0.00651565\n",
      "Iteration 40, loss = 0.00631554\n",
      "Iteration 41, loss = 0.00747107\n",
      "Iteration 42, loss = 0.00917713\n",
      "Iteration 43, loss = 0.00961138\n",
      "Iteration 44, loss = 0.00695936\n",
      "Iteration 45, loss = 0.01270969\n",
      "Iteration 46, loss = 0.00861153\n",
      "Iteration 47, loss = 0.00437709\n",
      "Iteration 48, loss = 0.01454344\n",
      "Iteration 49, loss = 0.01330259\n",
      "Iteration 50, loss = 0.00524332\n",
      "Iteration 1, loss = 0.14874333\n",
      "Iteration 2, loss = 0.04897121\n",
      "Iteration 3, loss = 0.03972580\n",
      "Iteration 4, loss = 0.03445673\n",
      "Iteration 5, loss = 0.03510801\n",
      "Iteration 6, loss = 0.02986320\n",
      "Iteration 7, loss = 0.02797829\n",
      "Iteration 8, loss = 0.02966009\n",
      "Iteration 9, loss = 0.02478845\n",
      "Iteration 10, loss = 0.02257138\n",
      "Iteration 11, loss = 0.03330266\n",
      "Iteration 12, loss = 0.02280490\n",
      "Iteration 13, loss = 0.02146397\n",
      "Iteration 14, loss = 0.02167231\n",
      "Iteration 15, loss = 0.02042199\n",
      "Iteration 16, loss = 0.01896892\n",
      "Iteration 17, loss = 0.02097858\n",
      "Iteration 18, loss = 0.01723299\n",
      "Iteration 19, loss = 0.01733796\n",
      "Iteration 20, loss = 0.01819822\n",
      "Iteration 21, loss = 0.01651615\n",
      "Iteration 22, loss = 0.01612122\n",
      "Iteration 23, loss = 0.01508642\n",
      "Iteration 24, loss = 0.01818704\n",
      "Iteration 25, loss = 0.01150254\n",
      "Iteration 26, loss = 0.01186979\n",
      "Iteration 27, loss = 0.01071878\n",
      "Iteration 28, loss = 0.01225013\n",
      "Iteration 29, loss = 0.01381648\n",
      "Iteration 30, loss = 0.01502619\n",
      "Iteration 31, loss = 0.01181794\n",
      "Iteration 32, loss = 0.01490738\n",
      "Iteration 33, loss = 0.01021058\n",
      "Iteration 34, loss = 0.01000195\n",
      "Iteration 35, loss = 0.01095786\n",
      "Iteration 36, loss = 0.01309218\n",
      "Iteration 37, loss = 0.01119558\n",
      "Iteration 38, loss = 0.00924328\n",
      "Iteration 39, loss = 0.00941901\n",
      "Iteration 40, loss = 0.01068508\n",
      "Iteration 41, loss = 0.01328675\n",
      "Iteration 42, loss = 0.01100718\n",
      "Iteration 43, loss = 0.00855643\n",
      "Iteration 44, loss = 0.00716092\n",
      "Iteration 45, loss = 0.00573177\n",
      "Iteration 46, loss = 0.00557452\n",
      "Iteration 47, loss = 0.00394645\n",
      "Iteration 48, loss = 0.00496652\n",
      "Iteration 49, loss = 0.00491070\n",
      "Iteration 50, loss = 0.01179482\n",
      "Iteration 1, loss = 0.14910358\n",
      "Iteration 2, loss = 0.04772093\n",
      "Iteration 3, loss = 0.03973088\n",
      "Iteration 4, loss = 0.03356502\n",
      "Iteration 5, loss = 0.03388989\n",
      "Iteration 6, loss = 0.02841598\n",
      "Iteration 7, loss = 0.03106286\n",
      "Iteration 8, loss = 0.02558636\n",
      "Iteration 9, loss = 0.02442653\n",
      "Iteration 10, loss = 0.02362071\n",
      "Iteration 11, loss = 0.02251649\n",
      "Iteration 12, loss = 0.02089783\n",
      "Iteration 13, loss = 0.02453988\n",
      "Iteration 14, loss = 0.01943652\n",
      "Iteration 15, loss = 0.01838285\n",
      "Iteration 16, loss = 0.01786047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.01679519\n",
      "Iteration 18, loss = 0.01576782\n",
      "Iteration 19, loss = 0.01617654\n",
      "Iteration 20, loss = 0.01434042\n",
      "Iteration 21, loss = 0.01272023\n",
      "Iteration 22, loss = 0.01344974\n",
      "Iteration 23, loss = 0.01435333\n",
      "Iteration 24, loss = 0.01336632\n",
      "Iteration 25, loss = 0.01090587\n",
      "Iteration 26, loss = 0.01554131\n",
      "Iteration 27, loss = 0.02041425\n",
      "Iteration 28, loss = 0.01920881\n",
      "Iteration 29, loss = 0.02095692\n",
      "Iteration 30, loss = 0.01202508\n",
      "Iteration 31, loss = 0.01043802\n",
      "Iteration 32, loss = 0.01000977\n",
      "Iteration 33, loss = 0.01545953\n",
      "Iteration 34, loss = 0.01604372\n",
      "Iteration 35, loss = 0.01304582\n",
      "Iteration 36, loss = 0.01804733\n",
      "Iteration 37, loss = 0.01415384\n",
      "Iteration 38, loss = 0.01019539\n",
      "Iteration 39, loss = 0.00735746\n",
      "Iteration 40, loss = 0.01160119\n",
      "Iteration 41, loss = 0.00789764\n",
      "Iteration 42, loss = 0.00905202\n",
      "Iteration 43, loss = 0.00545844\n",
      "Iteration 44, loss = 0.00586776\n",
      "Iteration 45, loss = 0.00524474\n",
      "Iteration 46, loss = 0.00860896\n",
      "Iteration 47, loss = 0.00638686\n",
      "Iteration 48, loss = 0.00455457\n",
      "Iteration 49, loss = 0.01152216\n",
      "Iteration 50, loss = 0.00906272\n",
      "Train Accuracy: 0.9921\n",
      "Running Memory: 1.35 MB\n",
      "Running Time: 106.86 Seconds\n",
      "Test  Accuracy: 0.9925\n",
      "Test  Presion:  0.9921\n",
      "Test  Recall:   0.9934\n",
      "Test  F1-score: 0.9927\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS1_bow_1, memory_MLP_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS1_bow_1, acc_MLP_DS1_bow_1, pre_MLP_DS1_bow_1, rec_MLP_DS1_bow_1, f1_MLP_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS1_bow_1, model_3, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.30000068\n",
      "Iteration 2, loss = 0.25348678\n",
      "Iteration 3, loss = 0.24713186\n",
      "Iteration 4, loss = 0.24259821\n",
      "Iteration 5, loss = 0.24000650\n",
      "Iteration 6, loss = 0.23784394\n",
      "Iteration 7, loss = 0.23531795\n",
      "Iteration 8, loss = 0.23461686\n",
      "Iteration 9, loss = 0.23280488\n",
      "Iteration 10, loss = 0.23076168\n",
      "Iteration 11, loss = 0.22956946\n",
      "Iteration 12, loss = 0.23102106\n",
      "Iteration 13, loss = 0.22746453\n",
      "Iteration 14, loss = 0.22765058\n",
      "Iteration 15, loss = 0.22607987\n",
      "Iteration 16, loss = 0.22517974\n",
      "Iteration 17, loss = 0.22457562\n",
      "Iteration 18, loss = 0.22320885\n",
      "Iteration 19, loss = 0.22281152\n",
      "Iteration 20, loss = 0.22205175\n",
      "Iteration 21, loss = 0.22025513\n",
      "Iteration 22, loss = 0.22152313\n",
      "Iteration 23, loss = 0.21902537\n",
      "Iteration 24, loss = 0.21835806\n",
      "Iteration 25, loss = 0.21788871\n",
      "Iteration 26, loss = 0.21640061\n",
      "Iteration 27, loss = 0.21753063\n",
      "Iteration 28, loss = 0.21501083\n",
      "Iteration 29, loss = 0.21395160\n",
      "Iteration 30, loss = 0.21385931\n",
      "Iteration 31, loss = 0.21357909\n",
      "Iteration 32, loss = 0.21131179\n",
      "Iteration 33, loss = 0.21066961\n",
      "Iteration 34, loss = 0.21034375\n",
      "Iteration 35, loss = 0.21128932\n",
      "Iteration 36, loss = 0.20932467\n",
      "Iteration 37, loss = 0.20916220\n",
      "Iteration 38, loss = 0.20756848\n",
      "Iteration 39, loss = 0.20765108\n",
      "Iteration 40, loss = 0.20665825\n",
      "Iteration 41, loss = 0.20495495\n",
      "Iteration 42, loss = 0.20577623\n",
      "Iteration 43, loss = 0.20402881\n",
      "Iteration 44, loss = 0.20455176\n",
      "Iteration 45, loss = 0.20384695\n",
      "Iteration 46, loss = 0.20386868\n",
      "Iteration 47, loss = 0.20324163\n",
      "Iteration 48, loss = 0.20196500\n",
      "Iteration 49, loss = 0.20241181\n",
      "Iteration 50, loss = 0.20258255\n",
      "Iteration 1, loss = 0.30556598\n",
      "Iteration 2, loss = 0.25335556\n",
      "Iteration 3, loss = 0.24771382\n",
      "Iteration 4, loss = 0.24338760\n",
      "Iteration 5, loss = 0.23976860\n",
      "Iteration 6, loss = 0.23734187\n",
      "Iteration 7, loss = 0.23648178\n",
      "Iteration 8, loss = 0.23355501\n",
      "Iteration 9, loss = 0.23214792\n",
      "Iteration 10, loss = 0.23189958\n",
      "Iteration 11, loss = 0.23033878\n",
      "Iteration 12, loss = 0.22942705\n",
      "Iteration 13, loss = 0.22751625\n",
      "Iteration 14, loss = 0.22627653\n",
      "Iteration 15, loss = 0.22622833\n",
      "Iteration 16, loss = 0.22424627\n",
      "Iteration 17, loss = 0.22429946\n",
      "Iteration 18, loss = 0.22264375\n",
      "Iteration 19, loss = 0.22133825\n",
      "Iteration 20, loss = 0.22100588\n",
      "Iteration 21, loss = 0.22004325\n",
      "Iteration 22, loss = 0.21954639\n",
      "Iteration 23, loss = 0.21914082\n",
      "Iteration 24, loss = 0.21927080\n",
      "Iteration 25, loss = 0.21572074\n",
      "Iteration 26, loss = 0.21694066\n",
      "Iteration 27, loss = 0.21540440\n",
      "Iteration 28, loss = 0.21420192\n",
      "Iteration 29, loss = 0.21311672\n",
      "Iteration 30, loss = 0.21274250\n",
      "Iteration 31, loss = 0.21260525\n",
      "Iteration 32, loss = 0.21194529\n",
      "Iteration 33, loss = 0.21032015\n",
      "Iteration 34, loss = 0.21156325\n",
      "Iteration 35, loss = 0.20936076\n",
      "Iteration 36, loss = 0.20851774\n",
      "Iteration 37, loss = 0.21007494\n",
      "Iteration 38, loss = 0.20834204\n",
      "Iteration 39, loss = 0.20641474\n",
      "Iteration 40, loss = 0.20670081\n",
      "Iteration 41, loss = 0.20609200\n",
      "Iteration 42, loss = 0.20449359\n",
      "Iteration 43, loss = 0.20378820\n",
      "Iteration 44, loss = 0.20460933\n",
      "Iteration 45, loss = 0.20509616\n",
      "Iteration 46, loss = 0.20316464\n",
      "Iteration 47, loss = 0.20526379\n",
      "Iteration 48, loss = 0.20313079\n",
      "Iteration 49, loss = 0.20124183\n",
      "Iteration 50, loss = 0.20298380\n",
      "Iteration 1, loss = 0.30830757\n",
      "Iteration 2, loss = 0.25594212\n",
      "Iteration 3, loss = 0.24649314\n",
      "Iteration 4, loss = 0.24330306\n",
      "Iteration 5, loss = 0.23931031\n",
      "Iteration 6, loss = 0.23751942\n",
      "Iteration 7, loss = 0.23551111\n",
      "Iteration 8, loss = 0.23316365\n",
      "Iteration 9, loss = 0.23214834\n",
      "Iteration 10, loss = 0.23199188\n",
      "Iteration 11, loss = 0.22922505\n",
      "Iteration 12, loss = 0.22904588\n",
      "Iteration 13, loss = 0.22627233\n",
      "Iteration 14, loss = 0.22660373\n",
      "Iteration 15, loss = 0.22543499\n",
      "Iteration 16, loss = 0.22335326\n",
      "Iteration 17, loss = 0.22345662\n",
      "Iteration 18, loss = 0.22205943\n",
      "Iteration 19, loss = 0.22207932\n",
      "Iteration 20, loss = 0.22109183\n",
      "Iteration 21, loss = 0.21997280\n",
      "Iteration 22, loss = 0.21991959\n",
      "Iteration 23, loss = 0.21897417\n",
      "Iteration 24, loss = 0.21578805\n",
      "Iteration 25, loss = 0.21617467\n",
      "Iteration 26, loss = 0.21723598\n",
      "Iteration 27, loss = 0.21579039\n",
      "Iteration 28, loss = 0.21379672\n",
      "Iteration 29, loss = 0.21389577\n",
      "Iteration 30, loss = 0.21254442\n",
      "Iteration 31, loss = 0.21288064\n",
      "Iteration 32, loss = 0.21110992\n",
      "Iteration 33, loss = 0.20966245\n",
      "Iteration 34, loss = 0.20966614\n",
      "Iteration 35, loss = 0.20933982\n",
      "Iteration 36, loss = 0.20916171\n",
      "Iteration 37, loss = 0.20885884\n",
      "Iteration 38, loss = 0.20757779\n",
      "Iteration 39, loss = 0.20719261\n",
      "Iteration 40, loss = 0.20601940\n",
      "Iteration 41, loss = 0.20608966\n",
      "Iteration 42, loss = 0.20534500\n",
      "Iteration 43, loss = 0.20488402\n",
      "Iteration 44, loss = 0.20372357\n",
      "Iteration 45, loss = 0.20145104\n",
      "Iteration 46, loss = 0.20206013\n",
      "Iteration 47, loss = 0.20047025\n",
      "Iteration 48, loss = 0.20088517\n",
      "Iteration 49, loss = 0.20062506\n",
      "Iteration 50, loss = 0.20032137\n",
      "Iteration 1, loss = 0.30423538\n",
      "Iteration 2, loss = 0.25477073\n",
      "Iteration 3, loss = 0.24585415\n",
      "Iteration 4, loss = 0.24149663\n",
      "Iteration 5, loss = 0.24027496\n",
      "Iteration 6, loss = 0.23690296\n",
      "Iteration 7, loss = 0.23624379\n",
      "Iteration 8, loss = 0.23413594\n",
      "Iteration 9, loss = 0.23182736\n",
      "Iteration 10, loss = 0.23175223\n",
      "Iteration 11, loss = 0.23046361\n",
      "Iteration 12, loss = 0.22734111\n",
      "Iteration 13, loss = 0.22878825\n",
      "Iteration 14, loss = 0.22754802\n",
      "Iteration 15, loss = 0.22584461\n",
      "Iteration 16, loss = 0.22538959\n",
      "Iteration 17, loss = 0.22507517\n",
      "Iteration 18, loss = 0.22317189\n",
      "Iteration 19, loss = 0.22236587\n",
      "Iteration 20, loss = 0.22184320\n",
      "Iteration 21, loss = 0.22075251\n",
      "Iteration 22, loss = 0.21938554\n",
      "Iteration 23, loss = 0.21963038\n",
      "Iteration 24, loss = 0.21851927\n",
      "Iteration 25, loss = 0.21705909\n",
      "Iteration 26, loss = 0.21670845\n",
      "Iteration 27, loss = 0.21668122\n",
      "Iteration 28, loss = 0.21670214\n",
      "Iteration 29, loss = 0.21461832\n",
      "Iteration 30, loss = 0.21385382\n",
      "Iteration 31, loss = 0.21309939\n",
      "Iteration 32, loss = 0.21129481\n",
      "Iteration 33, loss = 0.21133847\n",
      "Iteration 34, loss = 0.21058529\n",
      "Iteration 35, loss = 0.21008446\n",
      "Iteration 36, loss = 0.21045174\n",
      "Iteration 37, loss = 0.20853796\n",
      "Iteration 38, loss = 0.20819278\n",
      "Iteration 39, loss = 0.20736329\n",
      "Iteration 40, loss = 0.20503359\n",
      "Iteration 41, loss = 0.20649939\n",
      "Iteration 42, loss = 0.20575758\n",
      "Iteration 43, loss = 0.20446610\n",
      "Iteration 44, loss = 0.20513333\n",
      "Iteration 45, loss = 0.20417219\n",
      "Iteration 46, loss = 0.20532713\n",
      "Iteration 47, loss = 0.20519653\n",
      "Iteration 48, loss = 0.20354185\n",
      "Iteration 49, loss = 0.20358080\n",
      "Iteration 50, loss = 0.20111685\n",
      "Iteration 1, loss = 0.30479526\n",
      "Iteration 2, loss = 0.25545331\n",
      "Iteration 3, loss = 0.24636072\n",
      "Iteration 4, loss = 0.24357577\n",
      "Iteration 5, loss = 0.24161472\n",
      "Iteration 6, loss = 0.23623905\n",
      "Iteration 7, loss = 0.23528595\n",
      "Iteration 8, loss = 0.23442471\n",
      "Iteration 9, loss = 0.23273984\n",
      "Iteration 10, loss = 0.23174547\n",
      "Iteration 11, loss = 0.23058820\n",
      "Iteration 12, loss = 0.22905075\n",
      "Iteration 13, loss = 0.22881959\n",
      "Iteration 14, loss = 0.22742653\n",
      "Iteration 15, loss = 0.22670490\n",
      "Iteration 16, loss = 0.22555208\n",
      "Iteration 17, loss = 0.22588986\n",
      "Iteration 18, loss = 0.22459716\n",
      "Iteration 19, loss = 0.22260838\n",
      "Iteration 20, loss = 0.22151534\n",
      "Iteration 21, loss = 0.22060349\n",
      "Iteration 22, loss = 0.21977130\n",
      "Iteration 23, loss = 0.22012896\n",
      "Iteration 24, loss = 0.21954445\n",
      "Iteration 25, loss = 0.21780862\n",
      "Iteration 26, loss = 0.21730344\n",
      "Iteration 27, loss = 0.21653437\n",
      "Iteration 28, loss = 0.21700538\n",
      "Iteration 29, loss = 0.21483009\n",
      "Iteration 30, loss = 0.21451680\n",
      "Iteration 31, loss = 0.21267310\n",
      "Iteration 32, loss = 0.21273893\n",
      "Iteration 33, loss = 0.21139966\n",
      "Iteration 34, loss = 0.21156581\n",
      "Iteration 35, loss = 0.21022254\n",
      "Iteration 36, loss = 0.21046474\n",
      "Iteration 37, loss = 0.20995409\n",
      "Iteration 38, loss = 0.20980959\n",
      "Iteration 39, loss = 0.20819449\n",
      "Iteration 40, loss = 0.20801130\n",
      "Iteration 41, loss = 0.20789254\n",
      "Iteration 42, loss = 0.21036644\n",
      "Iteration 43, loss = 0.20669119\n",
      "Iteration 44, loss = 0.20607076\n",
      "Iteration 45, loss = 0.20497972\n",
      "Iteration 46, loss = 0.20354617\n",
      "Iteration 47, loss = 0.20270078\n",
      "Iteration 48, loss = 0.20349759\n",
      "Iteration 49, loss = 0.20077917\n",
      "Iteration 50, loss = 0.20197238\n",
      "Iteration 1, loss = 0.30553721\n",
      "Iteration 2, loss = 0.25382324\n",
      "Iteration 3, loss = 0.24592643\n",
      "Iteration 4, loss = 0.24279493\n",
      "Iteration 5, loss = 0.23924291\n",
      "Iteration 6, loss = 0.23667389\n",
      "Iteration 7, loss = 0.23497193\n",
      "Iteration 8, loss = 0.23462605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.23166705\n",
      "Iteration 10, loss = 0.23205756\n",
      "Iteration 11, loss = 0.22996246\n",
      "Iteration 12, loss = 0.22930972\n",
      "Iteration 13, loss = 0.22726738\n",
      "Iteration 14, loss = 0.22684715\n",
      "Iteration 15, loss = 0.22655120\n",
      "Iteration 16, loss = 0.22551469\n",
      "Iteration 17, loss = 0.22511724\n",
      "Iteration 18, loss = 0.22323745\n",
      "Iteration 19, loss = 0.22170149\n",
      "Iteration 20, loss = 0.22253370\n",
      "Iteration 21, loss = 0.22042791\n",
      "Iteration 22, loss = 0.21924334\n",
      "Iteration 23, loss = 0.21886539\n",
      "Iteration 24, loss = 0.21663002\n",
      "Iteration 25, loss = 0.21624657\n",
      "Iteration 26, loss = 0.21830900\n",
      "Iteration 27, loss = 0.21620639\n",
      "Iteration 28, loss = 0.21420244\n",
      "Iteration 29, loss = 0.21353564\n",
      "Iteration 30, loss = 0.21461041\n",
      "Iteration 31, loss = 0.21401529\n",
      "Iteration 32, loss = 0.21322955\n",
      "Iteration 33, loss = 0.21253051\n",
      "Iteration 34, loss = 0.21127465\n",
      "Iteration 35, loss = 0.21130201\n",
      "Iteration 36, loss = 0.21074601\n",
      "Iteration 37, loss = 0.20861309\n",
      "Iteration 38, loss = 0.20814627\n",
      "Iteration 39, loss = 0.20793251\n",
      "Iteration 40, loss = 0.20586062\n",
      "Iteration 41, loss = 0.20726975\n",
      "Iteration 42, loss = 0.20500626\n",
      "Iteration 43, loss = 0.20456373\n",
      "Iteration 44, loss = 0.20523638\n",
      "Iteration 45, loss = 0.20413658\n",
      "Iteration 46, loss = 0.20295352\n",
      "Iteration 47, loss = 0.20383343\n",
      "Iteration 48, loss = 0.20411572\n",
      "Iteration 49, loss = 0.20290251\n",
      "Iteration 50, loss = 0.20132343\n",
      "Iteration 1, loss = 0.30520878\n",
      "Iteration 2, loss = 0.25599285\n",
      "Iteration 3, loss = 0.24763124\n",
      "Iteration 4, loss = 0.24271140\n",
      "Iteration 5, loss = 0.23869587\n",
      "Iteration 6, loss = 0.23798820\n",
      "Iteration 7, loss = 0.23410806\n",
      "Iteration 8, loss = 0.23314123\n",
      "Iteration 9, loss = 0.23263508\n",
      "Iteration 10, loss = 0.23079448\n",
      "Iteration 11, loss = 0.22877282\n",
      "Iteration 12, loss = 0.22737868\n",
      "Iteration 13, loss = 0.22717694\n",
      "Iteration 14, loss = 0.22618215\n",
      "Iteration 15, loss = 0.22457870\n",
      "Iteration 16, loss = 0.22325812\n",
      "Iteration 17, loss = 0.22235191\n",
      "Iteration 18, loss = 0.22252785\n",
      "Iteration 19, loss = 0.22150288\n",
      "Iteration 20, loss = 0.21965869\n",
      "Iteration 21, loss = 0.21916341\n",
      "Iteration 22, loss = 0.21775785\n",
      "Iteration 23, loss = 0.21793230\n",
      "Iteration 24, loss = 0.21830838\n",
      "Iteration 25, loss = 0.21610417\n",
      "Iteration 26, loss = 0.21641273\n",
      "Iteration 27, loss = 0.21511343\n",
      "Iteration 28, loss = 0.21289308\n",
      "Iteration 29, loss = 0.21323045\n",
      "Iteration 30, loss = 0.21208525\n",
      "Iteration 31, loss = 0.21110869\n",
      "Iteration 32, loss = 0.21062179\n",
      "Iteration 33, loss = 0.20973405\n",
      "Iteration 34, loss = 0.20849287\n",
      "Iteration 35, loss = 0.20892785\n",
      "Iteration 36, loss = 0.20613637\n",
      "Iteration 37, loss = 0.20796462\n",
      "Iteration 38, loss = 0.20626262\n",
      "Iteration 39, loss = 0.20647041\n",
      "Iteration 40, loss = 0.20630535\n",
      "Iteration 41, loss = 0.20577302\n",
      "Iteration 42, loss = 0.20386225\n",
      "Iteration 43, loss = 0.20367328\n",
      "Iteration 44, loss = 0.20559787\n",
      "Iteration 45, loss = 0.20247289\n",
      "Iteration 46, loss = 0.20092403\n",
      "Iteration 47, loss = 0.19988254\n",
      "Iteration 48, loss = 0.19953760\n",
      "Iteration 49, loss = 0.19899483\n",
      "Iteration 50, loss = 0.19760695\n",
      "Iteration 1, loss = 0.30641373\n",
      "Iteration 2, loss = 0.25523386\n",
      "Iteration 3, loss = 0.24766816\n",
      "Iteration 4, loss = 0.24482178\n",
      "Iteration 5, loss = 0.24129748\n",
      "Iteration 6, loss = 0.23878816\n",
      "Iteration 7, loss = 0.23660845\n",
      "Iteration 8, loss = 0.23508064\n",
      "Iteration 9, loss = 0.23424766\n",
      "Iteration 10, loss = 0.23218676\n",
      "Iteration 11, loss = 0.23148050\n",
      "Iteration 12, loss = 0.23062285\n",
      "Iteration 13, loss = 0.23015809\n",
      "Iteration 14, loss = 0.22867731\n",
      "Iteration 15, loss = 0.22706592\n",
      "Iteration 16, loss = 0.22642308\n",
      "Iteration 17, loss = 0.22583999\n",
      "Iteration 18, loss = 0.22431610\n",
      "Iteration 19, loss = 0.22356770\n",
      "Iteration 20, loss = 0.22205237\n",
      "Iteration 21, loss = 0.22209265\n",
      "Iteration 22, loss = 0.22089917\n",
      "Iteration 23, loss = 0.22167664\n",
      "Iteration 24, loss = 0.21933534\n",
      "Iteration 25, loss = 0.21997101\n",
      "Iteration 26, loss = 0.21775227\n",
      "Iteration 27, loss = 0.21770534\n",
      "Iteration 28, loss = 0.21736908\n",
      "Iteration 29, loss = 0.21510718\n",
      "Iteration 30, loss = 0.21395949\n",
      "Iteration 31, loss = 0.21499181\n",
      "Iteration 32, loss = 0.21475125\n",
      "Iteration 33, loss = 0.21307329\n",
      "Iteration 34, loss = 0.21152645\n",
      "Iteration 35, loss = 0.21215390\n",
      "Iteration 36, loss = 0.20988135\n",
      "Iteration 37, loss = 0.20959163\n",
      "Iteration 38, loss = 0.20911734\n",
      "Iteration 39, loss = 0.20976330\n",
      "Iteration 40, loss = 0.20914322\n",
      "Iteration 41, loss = 0.20709130\n",
      "Iteration 42, loss = 0.20553462\n",
      "Iteration 43, loss = 0.20369536\n",
      "Iteration 44, loss = 0.20448525\n",
      "Iteration 45, loss = 0.20538017\n",
      "Iteration 46, loss = 0.20341691\n",
      "Iteration 47, loss = 0.20201029\n",
      "Iteration 48, loss = 0.20529569\n",
      "Iteration 49, loss = 0.20404317\n",
      "Iteration 50, loss = 0.20024307\n",
      "Iteration 1, loss = 0.30571178\n",
      "Iteration 2, loss = 0.25487083\n",
      "Iteration 3, loss = 0.24740260\n",
      "Iteration 4, loss = 0.24217079\n",
      "Iteration 5, loss = 0.23973895\n",
      "Iteration 6, loss = 0.23701914\n",
      "Iteration 7, loss = 0.23506621\n",
      "Iteration 8, loss = 0.23414364\n",
      "Iteration 9, loss = 0.23277477\n",
      "Iteration 10, loss = 0.23068509\n",
      "Iteration 11, loss = 0.22945291\n",
      "Iteration 12, loss = 0.22824019\n",
      "Iteration 13, loss = 0.22800413\n",
      "Iteration 14, loss = 0.22664789\n",
      "Iteration 15, loss = 0.22506341\n",
      "Iteration 16, loss = 0.22495121\n",
      "Iteration 17, loss = 0.22479400\n",
      "Iteration 18, loss = 0.22398118\n",
      "Iteration 19, loss = 0.22236119\n",
      "Iteration 20, loss = 0.22099445\n",
      "Iteration 21, loss = 0.22065921\n",
      "Iteration 22, loss = 0.22110555\n",
      "Iteration 23, loss = 0.21853294\n",
      "Iteration 24, loss = 0.21770433\n",
      "Iteration 25, loss = 0.21713024\n",
      "Iteration 26, loss = 0.21702479\n",
      "Iteration 27, loss = 0.21617996\n",
      "Iteration 28, loss = 0.21568742\n",
      "Iteration 29, loss = 0.21579534\n",
      "Iteration 30, loss = 0.21343167\n",
      "Iteration 31, loss = 0.21259866\n",
      "Iteration 32, loss = 0.21301724\n",
      "Iteration 33, loss = 0.21071742\n",
      "Iteration 34, loss = 0.21089851\n",
      "Iteration 35, loss = 0.21018176\n",
      "Iteration 36, loss = 0.21085623\n",
      "Iteration 37, loss = 0.21011744\n",
      "Iteration 38, loss = 0.20834672\n",
      "Iteration 39, loss = 0.20897094\n",
      "Iteration 40, loss = 0.20714349\n",
      "Iteration 41, loss = 0.20825043\n",
      "Iteration 42, loss = 0.20565168\n",
      "Iteration 43, loss = 0.20748014\n",
      "Iteration 44, loss = 0.20481497\n",
      "Iteration 45, loss = 0.20423407\n",
      "Iteration 46, loss = 0.20487014\n",
      "Iteration 47, loss = 0.20404894\n",
      "Iteration 48, loss = 0.20579878\n",
      "Iteration 49, loss = 0.20247682\n",
      "Iteration 50, loss = 0.20139473\n",
      "Iteration 1, loss = 0.30591633\n",
      "Iteration 2, loss = 0.25333730\n",
      "Iteration 3, loss = 0.24457622\n",
      "Iteration 4, loss = 0.24072346\n",
      "Iteration 5, loss = 0.23800601\n",
      "Iteration 6, loss = 0.23575244\n",
      "Iteration 7, loss = 0.23446908\n",
      "Iteration 8, loss = 0.23265270\n",
      "Iteration 9, loss = 0.23100334\n",
      "Iteration 10, loss = 0.22936766\n",
      "Iteration 11, loss = 0.22780363\n",
      "Iteration 12, loss = 0.22627363\n",
      "Iteration 13, loss = 0.22487012\n",
      "Iteration 14, loss = 0.22450735\n",
      "Iteration 15, loss = 0.22458920\n",
      "Iteration 16, loss = 0.22410557\n",
      "Iteration 17, loss = 0.22382106\n",
      "Iteration 18, loss = 0.22120295\n",
      "Iteration 19, loss = 0.21949390\n",
      "Iteration 20, loss = 0.21836318\n",
      "Iteration 21, loss = 0.21807114\n",
      "Iteration 22, loss = 0.21820058\n",
      "Iteration 23, loss = 0.21789933\n",
      "Iteration 24, loss = 0.21624376\n",
      "Iteration 25, loss = 0.21556746\n",
      "Iteration 26, loss = 0.21479475\n",
      "Iteration 27, loss = 0.21430578\n",
      "Iteration 28, loss = 0.21277528\n",
      "Iteration 29, loss = 0.21300711\n",
      "Iteration 30, loss = 0.21170485\n",
      "Iteration 31, loss = 0.21257884\n",
      "Iteration 32, loss = 0.21009785\n",
      "Iteration 33, loss = 0.20969332\n",
      "Iteration 34, loss = 0.20973972\n",
      "Iteration 35, loss = 0.20854578\n",
      "Iteration 36, loss = 0.20728105\n",
      "Iteration 37, loss = 0.20903594\n",
      "Iteration 38, loss = 0.20683992\n",
      "Iteration 39, loss = 0.20730516\n",
      "Iteration 40, loss = 0.20562709\n",
      "Iteration 41, loss = 0.20450616\n",
      "Iteration 42, loss = 0.20405233\n",
      "Iteration 43, loss = 0.20470875\n",
      "Iteration 44, loss = 0.20327221\n",
      "Iteration 45, loss = 0.20305268\n",
      "Iteration 46, loss = 0.20173617\n",
      "Iteration 47, loss = 0.20165290\n",
      "Iteration 48, loss = 0.20205580\n",
      "Iteration 49, loss = 0.20066290\n",
      "Iteration 50, loss = 0.20015563\n",
      "Iteration 1, loss = 0.30728899\n",
      "Iteration 2, loss = 0.25750685\n",
      "Iteration 3, loss = 0.24897862\n",
      "Iteration 4, loss = 0.24319162\n",
      "Iteration 5, loss = 0.24332696\n",
      "Iteration 6, loss = 0.23803712\n",
      "Iteration 7, loss = 0.23616946\n",
      "Iteration 8, loss = 0.23460789\n",
      "Iteration 9, loss = 0.23292508\n",
      "Iteration 10, loss = 0.23194354\n",
      "Iteration 11, loss = 0.22952775\n",
      "Iteration 12, loss = 0.22848834\n",
      "Iteration 13, loss = 0.22908241\n",
      "Iteration 14, loss = 0.22651075\n",
      "Iteration 15, loss = 0.22490752\n",
      "Iteration 16, loss = 0.22431022\n",
      "Iteration 17, loss = 0.22331316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.22282399\n",
      "Iteration 19, loss = 0.22153092\n",
      "Iteration 20, loss = 0.22085885\n",
      "Iteration 21, loss = 0.21945267\n",
      "Iteration 22, loss = 0.21916372\n",
      "Iteration 23, loss = 0.21732648\n",
      "Iteration 24, loss = 0.21769811\n",
      "Iteration 25, loss = 0.21585546\n",
      "Iteration 26, loss = 0.21492193\n",
      "Iteration 27, loss = 0.21496390\n",
      "Iteration 28, loss = 0.21432815\n",
      "Iteration 29, loss = 0.21299338\n",
      "Iteration 30, loss = 0.21259170\n",
      "Iteration 31, loss = 0.21147061\n",
      "Iteration 32, loss = 0.21191255\n",
      "Iteration 33, loss = 0.21085768\n",
      "Iteration 34, loss = 0.21053253\n",
      "Iteration 35, loss = 0.20941008\n",
      "Iteration 36, loss = 0.20822936\n",
      "Iteration 37, loss = 0.20810417\n",
      "Iteration 38, loss = 0.20748707\n",
      "Iteration 39, loss = 0.20623061\n",
      "Iteration 40, loss = 0.20678623\n",
      "Iteration 41, loss = 0.20593953\n",
      "Iteration 42, loss = 0.20331110\n",
      "Iteration 43, loss = 0.20201598\n",
      "Iteration 44, loss = 0.20291603\n",
      "Iteration 45, loss = 0.20175067\n",
      "Iteration 46, loss = 0.20334150\n",
      "Iteration 47, loss = 0.20245909\n",
      "Iteration 48, loss = 0.20163122\n",
      "Iteration 49, loss = 0.20019886\n",
      "Iteration 50, loss = 0.20020747\n",
      "Train Accuracy: 0.8861\n",
      "Running Memory: 0.05 MB\n",
      "Running Time: 124.04 Seconds\n",
      "Test  Accuracy: 0.8779\n",
      "Test  Presion:  0.8431\n",
      "Test  Recall:   0.9382\n",
      "Test  F1-score: 0.8881\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS1_bow_2, memory_MLP_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS1_bow_2, acc_MLP_DS1_bow_2, pre_MLP_DS1_bow_2, rec_MLP_DS1_bow_2, f1_MLP_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS1_bow_2, model_3, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.46900733\n",
      "Iteration 2, loss = 0.43537452\n",
      "Iteration 3, loss = 0.43379877\n",
      "Iteration 4, loss = 0.43077870\n",
      "Iteration 5, loss = 0.43000889\n",
      "Iteration 6, loss = 0.42957323\n",
      "Iteration 7, loss = 0.42801963\n",
      "Iteration 8, loss = 0.42778455\n",
      "Iteration 9, loss = 0.42712478\n",
      "Iteration 10, loss = 0.42624567\n",
      "Iteration 11, loss = 0.42660381\n",
      "Iteration 12, loss = 0.42687993\n",
      "Iteration 13, loss = 0.42543574\n",
      "Iteration 14, loss = 0.42512543\n",
      "Iteration 15, loss = 0.42550348\n",
      "Iteration 16, loss = 0.42473082\n",
      "Iteration 17, loss = 0.42421734\n",
      "Iteration 18, loss = 0.42422708\n",
      "Iteration 19, loss = 0.42391078\n",
      "Iteration 20, loss = 0.42340354\n",
      "Iteration 21, loss = 0.42313503\n",
      "Iteration 22, loss = 0.42319398\n",
      "Iteration 23, loss = 0.42360607\n",
      "Iteration 24, loss = 0.42259520\n",
      "Iteration 25, loss = 0.42317536\n",
      "Iteration 26, loss = 0.42164338\n",
      "Iteration 27, loss = 0.42230054\n",
      "Iteration 28, loss = 0.42227293\n",
      "Iteration 29, loss = 0.42188515\n",
      "Iteration 30, loss = 0.42152035\n",
      "Iteration 31, loss = 0.42139623\n",
      "Iteration 32, loss = 0.42033278\n",
      "Iteration 33, loss = 0.42077812\n",
      "Iteration 34, loss = 0.42041636\n",
      "Iteration 35, loss = 0.42127023\n",
      "Iteration 36, loss = 0.42023897\n",
      "Iteration 37, loss = 0.42004006\n",
      "Iteration 38, loss = 0.42078683\n",
      "Iteration 39, loss = 0.42006144\n",
      "Iteration 40, loss = 0.41977456\n",
      "Iteration 41, loss = 0.41992001\n",
      "Iteration 42, loss = 0.41938477\n",
      "Iteration 43, loss = 0.41980553\n",
      "Iteration 44, loss = 0.41946835\n",
      "Iteration 45, loss = 0.41875081\n",
      "Iteration 46, loss = 0.41886112\n",
      "Iteration 47, loss = 0.41913290\n",
      "Iteration 48, loss = 0.41857119\n",
      "Iteration 49, loss = 0.41832211\n",
      "Iteration 50, loss = 0.41885544\n",
      "Iteration 1, loss = 0.47401758\n",
      "Iteration 2, loss = 0.43590629\n",
      "Iteration 3, loss = 0.43339690\n",
      "Iteration 4, loss = 0.43214878\n",
      "Iteration 5, loss = 0.43182549\n",
      "Iteration 6, loss = 0.43049027\n",
      "Iteration 7, loss = 0.42954793\n",
      "Iteration 8, loss = 0.42902446\n",
      "Iteration 9, loss = 0.42691997\n",
      "Iteration 10, loss = 0.42863825\n",
      "Iteration 11, loss = 0.42779223\n",
      "Iteration 12, loss = 0.42615971\n",
      "Iteration 13, loss = 0.42644262\n",
      "Iteration 14, loss = 0.42628115\n",
      "Iteration 15, loss = 0.42639020\n",
      "Iteration 16, loss = 0.42531449\n",
      "Iteration 17, loss = 0.42559137\n",
      "Iteration 18, loss = 0.42465961\n",
      "Iteration 19, loss = 0.42478421\n",
      "Iteration 20, loss = 0.42407274\n",
      "Iteration 21, loss = 0.42411685\n",
      "Iteration 22, loss = 0.42399154\n",
      "Iteration 23, loss = 0.42388625\n",
      "Iteration 24, loss = 0.42403277\n",
      "Iteration 25, loss = 0.42401117\n",
      "Iteration 26, loss = 0.42377289\n",
      "Iteration 27, loss = 0.42298619\n",
      "Iteration 28, loss = 0.42269123\n",
      "Iteration 29, loss = 0.42301936\n",
      "Iteration 30, loss = 0.42174971\n",
      "Iteration 31, loss = 0.42250412\n",
      "Iteration 32, loss = 0.42164554\n",
      "Iteration 33, loss = 0.42133454\n",
      "Iteration 34, loss = 0.42173879\n",
      "Iteration 35, loss = 0.42141836\n",
      "Iteration 36, loss = 0.42136520\n",
      "Iteration 37, loss = 0.42115969\n",
      "Iteration 38, loss = 0.42099309\n",
      "Iteration 39, loss = 0.42072995\n",
      "Iteration 40, loss = 0.42106656\n",
      "Iteration 41, loss = 0.42100452\n",
      "Iteration 42, loss = 0.42082813\n",
      "Iteration 43, loss = 0.42063077\n",
      "Iteration 44, loss = 0.42043249\n",
      "Iteration 45, loss = 0.42019492\n",
      "Iteration 46, loss = 0.42047254\n",
      "Iteration 47, loss = 0.42028274\n",
      "Iteration 48, loss = 0.41994631\n",
      "Iteration 49, loss = 0.42026725\n",
      "Iteration 50, loss = 0.41989951\n",
      "Iteration 1, loss = 0.47195128\n",
      "Iteration 2, loss = 0.43642559\n",
      "Iteration 3, loss = 0.43405427\n",
      "Iteration 4, loss = 0.43147266\n",
      "Iteration 5, loss = 0.42942852\n",
      "Iteration 6, loss = 0.42876343\n",
      "Iteration 7, loss = 0.42775788\n",
      "Iteration 8, loss = 0.42885344\n",
      "Iteration 9, loss = 0.42747060\n",
      "Iteration 10, loss = 0.42726212\n",
      "Iteration 11, loss = 0.42594628\n",
      "Iteration 12, loss = 0.42592783\n",
      "Iteration 13, loss = 0.42511987\n",
      "Iteration 14, loss = 0.42520374\n",
      "Iteration 15, loss = 0.42450721\n",
      "Iteration 16, loss = 0.42362016\n",
      "Iteration 17, loss = 0.42406960\n",
      "Iteration 18, loss = 0.42384160\n",
      "Iteration 19, loss = 0.42418126\n",
      "Iteration 20, loss = 0.42373278\n",
      "Iteration 21, loss = 0.42315606\n",
      "Iteration 22, loss = 0.42350545\n",
      "Iteration 23, loss = 0.42258282\n",
      "Iteration 24, loss = 0.42206917\n",
      "Iteration 25, loss = 0.42314129\n",
      "Iteration 26, loss = 0.42322686\n",
      "Iteration 27, loss = 0.42220071\n",
      "Iteration 28, loss = 0.42168584\n",
      "Iteration 29, loss = 0.42250911\n",
      "Iteration 30, loss = 0.42165586\n",
      "Iteration 31, loss = 0.42063080\n",
      "Iteration 32, loss = 0.42161718\n",
      "Iteration 33, loss = 0.42113615\n",
      "Iteration 34, loss = 0.42138165\n",
      "Iteration 35, loss = 0.42067166\n",
      "Iteration 36, loss = 0.42067430\n",
      "Iteration 37, loss = 0.42037288\n",
      "Iteration 38, loss = 0.42065414\n",
      "Iteration 39, loss = 0.42057551\n",
      "Iteration 40, loss = 0.41962104\n",
      "Iteration 41, loss = 0.42066723\n",
      "Iteration 42, loss = 0.42019282\n",
      "Iteration 43, loss = 0.41970707\n",
      "Iteration 44, loss = 0.41984068\n",
      "Iteration 45, loss = 0.41960261\n",
      "Iteration 46, loss = 0.41938089\n",
      "Iteration 47, loss = 0.41898108\n",
      "Iteration 48, loss = 0.41932882\n",
      "Iteration 49, loss = 0.41901153\n",
      "Iteration 50, loss = 0.41930189\n",
      "Iteration 1, loss = 0.47355799\n",
      "Iteration 2, loss = 0.43721561\n",
      "Iteration 3, loss = 0.43406539\n",
      "Iteration 4, loss = 0.43322018\n",
      "Iteration 5, loss = 0.43129476\n",
      "Iteration 6, loss = 0.43042453\n",
      "Iteration 7, loss = 0.42980443\n",
      "Iteration 8, loss = 0.42910597\n",
      "Iteration 9, loss = 0.42839622\n",
      "Iteration 10, loss = 0.42781639\n",
      "Iteration 11, loss = 0.42698525\n",
      "Iteration 12, loss = 0.42656905\n",
      "Iteration 13, loss = 0.42681468\n",
      "Iteration 14, loss = 0.42607690\n",
      "Iteration 15, loss = 0.42595701\n",
      "Iteration 16, loss = 0.42483079\n",
      "Iteration 17, loss = 0.42558664\n",
      "Iteration 18, loss = 0.42442815\n",
      "Iteration 19, loss = 0.42461671\n",
      "Iteration 20, loss = 0.42453821\n",
      "Iteration 21, loss = 0.42520451\n",
      "Iteration 22, loss = 0.42267674\n",
      "Iteration 23, loss = 0.42443009\n",
      "Iteration 24, loss = 0.42395128\n",
      "Iteration 25, loss = 0.42310605\n",
      "Iteration 26, loss = 0.42289944\n",
      "Iteration 27, loss = 0.42277088\n",
      "Iteration 28, loss = 0.42253569\n",
      "Iteration 29, loss = 0.42245121\n",
      "Iteration 30, loss = 0.42215518\n",
      "Iteration 31, loss = 0.42181544\n",
      "Iteration 32, loss = 0.42167842\n",
      "Iteration 33, loss = 0.42116628\n",
      "Iteration 34, loss = 0.42136780\n",
      "Iteration 35, loss = 0.42107367\n",
      "Iteration 36, loss = 0.42194238\n",
      "Iteration 37, loss = 0.42168572\n",
      "Iteration 38, loss = 0.42124993\n",
      "Iteration 39, loss = 0.42050678\n",
      "Iteration 40, loss = 0.42054546\n",
      "Iteration 41, loss = 0.42118350\n",
      "Iteration 42, loss = 0.42078249\n",
      "Iteration 43, loss = 0.42054039\n",
      "Iteration 44, loss = 0.42105110\n",
      "Iteration 45, loss = 0.42007346\n",
      "Iteration 46, loss = 0.42028533\n",
      "Iteration 47, loss = 0.42042943\n",
      "Iteration 48, loss = 0.42058163\n",
      "Iteration 49, loss = 0.41985299\n",
      "Iteration 50, loss = 0.42011939\n",
      "Iteration 1, loss = 0.47263622\n",
      "Iteration 2, loss = 0.43584404\n",
      "Iteration 3, loss = 0.43169029\n",
      "Iteration 4, loss = 0.43194398\n",
      "Iteration 5, loss = 0.42912079\n",
      "Iteration 6, loss = 0.42696505\n",
      "Iteration 7, loss = 0.42696325\n",
      "Iteration 8, loss = 0.42668008\n",
      "Iteration 9, loss = 0.42597147\n",
      "Iteration 10, loss = 0.42556229\n",
      "Iteration 11, loss = 0.42511627\n",
      "Iteration 12, loss = 0.42461843\n",
      "Iteration 13, loss = 0.42438181\n",
      "Iteration 14, loss = 0.42461644\n",
      "Iteration 15, loss = 0.42339084\n",
      "Iteration 16, loss = 0.42379331\n",
      "Iteration 17, loss = 0.42291477\n",
      "Iteration 18, loss = 0.42361637\n",
      "Iteration 19, loss = 0.42246134\n",
      "Iteration 20, loss = 0.42167725\n",
      "Iteration 21, loss = 0.42177237\n",
      "Iteration 22, loss = 0.42205063\n",
      "Iteration 23, loss = 0.42273307\n",
      "Iteration 24, loss = 0.42169899\n",
      "Iteration 25, loss = 0.42167936\n",
      "Iteration 26, loss = 0.42085705\n",
      "Iteration 27, loss = 0.42084655\n",
      "Iteration 28, loss = 0.42075336\n",
      "Iteration 29, loss = 0.42101651\n",
      "Iteration 30, loss = 0.42069005\n",
      "Iteration 31, loss = 0.41989037\n",
      "Iteration 32, loss = 0.42036956\n",
      "Iteration 33, loss = 0.41984047\n",
      "Iteration 34, loss = 0.41990751\n",
      "Iteration 35, loss = 0.41996354\n",
      "Iteration 36, loss = 0.41921176\n",
      "Iteration 37, loss = 0.41919270\n",
      "Iteration 38, loss = 0.41910138\n",
      "Iteration 39, loss = 0.41941948\n",
      "Iteration 40, loss = 0.41881137\n",
      "Iteration 41, loss = 0.41888996\n",
      "Iteration 42, loss = 0.41896453\n",
      "Iteration 43, loss = 0.41866075\n",
      "Iteration 44, loss = 0.41856200\n",
      "Iteration 45, loss = 0.41789161\n",
      "Iteration 46, loss = 0.41844724\n",
      "Iteration 47, loss = 0.41791811\n",
      "Iteration 48, loss = 0.41781638\n",
      "Iteration 49, loss = 0.41814998\n",
      "Iteration 50, loss = 0.41733475\n",
      "Iteration 1, loss = 0.47211946\n",
      "Iteration 2, loss = 0.43461550\n",
      "Iteration 3, loss = 0.43180815\n",
      "Iteration 4, loss = 0.43156944\n",
      "Iteration 5, loss = 0.42948589\n",
      "Iteration 6, loss = 0.42993310\n",
      "Iteration 7, loss = 0.42910735\n",
      "Iteration 8, loss = 0.42701910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.42780808\n",
      "Iteration 10, loss = 0.42657239\n",
      "Iteration 11, loss = 0.42656690\n",
      "Iteration 12, loss = 0.42552212\n",
      "Iteration 13, loss = 0.42535596\n",
      "Iteration 14, loss = 0.42574116\n",
      "Iteration 15, loss = 0.42411734\n",
      "Iteration 16, loss = 0.42413966\n",
      "Iteration 17, loss = 0.42399484\n",
      "Iteration 18, loss = 0.42415965\n",
      "Iteration 19, loss = 0.42347526\n",
      "Iteration 20, loss = 0.42341510\n",
      "Iteration 21, loss = 0.42301775\n",
      "Iteration 22, loss = 0.42275564\n",
      "Iteration 23, loss = 0.42172858\n",
      "Iteration 24, loss = 0.42251303\n",
      "Iteration 25, loss = 0.42254355\n",
      "Iteration 26, loss = 0.42277545\n",
      "Iteration 27, loss = 0.42165030\n",
      "Iteration 28, loss = 0.42193985\n",
      "Iteration 29, loss = 0.42093112\n",
      "Iteration 30, loss = 0.42070926\n",
      "Iteration 31, loss = 0.42140608\n",
      "Iteration 32, loss = 0.42087631\n",
      "Iteration 33, loss = 0.42121667\n",
      "Iteration 34, loss = 0.42040164\n",
      "Iteration 35, loss = 0.42083632\n",
      "Iteration 36, loss = 0.42028331\n",
      "Iteration 37, loss = 0.41979326\n",
      "Iteration 38, loss = 0.42061867\n",
      "Iteration 39, loss = 0.42009658\n",
      "Iteration 40, loss = 0.41932751\n",
      "Iteration 41, loss = 0.41960313\n",
      "Iteration 42, loss = 0.41934218\n",
      "Iteration 43, loss = 0.41909328\n",
      "Iteration 44, loss = 0.41976619\n",
      "Iteration 45, loss = 0.41926845\n",
      "Iteration 46, loss = 0.41895598\n",
      "Iteration 47, loss = 0.41890488\n",
      "Iteration 48, loss = 0.41852911\n",
      "Iteration 49, loss = 0.41877806\n",
      "Iteration 50, loss = 0.41827049\n",
      "Iteration 1, loss = 0.47203760\n",
      "Iteration 2, loss = 0.43580162\n",
      "Iteration 3, loss = 0.43328404\n",
      "Iteration 4, loss = 0.43138943\n",
      "Iteration 5, loss = 0.43108155\n",
      "Iteration 6, loss = 0.42916281\n",
      "Iteration 7, loss = 0.42835555\n",
      "Iteration 8, loss = 0.42874864\n",
      "Iteration 9, loss = 0.42826857\n",
      "Iteration 10, loss = 0.42765204\n",
      "Iteration 11, loss = 0.42686488\n",
      "Iteration 12, loss = 0.42583393\n",
      "Iteration 13, loss = 0.42588109\n",
      "Iteration 14, loss = 0.42555018\n",
      "Iteration 15, loss = 0.42554542\n",
      "Iteration 16, loss = 0.42567852\n",
      "Iteration 17, loss = 0.42597436\n",
      "Iteration 18, loss = 0.42468001\n",
      "Iteration 19, loss = 0.42415837\n",
      "Iteration 20, loss = 0.42401201\n",
      "Iteration 21, loss = 0.42343225\n",
      "Iteration 22, loss = 0.42423209\n",
      "Iteration 23, loss = 0.42388003\n",
      "Iteration 24, loss = 0.42392839\n",
      "Iteration 25, loss = 0.42355984\n",
      "Iteration 26, loss = 0.42280421\n",
      "Iteration 27, loss = 0.42322216\n",
      "Iteration 28, loss = 0.42286990\n",
      "Iteration 29, loss = 0.42294693\n",
      "Iteration 30, loss = 0.42232952\n",
      "Iteration 31, loss = 0.42211207\n",
      "Iteration 32, loss = 0.42178158\n",
      "Iteration 33, loss = 0.42148232\n",
      "Iteration 34, loss = 0.42213553\n",
      "Iteration 35, loss = 0.42149709\n",
      "Iteration 36, loss = 0.42157496\n",
      "Iteration 37, loss = 0.42106236\n",
      "Iteration 38, loss = 0.42182375\n",
      "Iteration 39, loss = 0.42098231\n",
      "Iteration 40, loss = 0.42080804\n",
      "Iteration 41, loss = 0.42060649\n",
      "Iteration 42, loss = 0.42017901\n",
      "Iteration 43, loss = 0.42054561\n",
      "Iteration 44, loss = 0.42025459\n",
      "Iteration 45, loss = 0.41984603\n",
      "Iteration 46, loss = 0.42011857\n",
      "Iteration 47, loss = 0.41984964\n",
      "Iteration 48, loss = 0.41998178\n",
      "Iteration 49, loss = 0.41955129\n",
      "Iteration 50, loss = 0.41936697\n",
      "Iteration 1, loss = 0.47117951\n",
      "Iteration 2, loss = 0.43559087\n",
      "Iteration 3, loss = 0.43253683\n",
      "Iteration 4, loss = 0.43200228\n",
      "Iteration 5, loss = 0.42951877\n",
      "Iteration 6, loss = 0.42850884\n",
      "Iteration 7, loss = 0.42786367\n",
      "Iteration 8, loss = 0.42691959\n",
      "Iteration 9, loss = 0.42720126\n",
      "Iteration 10, loss = 0.42613153\n",
      "Iteration 11, loss = 0.42673796\n",
      "Iteration 12, loss = 0.42511131\n",
      "Iteration 13, loss = 0.42499356\n",
      "Iteration 14, loss = 0.42533597\n",
      "Iteration 15, loss = 0.42461249\n",
      "Iteration 16, loss = 0.42456322\n",
      "Iteration 17, loss = 0.42412838\n",
      "Iteration 18, loss = 0.42343278\n",
      "Iteration 19, loss = 0.42361670\n",
      "Iteration 20, loss = 0.42318627\n",
      "Iteration 21, loss = 0.42251080\n",
      "Iteration 22, loss = 0.42297395\n",
      "Iteration 23, loss = 0.42277269\n",
      "Iteration 24, loss = 0.42209862\n",
      "Iteration 25, loss = 0.42233348\n",
      "Iteration 26, loss = 0.42175869\n",
      "Iteration 27, loss = 0.42112261\n",
      "Iteration 28, loss = 0.42138566\n",
      "Iteration 29, loss = 0.42062963\n",
      "Iteration 30, loss = 0.42099288\n",
      "Iteration 31, loss = 0.42075380\n",
      "Iteration 32, loss = 0.42103227\n",
      "Iteration 33, loss = 0.42098760\n",
      "Iteration 34, loss = 0.42035465\n",
      "Iteration 35, loss = 0.42013605\n",
      "Iteration 36, loss = 0.41976614\n",
      "Iteration 37, loss = 0.42001818\n",
      "Iteration 38, loss = 0.42023703\n",
      "Iteration 39, loss = 0.41933284\n",
      "Iteration 40, loss = 0.41915848\n",
      "Iteration 41, loss = 0.41925782\n",
      "Iteration 42, loss = 0.41960503\n",
      "Iteration 43, loss = 0.41959145\n",
      "Iteration 44, loss = 0.41899042\n",
      "Iteration 45, loss = 0.41880270\n",
      "Iteration 46, loss = 0.41854126\n",
      "Iteration 47, loss = 0.41845518\n",
      "Iteration 48, loss = 0.41917125\n",
      "Iteration 49, loss = 0.41886471\n",
      "Iteration 50, loss = 0.41849018\n",
      "Iteration 1, loss = 0.46999687\n",
      "Iteration 2, loss = 0.43450167\n",
      "Iteration 3, loss = 0.43071280\n",
      "Iteration 4, loss = 0.42927276\n",
      "Iteration 5, loss = 0.43017210\n",
      "Iteration 6, loss = 0.42736208\n",
      "Iteration 7, loss = 0.42704672\n",
      "Iteration 8, loss = 0.42700724\n",
      "Iteration 9, loss = 0.42547282\n",
      "Iteration 10, loss = 0.42526470\n",
      "Iteration 11, loss = 0.42487020\n",
      "Iteration 12, loss = 0.42472824\n",
      "Iteration 13, loss = 0.42340642\n",
      "Iteration 14, loss = 0.42374496\n",
      "Iteration 15, loss = 0.42405619\n",
      "Iteration 16, loss = 0.42288107\n",
      "Iteration 17, loss = 0.42298391\n",
      "Iteration 18, loss = 0.42274409\n",
      "Iteration 19, loss = 0.42207429\n",
      "Iteration 20, loss = 0.42255117\n",
      "Iteration 21, loss = 0.42173713\n",
      "Iteration 22, loss = 0.42164685\n",
      "Iteration 23, loss = 0.42112470\n",
      "Iteration 24, loss = 0.42119184\n",
      "Iteration 25, loss = 0.42145222\n",
      "Iteration 26, loss = 0.42017013\n",
      "Iteration 27, loss = 0.42102505\n",
      "Iteration 28, loss = 0.42036339\n",
      "Iteration 29, loss = 0.42097195\n",
      "Iteration 30, loss = 0.42000604\n",
      "Iteration 31, loss = 0.41996601\n",
      "Iteration 32, loss = 0.42013589\n",
      "Iteration 33, loss = 0.41921972\n",
      "Iteration 34, loss = 0.41948134\n",
      "Iteration 35, loss = 0.41958541\n",
      "Iteration 36, loss = 0.41888701\n",
      "Iteration 37, loss = 0.41860579\n",
      "Iteration 38, loss = 0.41854035\n",
      "Iteration 39, loss = 0.41842887\n",
      "Iteration 40, loss = 0.41823235\n",
      "Iteration 41, loss = 0.41844399\n",
      "Iteration 42, loss = 0.41823386\n",
      "Iteration 43, loss = 0.41778336\n",
      "Iteration 44, loss = 0.41745039\n",
      "Iteration 45, loss = 0.41765516\n",
      "Iteration 46, loss = 0.41755701\n",
      "Iteration 47, loss = 0.41747081\n",
      "Iteration 48, loss = 0.41770584\n",
      "Iteration 49, loss = 0.41727877\n",
      "Iteration 50, loss = 0.41678632\n",
      "Iteration 1, loss = 0.47333208\n",
      "Iteration 2, loss = 0.43632271\n",
      "Iteration 3, loss = 0.43252862\n",
      "Iteration 4, loss = 0.43154334\n",
      "Iteration 5, loss = 0.43072808\n",
      "Iteration 6, loss = 0.42886899\n",
      "Iteration 7, loss = 0.42898815\n",
      "Iteration 8, loss = 0.42756951\n",
      "Iteration 9, loss = 0.42707360\n",
      "Iteration 10, loss = 0.42680187\n",
      "Iteration 11, loss = 0.42616775\n",
      "Iteration 12, loss = 0.42600992\n",
      "Iteration 13, loss = 0.42537536\n",
      "Iteration 14, loss = 0.42515249\n",
      "Iteration 15, loss = 0.42530693\n",
      "Iteration 16, loss = 0.42506328\n",
      "Iteration 17, loss = 0.42472898\n",
      "Iteration 18, loss = 0.42465791\n",
      "Iteration 19, loss = 0.42475091\n",
      "Iteration 20, loss = 0.42325740\n",
      "Iteration 21, loss = 0.42346996\n",
      "Iteration 22, loss = 0.42291152\n",
      "Iteration 23, loss = 0.42289756\n",
      "Iteration 24, loss = 0.42253347\n",
      "Iteration 25, loss = 0.42301443\n",
      "Iteration 26, loss = 0.42245857\n",
      "Iteration 27, loss = 0.42233175\n",
      "Iteration 28, loss = 0.42218932\n",
      "Iteration 29, loss = 0.42129808\n",
      "Iteration 30, loss = 0.42212600\n",
      "Iteration 31, loss = 0.42132159\n",
      "Iteration 32, loss = 0.42090683\n",
      "Iteration 33, loss = 0.42084177\n",
      "Iteration 34, loss = 0.42174058\n",
      "Iteration 35, loss = 0.42055425\n",
      "Iteration 36, loss = 0.42071313\n",
      "Iteration 37, loss = 0.42028625\n",
      "Iteration 38, loss = 0.42042747\n",
      "Iteration 39, loss = 0.42007136\n",
      "Iteration 40, loss = 0.42009280\n",
      "Iteration 41, loss = 0.41985271\n",
      "Iteration 42, loss = 0.41967967\n",
      "Iteration 43, loss = 0.41934373\n",
      "Iteration 44, loss = 0.41941945\n",
      "Iteration 45, loss = 0.41904141\n",
      "Iteration 46, loss = 0.41914887\n",
      "Iteration 47, loss = 0.41901133\n",
      "Iteration 48, loss = 0.41926932\n",
      "Iteration 49, loss = 0.41934038\n",
      "Iteration 50, loss = 0.41903024\n",
      "Iteration 1, loss = 0.47140167\n",
      "Iteration 2, loss = 0.43671948\n",
      "Iteration 3, loss = 0.43329920\n",
      "Iteration 4, loss = 0.43199417\n",
      "Iteration 5, loss = 0.43164588\n",
      "Iteration 6, loss = 0.42990414\n",
      "Iteration 7, loss = 0.42888144\n",
      "Iteration 8, loss = 0.42826104\n",
      "Iteration 9, loss = 0.42729000\n",
      "Iteration 10, loss = 0.42681982\n",
      "Iteration 11, loss = 0.42719481\n",
      "Iteration 12, loss = 0.42682932\n",
      "Iteration 13, loss = 0.42599215\n",
      "Iteration 14, loss = 0.42604349\n",
      "Iteration 15, loss = 0.42492161\n",
      "Iteration 16, loss = 0.42465050\n",
      "Iteration 17, loss = 0.42494650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.42394452\n",
      "Iteration 19, loss = 0.42392477\n",
      "Iteration 20, loss = 0.42398804\n",
      "Iteration 21, loss = 0.42395272\n",
      "Iteration 22, loss = 0.42292443\n",
      "Iteration 23, loss = 0.42354013\n",
      "Iteration 24, loss = 0.42241118\n",
      "Iteration 25, loss = 0.42318648\n",
      "Iteration 26, loss = 0.42275811\n",
      "Iteration 27, loss = 0.42260817\n",
      "Iteration 28, loss = 0.42247028\n",
      "Iteration 29, loss = 0.42195538\n",
      "Iteration 30, loss = 0.42186671\n",
      "Iteration 31, loss = 0.42232974\n",
      "Iteration 32, loss = 0.42071335\n",
      "Iteration 33, loss = 0.42172683\n",
      "Iteration 34, loss = 0.42116588\n",
      "Iteration 35, loss = 0.42101176\n",
      "Iteration 36, loss = 0.42124440\n",
      "Iteration 37, loss = 0.42057874\n",
      "Iteration 38, loss = 0.42042925\n",
      "Iteration 39, loss = 0.41986654\n",
      "Iteration 40, loss = 0.42019364\n",
      "Iteration 41, loss = 0.42033751\n",
      "Iteration 42, loss = 0.41975085\n",
      "Iteration 43, loss = 0.41916104\n",
      "Iteration 44, loss = 0.41955029\n",
      "Iteration 45, loss = 0.41936018\n",
      "Iteration 46, loss = 0.41930238\n",
      "Iteration 47, loss = 0.41925550\n",
      "Iteration 48, loss = 0.41878894\n",
      "Iteration 49, loss = 0.41948072\n",
      "Iteration 50, loss = 0.41895026\n",
      "Train Accuracy: 0.7319\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 113.06 Seconds\n",
      "Test  Accuracy: 0.7154\n",
      "Test  Presion:  0.6561\n",
      "Test  Recall:   0.9441\n",
      "Test  F1-score: 0.7742\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS1_bow_3, memory_MLP_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS1_bow_3, acc_MLP_DS1_bow_3, pre_MLP_DS1_bow_3, rec_MLP_DS1_bow_3, f1_MLP_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS1_bow_3, model_3, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.19701872\n",
      "Iteration 2, loss = 0.05575562\n",
      "Iteration 3, loss = 0.04847260\n",
      "Iteration 4, loss = 0.04393710\n",
      "Iteration 5, loss = 0.04104780\n",
      "Iteration 6, loss = 0.03764490\n",
      "Iteration 7, loss = 0.03606758\n",
      "Iteration 8, loss = 0.03369157\n",
      "Iteration 9, loss = 0.03222800\n",
      "Iteration 10, loss = 0.03151140\n",
      "Iteration 11, loss = 0.03027621\n",
      "Iteration 12, loss = 0.02910134\n",
      "Iteration 13, loss = 0.03307784\n",
      "Iteration 14, loss = 0.02960876\n",
      "Iteration 15, loss = 0.02734724\n",
      "Iteration 16, loss = 0.02750764\n",
      "Iteration 17, loss = 0.02740917\n",
      "Iteration 18, loss = 0.02638641\n",
      "Iteration 19, loss = 0.02583965\n",
      "Iteration 20, loss = 0.02583967\n",
      "Iteration 21, loss = 0.02675961\n",
      "Iteration 22, loss = 0.02607254\n",
      "Iteration 23, loss = 0.02416062\n",
      "Iteration 24, loss = 0.02630861\n",
      "Iteration 25, loss = 0.02371564\n",
      "Iteration 26, loss = 0.02298468\n",
      "Iteration 27, loss = 0.02294152\n",
      "Iteration 28, loss = 0.02292414\n",
      "Iteration 29, loss = 0.02278739\n",
      "Iteration 30, loss = 0.02149942\n",
      "Iteration 31, loss = 0.02164079\n",
      "Iteration 32, loss = 0.02246890\n",
      "Iteration 33, loss = 0.02228567\n",
      "Iteration 34, loss = 0.02039839\n",
      "Iteration 35, loss = 0.01938800\n",
      "Iteration 36, loss = 0.02768301\n",
      "Iteration 37, loss = 0.02032205\n",
      "Iteration 38, loss = 0.01974775\n",
      "Iteration 39, loss = 0.01827764\n",
      "Iteration 40, loss = 0.01943551\n",
      "Iteration 41, loss = 0.01869266\n",
      "Iteration 42, loss = 0.01840182\n",
      "Iteration 43, loss = 0.01687041\n",
      "Iteration 44, loss = 0.01743752\n",
      "Iteration 45, loss = 0.01828594\n",
      "Iteration 46, loss = 0.01717175\n",
      "Iteration 47, loss = 0.01641609\n",
      "Iteration 48, loss = 0.01626742\n",
      "Iteration 49, loss = 0.01578505\n",
      "Iteration 50, loss = 0.01550022\n",
      "Iteration 1, loss = 0.20971745\n",
      "Iteration 2, loss = 0.05916594\n",
      "Iteration 3, loss = 0.04931120\n",
      "Iteration 4, loss = 0.04486148\n",
      "Iteration 5, loss = 0.04161170\n",
      "Iteration 6, loss = 0.03921540\n",
      "Iteration 7, loss = 0.03719041\n",
      "Iteration 8, loss = 0.03498163\n",
      "Iteration 9, loss = 0.03379475\n",
      "Iteration 10, loss = 0.03394767\n",
      "Iteration 11, loss = 0.03127435\n",
      "Iteration 12, loss = 0.03023138\n",
      "Iteration 13, loss = 0.02952815\n",
      "Iteration 14, loss = 0.02985568\n",
      "Iteration 15, loss = 0.02824007\n",
      "Iteration 16, loss = 0.02804750\n",
      "Iteration 17, loss = 0.02810644\n",
      "Iteration 18, loss = 0.02757847\n",
      "Iteration 19, loss = 0.02738629\n",
      "Iteration 20, loss = 0.02612289\n",
      "Iteration 21, loss = 0.02592697\n",
      "Iteration 22, loss = 0.02591265\n",
      "Iteration 23, loss = 0.02596739\n",
      "Iteration 24, loss = 0.02475053\n",
      "Iteration 25, loss = 0.02459374\n",
      "Iteration 26, loss = 0.02419800\n",
      "Iteration 27, loss = 0.02411497\n",
      "Iteration 28, loss = 0.02387866\n",
      "Iteration 29, loss = 0.02379912\n",
      "Iteration 30, loss = 0.02404810\n",
      "Iteration 31, loss = 0.02405464\n",
      "Iteration 32, loss = 0.02221417\n",
      "Iteration 33, loss = 0.02293261\n",
      "Iteration 34, loss = 0.02160706\n",
      "Iteration 35, loss = 0.02135669\n",
      "Iteration 36, loss = 0.02147278\n",
      "Iteration 37, loss = 0.02121069\n",
      "Iteration 38, loss = 0.02021490\n",
      "Iteration 39, loss = 0.02167630\n",
      "Iteration 40, loss = 0.02027229\n",
      "Iteration 41, loss = 0.02028044\n",
      "Iteration 42, loss = 0.01945558\n",
      "Iteration 43, loss = 0.01833176\n",
      "Iteration 44, loss = 0.01785080\n",
      "Iteration 45, loss = 0.01755898\n",
      "Iteration 46, loss = 0.01803520\n",
      "Iteration 47, loss = 0.01699686\n",
      "Iteration 48, loss = 0.01714048\n",
      "Iteration 49, loss = 0.01674773\n",
      "Iteration 50, loss = 0.01609334\n",
      "Iteration 1, loss = 0.21363989\n",
      "Iteration 2, loss = 0.05936624\n",
      "Iteration 3, loss = 0.05091514\n",
      "Iteration 4, loss = 0.04554531\n",
      "Iteration 5, loss = 0.04290138\n",
      "Iteration 6, loss = 0.04023931\n",
      "Iteration 7, loss = 0.03774537\n",
      "Iteration 8, loss = 0.03551319\n",
      "Iteration 9, loss = 0.03403293\n",
      "Iteration 10, loss = 0.03255078\n",
      "Iteration 11, loss = 0.03248298\n",
      "Iteration 12, loss = 0.03197727\n",
      "Iteration 13, loss = 0.02998980\n",
      "Iteration 14, loss = 0.02989159\n",
      "Iteration 15, loss = 0.02933643\n",
      "Iteration 16, loss = 0.02915771\n",
      "Iteration 17, loss = 0.02877534\n",
      "Iteration 18, loss = 0.02780480\n",
      "Iteration 19, loss = 0.02813700\n",
      "Iteration 20, loss = 0.02772259\n",
      "Iteration 21, loss = 0.02674029\n",
      "Iteration 22, loss = 0.02620295\n",
      "Iteration 23, loss = 0.02640340\n",
      "Iteration 24, loss = 0.02560390\n",
      "Iteration 25, loss = 0.02588675\n",
      "Iteration 26, loss = 0.02511708\n",
      "Iteration 27, loss = 0.02407577\n",
      "Iteration 28, loss = 0.02398991\n",
      "Iteration 29, loss = 0.02364752\n",
      "Iteration 30, loss = 0.02362360\n",
      "Iteration 31, loss = 0.02252060\n",
      "Iteration 32, loss = 0.02294226\n",
      "Iteration 33, loss = 0.02181040\n",
      "Iteration 34, loss = 0.02169461\n",
      "Iteration 35, loss = 0.02176741\n",
      "Iteration 36, loss = 0.02184278\n",
      "Iteration 37, loss = 0.02209881\n",
      "Iteration 38, loss = 0.02060874\n",
      "Iteration 39, loss = 0.02026572\n",
      "Iteration 40, loss = 0.01994148\n",
      "Iteration 41, loss = 0.01887712\n",
      "Iteration 42, loss = 0.01912837\n",
      "Iteration 43, loss = 0.01943129\n",
      "Iteration 44, loss = 0.01890719\n",
      "Iteration 45, loss = 0.01812051\n",
      "Iteration 46, loss = 0.01703560\n",
      "Iteration 47, loss = 0.01759858\n",
      "Iteration 48, loss = 0.01716450\n",
      "Iteration 49, loss = 0.01613487\n",
      "Iteration 50, loss = 0.01621387\n",
      "Iteration 1, loss = 0.20853849\n",
      "Iteration 2, loss = 0.05787998\n",
      "Iteration 3, loss = 0.05009745\n",
      "Iteration 4, loss = 0.04508284\n",
      "Iteration 5, loss = 0.04173426\n",
      "Iteration 6, loss = 0.03940729\n",
      "Iteration 7, loss = 0.03589820\n",
      "Iteration 8, loss = 0.03359075\n",
      "Iteration 9, loss = 0.03255277\n",
      "Iteration 10, loss = 0.03126752\n",
      "Iteration 11, loss = 0.03075401\n",
      "Iteration 12, loss = 0.02918437\n",
      "Iteration 13, loss = 0.02831016\n",
      "Iteration 14, loss = 0.02793107\n",
      "Iteration 15, loss = 0.02750545\n",
      "Iteration 16, loss = 0.02701121\n",
      "Iteration 17, loss = 0.02718689\n",
      "Iteration 18, loss = 0.02632460\n",
      "Iteration 19, loss = 0.02544416\n",
      "Iteration 20, loss = 0.02490458\n",
      "Iteration 21, loss = 0.02517790\n",
      "Iteration 22, loss = 0.02726331\n",
      "Iteration 23, loss = 0.02353171\n",
      "Iteration 24, loss = 0.02490595\n",
      "Iteration 25, loss = 0.02378630\n",
      "Iteration 26, loss = 0.02272337\n",
      "Iteration 27, loss = 0.02298042\n",
      "Iteration 28, loss = 0.02305690\n",
      "Iteration 29, loss = 0.02211601\n",
      "Iteration 30, loss = 0.02186415\n",
      "Iteration 31, loss = 0.02166920\n",
      "Iteration 32, loss = 0.02199780\n",
      "Iteration 33, loss = 0.02035420\n",
      "Iteration 34, loss = 0.01949393\n",
      "Iteration 35, loss = 0.02027837\n",
      "Iteration 36, loss = 0.01990612\n",
      "Iteration 37, loss = 0.01960584\n",
      "Iteration 38, loss = 0.01857694\n",
      "Iteration 39, loss = 0.01871301\n",
      "Iteration 40, loss = 0.01827916\n",
      "Iteration 41, loss = 0.01803164\n",
      "Iteration 42, loss = 0.01791776\n",
      "Iteration 43, loss = 0.01798680\n",
      "Iteration 44, loss = 0.01685924\n",
      "Iteration 45, loss = 0.01777942\n",
      "Iteration 46, loss = 0.01605491\n",
      "Iteration 47, loss = 0.01580925\n",
      "Iteration 48, loss = 0.01517679\n",
      "Iteration 49, loss = 0.01570411\n",
      "Iteration 50, loss = 0.01463132\n",
      "Iteration 1, loss = 0.21440752\n",
      "Iteration 2, loss = 0.05936085\n",
      "Iteration 3, loss = 0.04966310\n",
      "Iteration 4, loss = 0.04642860\n",
      "Iteration 5, loss = 0.04233979\n",
      "Iteration 6, loss = 0.04016284\n",
      "Iteration 7, loss = 0.03671924\n",
      "Iteration 8, loss = 0.03429404\n",
      "Iteration 9, loss = 0.03288692\n",
      "Iteration 10, loss = 0.03165723\n",
      "Iteration 11, loss = 0.03025763\n",
      "Iteration 12, loss = 0.03053283\n",
      "Iteration 13, loss = 0.02894092\n",
      "Iteration 14, loss = 0.02823470\n",
      "Iteration 15, loss = 0.02885428\n",
      "Iteration 16, loss = 0.02772161\n",
      "Iteration 17, loss = 0.02725018\n",
      "Iteration 18, loss = 0.02751694\n",
      "Iteration 19, loss = 0.02684392\n",
      "Iteration 20, loss = 0.02506695\n",
      "Iteration 21, loss = 0.02630841\n",
      "Iteration 22, loss = 0.02511285\n",
      "Iteration 23, loss = 0.02438111\n",
      "Iteration 24, loss = 0.02511600\n",
      "Iteration 25, loss = 0.02454925\n",
      "Iteration 26, loss = 0.02348053\n",
      "Iteration 27, loss = 0.02246149\n",
      "Iteration 28, loss = 0.02274744\n",
      "Iteration 29, loss = 0.02249641\n",
      "Iteration 30, loss = 0.02191756\n",
      "Iteration 31, loss = 0.02145706\n",
      "Iteration 32, loss = 0.02129591\n",
      "Iteration 33, loss = 0.02187521\n",
      "Iteration 34, loss = 0.01992601\n",
      "Iteration 35, loss = 0.02096525\n",
      "Iteration 36, loss = 0.02010242\n",
      "Iteration 37, loss = 0.01947516\n",
      "Iteration 38, loss = 0.01924636\n",
      "Iteration 39, loss = 0.01908293\n",
      "Iteration 40, loss = 0.01913575\n",
      "Iteration 41, loss = 0.01785471\n",
      "Iteration 42, loss = 0.01757567\n",
      "Iteration 43, loss = 0.01701843\n",
      "Iteration 44, loss = 0.01777703\n",
      "Iteration 45, loss = 0.01758339\n",
      "Iteration 46, loss = 0.01772166\n",
      "Iteration 47, loss = 0.01599259\n",
      "Iteration 48, loss = 0.01627485\n",
      "Iteration 49, loss = 0.01660284\n",
      "Iteration 50, loss = 0.01607816\n",
      "Iteration 1, loss = 0.21114818\n",
      "Iteration 2, loss = 0.05780715\n",
      "Iteration 3, loss = 0.05057822\n",
      "Iteration 4, loss = 0.04580273\n",
      "Iteration 5, loss = 0.04253722\n",
      "Iteration 6, loss = 0.03912910\n",
      "Iteration 7, loss = 0.03710544\n",
      "Iteration 8, loss = 0.03521140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.03349052\n",
      "Iteration 10, loss = 0.03210462\n",
      "Iteration 11, loss = 0.03112863\n",
      "Iteration 12, loss = 0.03061458\n",
      "Iteration 13, loss = 0.02964542\n",
      "Iteration 14, loss = 0.02945421\n",
      "Iteration 15, loss = 0.03130879\n",
      "Iteration 16, loss = 0.02771127\n",
      "Iteration 17, loss = 0.02836793\n",
      "Iteration 18, loss = 0.02687665\n",
      "Iteration 19, loss = 0.02698967\n",
      "Iteration 20, loss = 0.02625623\n",
      "Iteration 21, loss = 0.02587985\n",
      "Iteration 22, loss = 0.02626600\n",
      "Iteration 23, loss = 0.02627848\n",
      "Iteration 24, loss = 0.02751348\n",
      "Iteration 25, loss = 0.02436012\n",
      "Iteration 26, loss = 0.02361222\n",
      "Iteration 27, loss = 0.02372466\n",
      "Iteration 28, loss = 0.02371493\n",
      "Iteration 29, loss = 0.02349817\n",
      "Iteration 30, loss = 0.02210422\n",
      "Iteration 31, loss = 0.02162664\n",
      "Iteration 32, loss = 0.02149521\n",
      "Iteration 33, loss = 0.02209374\n",
      "Iteration 34, loss = 0.02176916\n",
      "Iteration 35, loss = 0.02276550\n",
      "Iteration 36, loss = 0.02041563\n",
      "Iteration 37, loss = 0.01948391\n",
      "Iteration 38, loss = 0.02094943\n",
      "Iteration 39, loss = 0.01949678\n",
      "Iteration 40, loss = 0.01853133\n",
      "Iteration 41, loss = 0.01831380\n",
      "Iteration 42, loss = 0.01931926\n",
      "Iteration 43, loss = 0.01787305\n",
      "Iteration 44, loss = 0.01781433\n",
      "Iteration 45, loss = 0.01760002\n",
      "Iteration 46, loss = 0.01691125\n",
      "Iteration 47, loss = 0.01721876\n",
      "Iteration 48, loss = 0.01644633\n",
      "Iteration 49, loss = 0.01612209\n",
      "Iteration 50, loss = 0.01567072\n",
      "Iteration 1, loss = 0.20918634\n",
      "Iteration 2, loss = 0.05862579\n",
      "Iteration 3, loss = 0.04855790\n",
      "Iteration 4, loss = 0.04431721\n",
      "Iteration 5, loss = 0.04105396\n",
      "Iteration 6, loss = 0.03882369\n",
      "Iteration 7, loss = 0.03565571\n",
      "Iteration 8, loss = 0.03345019\n",
      "Iteration 9, loss = 0.03202376\n",
      "Iteration 10, loss = 0.03127513\n",
      "Iteration 11, loss = 0.02976668\n",
      "Iteration 12, loss = 0.02849998\n",
      "Iteration 13, loss = 0.02857588\n",
      "Iteration 14, loss = 0.02762983\n",
      "Iteration 15, loss = 0.02703877\n",
      "Iteration 16, loss = 0.02676166\n",
      "Iteration 17, loss = 0.02632714\n",
      "Iteration 18, loss = 0.02596851\n",
      "Iteration 19, loss = 0.02551011\n",
      "Iteration 20, loss = 0.02476156\n",
      "Iteration 21, loss = 0.02429929\n",
      "Iteration 22, loss = 0.02417398\n",
      "Iteration 23, loss = 0.02424573\n",
      "Iteration 24, loss = 0.02413755\n",
      "Iteration 25, loss = 0.02431526\n",
      "Iteration 26, loss = 0.02245079\n",
      "Iteration 27, loss = 0.02262116\n",
      "Iteration 28, loss = 0.02300546\n",
      "Iteration 29, loss = 0.02252175\n",
      "Iteration 30, loss = 0.02113268\n",
      "Iteration 31, loss = 0.02135011\n",
      "Iteration 32, loss = 0.02143033\n",
      "Iteration 33, loss = 0.02114756\n",
      "Iteration 34, loss = 0.01972892\n",
      "Iteration 35, loss = 0.02024672\n",
      "Iteration 36, loss = 0.01883674\n",
      "Iteration 37, loss = 0.01953737\n",
      "Iteration 38, loss = 0.01841237\n",
      "Iteration 39, loss = 0.01807634\n",
      "Iteration 40, loss = 0.01750541\n",
      "Iteration 41, loss = 0.01751775\n",
      "Iteration 42, loss = 0.01681169\n",
      "Iteration 43, loss = 0.01669447\n",
      "Iteration 44, loss = 0.01668079\n",
      "Iteration 45, loss = 0.01606386\n",
      "Iteration 46, loss = 0.01628042\n",
      "Iteration 47, loss = 0.01529185\n",
      "Iteration 48, loss = 0.01585276\n",
      "Iteration 49, loss = 0.01645271\n",
      "Iteration 50, loss = 0.01501533\n",
      "Iteration 1, loss = 0.21318789\n",
      "Iteration 2, loss = 0.05995198\n",
      "Iteration 3, loss = 0.05134443\n",
      "Iteration 4, loss = 0.04666536\n",
      "Iteration 5, loss = 0.04296024\n",
      "Iteration 6, loss = 0.04074216\n",
      "Iteration 7, loss = 0.03722760\n",
      "Iteration 8, loss = 0.03587843\n",
      "Iteration 9, loss = 0.03370530\n",
      "Iteration 10, loss = 0.03252188\n",
      "Iteration 11, loss = 0.03188125\n",
      "Iteration 12, loss = 0.03085801\n",
      "Iteration 13, loss = 0.03036554\n",
      "Iteration 14, loss = 0.02936714\n",
      "Iteration 15, loss = 0.02959681\n",
      "Iteration 16, loss = 0.02877419\n",
      "Iteration 17, loss = 0.02835309\n",
      "Iteration 18, loss = 0.02770417\n",
      "Iteration 19, loss = 0.02735227\n",
      "Iteration 20, loss = 0.02755121\n",
      "Iteration 21, loss = 0.02716171\n",
      "Iteration 22, loss = 0.02572700\n",
      "Iteration 23, loss = 0.02821262\n",
      "Iteration 24, loss = 0.02683374\n",
      "Iteration 25, loss = 0.02555046\n",
      "Iteration 26, loss = 0.02527744\n",
      "Iteration 27, loss = 0.02496224\n",
      "Iteration 28, loss = 0.02462453\n",
      "Iteration 29, loss = 0.02461598\n",
      "Iteration 30, loss = 0.02401498\n",
      "Iteration 31, loss = 0.02291170\n",
      "Iteration 32, loss = 0.02261266\n",
      "Iteration 33, loss = 0.02245371\n",
      "Iteration 34, loss = 0.02186679\n",
      "Iteration 35, loss = 0.02078617\n",
      "Iteration 36, loss = 0.02077843\n",
      "Iteration 37, loss = 0.02031494\n",
      "Iteration 38, loss = 0.02093650\n",
      "Iteration 39, loss = 0.02023538\n",
      "Iteration 40, loss = 0.02080070\n",
      "Iteration 41, loss = 0.01879051\n",
      "Iteration 42, loss = 0.01907118\n",
      "Iteration 43, loss = 0.01928734\n",
      "Iteration 44, loss = 0.01784964\n",
      "Iteration 45, loss = 0.01710825\n",
      "Iteration 46, loss = 0.01743040\n",
      "Iteration 47, loss = 0.01638108\n",
      "Iteration 48, loss = 0.01625836\n",
      "Iteration 49, loss = 0.01712064\n",
      "Iteration 50, loss = 0.01677848\n",
      "Iteration 1, loss = 0.21078957\n",
      "Iteration 2, loss = 0.05869835\n",
      "Iteration 3, loss = 0.05063203\n",
      "Iteration 4, loss = 0.04658239\n",
      "Iteration 5, loss = 0.04276577\n",
      "Iteration 6, loss = 0.03975406\n",
      "Iteration 7, loss = 0.03690615\n",
      "Iteration 8, loss = 0.03454730\n",
      "Iteration 9, loss = 0.03348813\n",
      "Iteration 10, loss = 0.03198251\n",
      "Iteration 11, loss = 0.03112924\n",
      "Iteration 12, loss = 0.03041636\n",
      "Iteration 13, loss = 0.02863738\n",
      "Iteration 14, loss = 0.02852190\n",
      "Iteration 15, loss = 0.03007157\n",
      "Iteration 16, loss = 0.02739538\n",
      "Iteration 17, loss = 0.02751226\n",
      "Iteration 18, loss = 0.02659456\n",
      "Iteration 19, loss = 0.02600094\n",
      "Iteration 20, loss = 0.02635229\n",
      "Iteration 21, loss = 0.02518323\n",
      "Iteration 22, loss = 0.02512820\n",
      "Iteration 23, loss = 0.02504147\n",
      "Iteration 24, loss = 0.02483706\n",
      "Iteration 25, loss = 0.02428341\n",
      "Iteration 26, loss = 0.02393375\n",
      "Iteration 27, loss = 0.02327288\n",
      "Iteration 28, loss = 0.02303088\n",
      "Iteration 29, loss = 0.02183925\n",
      "Iteration 30, loss = 0.02296470\n",
      "Iteration 31, loss = 0.02244805\n",
      "Iteration 32, loss = 0.02088392\n",
      "Iteration 33, loss = 0.02027587\n",
      "Iteration 34, loss = 0.02035300\n",
      "Iteration 35, loss = 0.02544816\n",
      "Iteration 36, loss = 0.02078891\n",
      "Iteration 37, loss = 0.02017235\n",
      "Iteration 38, loss = 0.01867035\n",
      "Iteration 39, loss = 0.01872706\n",
      "Iteration 40, loss = 0.01830981\n",
      "Iteration 41, loss = 0.01780429\n",
      "Iteration 42, loss = 0.01684292\n",
      "Iteration 43, loss = 0.01751321\n",
      "Iteration 44, loss = 0.01666667\n",
      "Iteration 45, loss = 0.01686785\n",
      "Iteration 46, loss = 0.01627280\n",
      "Iteration 47, loss = 0.01571341\n",
      "Iteration 48, loss = 0.01574569\n",
      "Iteration 49, loss = 0.01558779\n",
      "Iteration 50, loss = 0.01475844\n",
      "Iteration 1, loss = 0.21085005\n",
      "Iteration 2, loss = 0.06077896\n",
      "Iteration 3, loss = 0.05128721\n",
      "Iteration 4, loss = 0.04688284\n",
      "Iteration 5, loss = 0.04374273\n",
      "Iteration 6, loss = 0.04007597\n",
      "Iteration 7, loss = 0.03743897\n",
      "Iteration 8, loss = 0.03582999\n",
      "Iteration 9, loss = 0.03489319\n",
      "Iteration 10, loss = 0.03295990\n",
      "Iteration 11, loss = 0.03174389\n",
      "Iteration 12, loss = 0.03098406\n",
      "Iteration 13, loss = 0.02992745\n",
      "Iteration 14, loss = 0.02944421\n",
      "Iteration 15, loss = 0.02927453\n",
      "Iteration 16, loss = 0.02874405\n",
      "Iteration 17, loss = 0.02797859\n",
      "Iteration 18, loss = 0.02770550\n",
      "Iteration 19, loss = 0.02843981\n",
      "Iteration 20, loss = 0.02681920\n",
      "Iteration 21, loss = 0.02685555\n",
      "Iteration 22, loss = 0.02650137\n",
      "Iteration 23, loss = 0.02699826\n",
      "Iteration 24, loss = 0.02546271\n",
      "Iteration 25, loss = 0.02486704\n",
      "Iteration 26, loss = 0.02478100\n",
      "Iteration 27, loss = 0.02388177\n",
      "Iteration 28, loss = 0.02441786\n",
      "Iteration 29, loss = 0.02358056\n",
      "Iteration 30, loss = 0.02328619\n",
      "Iteration 31, loss = 0.02294649\n",
      "Iteration 32, loss = 0.02272616\n",
      "Iteration 33, loss = 0.02174832\n",
      "Iteration 34, loss = 0.02186146\n",
      "Iteration 35, loss = 0.02126893\n",
      "Iteration 36, loss = 0.02076793\n",
      "Iteration 37, loss = 0.02054822\n",
      "Iteration 38, loss = 0.01996092\n",
      "Iteration 39, loss = 0.01947414\n",
      "Iteration 40, loss = 0.01905675\n",
      "Iteration 41, loss = 0.01888382\n",
      "Iteration 42, loss = 0.01935743\n",
      "Iteration 43, loss = 0.01842833\n",
      "Iteration 44, loss = 0.01801510\n",
      "Iteration 45, loss = 0.01730319\n",
      "Iteration 46, loss = 0.01719261\n",
      "Iteration 47, loss = 0.01589521\n",
      "Iteration 48, loss = 0.01612218\n",
      "Iteration 49, loss = 0.01582674\n",
      "Iteration 50, loss = 0.01537429\n",
      "Iteration 1, loss = 0.21182380\n",
      "Iteration 2, loss = 0.05729916\n",
      "Iteration 3, loss = 0.04971307\n",
      "Iteration 4, loss = 0.04568238\n",
      "Iteration 5, loss = 0.04141424\n",
      "Iteration 6, loss = 0.03851186\n",
      "Iteration 7, loss = 0.03676125\n",
      "Iteration 8, loss = 0.03391214\n",
      "Iteration 9, loss = 0.03266897\n",
      "Iteration 10, loss = 0.03141295\n",
      "Iteration 11, loss = 0.03064744\n",
      "Iteration 12, loss = 0.02999002\n",
      "Iteration 13, loss = 0.02913918\n",
      "Iteration 14, loss = 0.02881023\n",
      "Iteration 15, loss = 0.02874147\n",
      "Iteration 16, loss = 0.02751484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, loss = 0.02786459\n",
      "Iteration 18, loss = 0.02674294\n",
      "Iteration 19, loss = 0.02683170\n",
      "Iteration 20, loss = 0.02620182\n",
      "Iteration 21, loss = 0.02621805\n",
      "Iteration 22, loss = 0.02640563\n",
      "Iteration 23, loss = 0.02464975\n",
      "Iteration 24, loss = 0.02539841\n",
      "Iteration 25, loss = 0.02455578\n",
      "Iteration 26, loss = 0.02375056\n",
      "Iteration 27, loss = 0.02419586\n",
      "Iteration 28, loss = 0.02357853\n",
      "Iteration 29, loss = 0.02329288\n",
      "Iteration 30, loss = 0.02277817\n",
      "Iteration 31, loss = 0.02358584\n",
      "Iteration 32, loss = 0.02153241\n",
      "Iteration 33, loss = 0.02249194\n",
      "Iteration 34, loss = 0.02048167\n",
      "Iteration 35, loss = 0.02091150\n",
      "Iteration 36, loss = 0.02000403\n",
      "Iteration 37, loss = 0.02157499\n",
      "Iteration 38, loss = 0.01984990\n",
      "Iteration 39, loss = 0.01896374\n",
      "Iteration 40, loss = 0.01883698\n",
      "Iteration 41, loss = 0.01877575\n",
      "Iteration 42, loss = 0.01893807\n",
      "Iteration 43, loss = 0.01934575\n",
      "Iteration 44, loss = 0.01811655\n",
      "Iteration 45, loss = 0.01780758\n",
      "Iteration 46, loss = 0.01744183\n",
      "Iteration 47, loss = 0.01693656\n",
      "Iteration 48, loss = 0.01605440\n",
      "Iteration 49, loss = 0.01623140\n",
      "Iteration 50, loss = 0.01581372\n",
      "Train Accuracy: 0.9915\n",
      "Running Memory: 0.21 MB\n",
      "Running Time: 103.14 Seconds\n",
      "Test  Accuracy: 0.9370\n",
      "Test  Presion:  0.8942\n",
      "Test  Recall:   0.9958\n",
      "Test  F1-score: 0.9423\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS1_tfidf_1, memory_MLP_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS1_tfidf_1, acc_MLP_DS1_tfidf_1, pre_MLP_DS1_tfidf_1, rec_MLP_DS1_tfidf_1, f1_MLP_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS1_tfidf_1, model_3, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.33128253\n",
      "Iteration 2, loss = 0.26991698\n",
      "Iteration 3, loss = 0.26417065\n",
      "Iteration 4, loss = 0.25941858\n",
      "Iteration 5, loss = 0.25780923\n",
      "Iteration 6, loss = 0.25498434\n",
      "Iteration 7, loss = 0.25229486\n",
      "Iteration 8, loss = 0.25106801\n",
      "Iteration 9, loss = 0.24931910\n",
      "Iteration 10, loss = 0.24811395\n",
      "Iteration 11, loss = 0.24659651\n",
      "Iteration 12, loss = 0.24645447\n",
      "Iteration 13, loss = 0.24427200\n",
      "Iteration 14, loss = 0.24396847\n",
      "Iteration 15, loss = 0.24323844\n",
      "Iteration 16, loss = 0.24150139\n",
      "Iteration 17, loss = 0.24139518\n",
      "Iteration 18, loss = 0.24030025\n",
      "Iteration 19, loss = 0.23956620\n",
      "Iteration 20, loss = 0.23865762\n",
      "Iteration 21, loss = 0.23816796\n",
      "Iteration 22, loss = 0.23845887\n",
      "Iteration 23, loss = 0.23692466\n",
      "Iteration 24, loss = 0.23557178\n",
      "Iteration 25, loss = 0.23701618\n",
      "Iteration 26, loss = 0.23541335\n",
      "Iteration 27, loss = 0.23609593\n",
      "Iteration 28, loss = 0.23406806\n",
      "Iteration 29, loss = 0.23344524\n",
      "Iteration 30, loss = 0.23372958\n",
      "Iteration 31, loss = 0.23311706\n",
      "Iteration 32, loss = 0.23165442\n",
      "Iteration 33, loss = 0.23122800\n",
      "Iteration 34, loss = 0.23202936\n",
      "Iteration 35, loss = 0.23193019\n",
      "Iteration 36, loss = 0.23102818\n",
      "Iteration 37, loss = 0.23097091\n",
      "Iteration 38, loss = 0.23082705\n",
      "Iteration 39, loss = 0.22924668\n",
      "Iteration 40, loss = 0.22952579\n",
      "Iteration 41, loss = 0.22848322\n",
      "Iteration 42, loss = 0.22849981\n",
      "Iteration 43, loss = 0.22830700\n",
      "Iteration 44, loss = 0.22832608\n",
      "Iteration 45, loss = 0.22710875\n",
      "Iteration 46, loss = 0.22581942\n",
      "Iteration 47, loss = 0.22633080\n",
      "Iteration 48, loss = 0.22543154\n",
      "Iteration 49, loss = 0.22660555\n",
      "Iteration 50, loss = 0.22586298\n",
      "Iteration 1, loss = 0.33957809\n",
      "Iteration 2, loss = 0.26953203\n",
      "Iteration 3, loss = 0.26502797\n",
      "Iteration 4, loss = 0.26155201\n",
      "Iteration 5, loss = 0.25843675\n",
      "Iteration 6, loss = 0.25503633\n",
      "Iteration 7, loss = 0.25460162\n",
      "Iteration 8, loss = 0.25184742\n",
      "Iteration 9, loss = 0.25061637\n",
      "Iteration 10, loss = 0.25033237\n",
      "Iteration 11, loss = 0.24755154\n",
      "Iteration 12, loss = 0.24581754\n",
      "Iteration 13, loss = 0.24524712\n",
      "Iteration 14, loss = 0.24459847\n",
      "Iteration 15, loss = 0.24352498\n",
      "Iteration 16, loss = 0.24181285\n",
      "Iteration 17, loss = 0.24206473\n",
      "Iteration 18, loss = 0.23980687\n",
      "Iteration 19, loss = 0.23984626\n",
      "Iteration 20, loss = 0.23924331\n",
      "Iteration 21, loss = 0.23857827\n",
      "Iteration 22, loss = 0.23867240\n",
      "Iteration 23, loss = 0.23833783\n",
      "Iteration 24, loss = 0.23677746\n",
      "Iteration 25, loss = 0.23583925\n",
      "Iteration 26, loss = 0.23566027\n",
      "Iteration 27, loss = 0.23542808\n",
      "Iteration 28, loss = 0.23402315\n",
      "Iteration 29, loss = 0.23361756\n",
      "Iteration 30, loss = 0.23411679\n",
      "Iteration 31, loss = 0.23315116\n",
      "Iteration 32, loss = 0.23295078\n",
      "Iteration 33, loss = 0.23248167\n",
      "Iteration 34, loss = 0.23201380\n",
      "Iteration 35, loss = 0.23174724\n",
      "Iteration 36, loss = 0.23124827\n",
      "Iteration 37, loss = 0.23134567\n",
      "Iteration 38, loss = 0.23020722\n",
      "Iteration 39, loss = 0.22939752\n",
      "Iteration 40, loss = 0.23008051\n",
      "Iteration 41, loss = 0.22924029\n",
      "Iteration 42, loss = 0.22896759\n",
      "Iteration 43, loss = 0.22758274\n",
      "Iteration 44, loss = 0.22805779\n",
      "Iteration 45, loss = 0.22736209\n",
      "Iteration 46, loss = 0.22690255\n",
      "Iteration 47, loss = 0.22599373\n",
      "Iteration 48, loss = 0.22648630\n",
      "Iteration 49, loss = 0.22599105\n",
      "Iteration 50, loss = 0.22544228\n",
      "Iteration 1, loss = 0.33922376\n",
      "Iteration 2, loss = 0.27074571\n",
      "Iteration 3, loss = 0.26471939\n",
      "Iteration 4, loss = 0.26296410\n",
      "Iteration 5, loss = 0.25761170\n",
      "Iteration 6, loss = 0.25616478\n",
      "Iteration 7, loss = 0.25288594\n",
      "Iteration 8, loss = 0.25165991\n",
      "Iteration 9, loss = 0.24902143\n",
      "Iteration 10, loss = 0.24971123\n",
      "Iteration 11, loss = 0.24836108\n",
      "Iteration 12, loss = 0.24686262\n",
      "Iteration 13, loss = 0.24437636\n",
      "Iteration 14, loss = 0.24462294\n",
      "Iteration 15, loss = 0.24372717\n",
      "Iteration 16, loss = 0.24137146\n",
      "Iteration 17, loss = 0.24264881\n",
      "Iteration 18, loss = 0.24067879\n",
      "Iteration 19, loss = 0.24029693\n",
      "Iteration 20, loss = 0.24082840\n",
      "Iteration 21, loss = 0.23952084\n",
      "Iteration 22, loss = 0.23987130\n",
      "Iteration 23, loss = 0.23860762\n",
      "Iteration 24, loss = 0.23703427\n",
      "Iteration 25, loss = 0.23622652\n",
      "Iteration 26, loss = 0.23771512\n",
      "Iteration 27, loss = 0.23644659\n",
      "Iteration 28, loss = 0.23473878\n",
      "Iteration 29, loss = 0.23566110\n",
      "Iteration 30, loss = 0.23392019\n",
      "Iteration 31, loss = 0.23521147\n",
      "Iteration 32, loss = 0.23305053\n",
      "Iteration 33, loss = 0.23243672\n",
      "Iteration 34, loss = 0.23362110\n",
      "Iteration 35, loss = 0.23174280\n",
      "Iteration 36, loss = 0.23102584\n",
      "Iteration 37, loss = 0.23140275\n",
      "Iteration 38, loss = 0.23071001\n",
      "Iteration 39, loss = 0.22992702\n",
      "Iteration 40, loss = 0.23044371\n",
      "Iteration 41, loss = 0.22908349\n",
      "Iteration 42, loss = 0.22887722\n",
      "Iteration 43, loss = 0.22837197\n",
      "Iteration 44, loss = 0.22810865\n",
      "Iteration 45, loss = 0.22745054\n",
      "Iteration 46, loss = 0.22745458\n",
      "Iteration 47, loss = 0.22701034\n",
      "Iteration 48, loss = 0.22707213\n",
      "Iteration 49, loss = 0.22722193\n",
      "Iteration 50, loss = 0.22697511\n",
      "Iteration 1, loss = 0.33716656\n",
      "Iteration 2, loss = 0.27034816\n",
      "Iteration 3, loss = 0.26322683\n",
      "Iteration 4, loss = 0.26008334\n",
      "Iteration 5, loss = 0.25723740\n",
      "Iteration 6, loss = 0.25415993\n",
      "Iteration 7, loss = 0.25234871\n",
      "Iteration 8, loss = 0.25079534\n",
      "Iteration 9, loss = 0.24871951\n",
      "Iteration 10, loss = 0.24812215\n",
      "Iteration 11, loss = 0.24678918\n",
      "Iteration 12, loss = 0.24483695\n",
      "Iteration 13, loss = 0.24526955\n",
      "Iteration 14, loss = 0.24466131\n",
      "Iteration 15, loss = 0.24353959\n",
      "Iteration 16, loss = 0.24220292\n",
      "Iteration 17, loss = 0.24249345\n",
      "Iteration 18, loss = 0.24077843\n",
      "Iteration 19, loss = 0.23980830\n",
      "Iteration 20, loss = 0.23952986\n",
      "Iteration 21, loss = 0.23847045\n",
      "Iteration 22, loss = 0.23717963\n",
      "Iteration 23, loss = 0.23727696\n",
      "Iteration 24, loss = 0.23706522\n",
      "Iteration 25, loss = 0.23561821\n",
      "Iteration 26, loss = 0.23447921\n",
      "Iteration 27, loss = 0.23490002\n",
      "Iteration 28, loss = 0.23449737\n",
      "Iteration 29, loss = 0.23362092\n",
      "Iteration 30, loss = 0.23379014\n",
      "Iteration 31, loss = 0.23340405\n",
      "Iteration 32, loss = 0.23195591\n",
      "Iteration 33, loss = 0.23169604\n",
      "Iteration 34, loss = 0.23181851\n",
      "Iteration 35, loss = 0.23150951\n",
      "Iteration 36, loss = 0.23161090\n",
      "Iteration 37, loss = 0.22962752\n",
      "Iteration 38, loss = 0.23033993\n",
      "Iteration 39, loss = 0.23001394\n",
      "Iteration 40, loss = 0.22865172\n",
      "Iteration 41, loss = 0.22975138\n",
      "Iteration 42, loss = 0.22920860\n",
      "Iteration 43, loss = 0.22979887\n",
      "Iteration 44, loss = 0.22852328\n",
      "Iteration 45, loss = 0.22815696\n",
      "Iteration 46, loss = 0.22750091\n",
      "Iteration 47, loss = 0.22750526\n",
      "Iteration 48, loss = 0.22665831\n",
      "Iteration 49, loss = 0.22758979\n",
      "Iteration 50, loss = 0.22619791\n",
      "Iteration 1, loss = 0.33814996\n",
      "Iteration 2, loss = 0.27215467\n",
      "Iteration 3, loss = 0.26409507\n",
      "Iteration 4, loss = 0.26247960\n",
      "Iteration 5, loss = 0.25849458\n",
      "Iteration 6, loss = 0.25441342\n",
      "Iteration 7, loss = 0.25427417\n",
      "Iteration 8, loss = 0.25212087\n",
      "Iteration 9, loss = 0.25026291\n",
      "Iteration 10, loss = 0.24909168\n",
      "Iteration 11, loss = 0.24884298\n",
      "Iteration 12, loss = 0.24680896\n",
      "Iteration 13, loss = 0.24721449\n",
      "Iteration 14, loss = 0.24473830\n",
      "Iteration 15, loss = 0.24440080\n",
      "Iteration 16, loss = 0.24271047\n",
      "Iteration 17, loss = 0.24300637\n",
      "Iteration 18, loss = 0.24212157\n",
      "Iteration 19, loss = 0.24080582\n",
      "Iteration 20, loss = 0.23954620\n",
      "Iteration 21, loss = 0.23888817\n",
      "Iteration 22, loss = 0.23857908\n",
      "Iteration 23, loss = 0.23805188\n",
      "Iteration 24, loss = 0.23765675\n",
      "Iteration 25, loss = 0.23670437\n",
      "Iteration 26, loss = 0.23626101\n",
      "Iteration 27, loss = 0.23577571\n",
      "Iteration 28, loss = 0.23489089\n",
      "Iteration 29, loss = 0.23472774\n",
      "Iteration 30, loss = 0.23451515\n",
      "Iteration 31, loss = 0.23357789\n",
      "Iteration 32, loss = 0.23336364\n",
      "Iteration 33, loss = 0.23198016\n",
      "Iteration 34, loss = 0.23125136\n",
      "Iteration 35, loss = 0.23179364\n",
      "Iteration 36, loss = 0.23160211\n",
      "Iteration 37, loss = 0.23071024\n",
      "Iteration 38, loss = 0.23060836\n",
      "Iteration 39, loss = 0.23001822\n",
      "Iteration 40, loss = 0.22996104\n",
      "Iteration 41, loss = 0.22949967\n",
      "Iteration 42, loss = 0.22909313\n",
      "Iteration 43, loss = 0.22815597\n",
      "Iteration 44, loss = 0.22837946\n",
      "Iteration 45, loss = 0.22781667\n",
      "Iteration 46, loss = 0.22765676\n",
      "Iteration 47, loss = 0.22670114\n",
      "Iteration 48, loss = 0.22600182\n",
      "Iteration 49, loss = 0.22588315\n",
      "Iteration 50, loss = 0.22552887\n",
      "Iteration 1, loss = 0.33770999\n",
      "Iteration 2, loss = 0.26959545\n",
      "Iteration 3, loss = 0.26319136\n",
      "Iteration 4, loss = 0.26089469\n",
      "Iteration 5, loss = 0.25742013\n",
      "Iteration 6, loss = 0.25582782\n",
      "Iteration 7, loss = 0.25305010\n",
      "Iteration 8, loss = 0.25220043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.25093060\n",
      "Iteration 10, loss = 0.24907521\n",
      "Iteration 11, loss = 0.24791579\n",
      "Iteration 12, loss = 0.24692376\n",
      "Iteration 13, loss = 0.24531769\n",
      "Iteration 14, loss = 0.24490174\n",
      "Iteration 15, loss = 0.24439790\n",
      "Iteration 16, loss = 0.24295511\n",
      "Iteration 17, loss = 0.24252928\n",
      "Iteration 18, loss = 0.24112655\n",
      "Iteration 19, loss = 0.23998046\n",
      "Iteration 20, loss = 0.24011106\n",
      "Iteration 21, loss = 0.23873939\n",
      "Iteration 22, loss = 0.23830516\n",
      "Iteration 23, loss = 0.23807279\n",
      "Iteration 24, loss = 0.23652605\n",
      "Iteration 25, loss = 0.23609116\n",
      "Iteration 26, loss = 0.23700129\n",
      "Iteration 27, loss = 0.23609464\n",
      "Iteration 28, loss = 0.23568189\n",
      "Iteration 29, loss = 0.23430508\n",
      "Iteration 30, loss = 0.23535408\n",
      "Iteration 31, loss = 0.23424971\n",
      "Iteration 32, loss = 0.23415260\n",
      "Iteration 33, loss = 0.23429683\n",
      "Iteration 34, loss = 0.23225996\n",
      "Iteration 35, loss = 0.23274936\n",
      "Iteration 36, loss = 0.23238220\n",
      "Iteration 37, loss = 0.23213466\n",
      "Iteration 38, loss = 0.23031222\n",
      "Iteration 39, loss = 0.23021970\n",
      "Iteration 40, loss = 0.23024809\n",
      "Iteration 41, loss = 0.23008970\n",
      "Iteration 42, loss = 0.22959346\n",
      "Iteration 43, loss = 0.22905886\n",
      "Iteration 44, loss = 0.22900981\n",
      "Iteration 45, loss = 0.22791229\n",
      "Iteration 46, loss = 0.22711050\n",
      "Iteration 47, loss = 0.22957544\n",
      "Iteration 48, loss = 0.22810351\n",
      "Iteration 49, loss = 0.22693915\n",
      "Iteration 50, loss = 0.22589445\n",
      "Iteration 1, loss = 0.33718366\n",
      "Iteration 2, loss = 0.27001010\n",
      "Iteration 3, loss = 0.26375502\n",
      "Iteration 4, loss = 0.26048305\n",
      "Iteration 5, loss = 0.25703701\n",
      "Iteration 6, loss = 0.25464312\n",
      "Iteration 7, loss = 0.25264819\n",
      "Iteration 8, loss = 0.25122212\n",
      "Iteration 9, loss = 0.25151085\n",
      "Iteration 10, loss = 0.24911690\n",
      "Iteration 11, loss = 0.24722208\n",
      "Iteration 12, loss = 0.24634361\n",
      "Iteration 13, loss = 0.24632369\n",
      "Iteration 14, loss = 0.24490450\n",
      "Iteration 15, loss = 0.24426523\n",
      "Iteration 16, loss = 0.24313997\n",
      "Iteration 17, loss = 0.24154513\n",
      "Iteration 18, loss = 0.24135833\n",
      "Iteration 19, loss = 0.24132568\n",
      "Iteration 20, loss = 0.24075854\n",
      "Iteration 21, loss = 0.23875940\n",
      "Iteration 22, loss = 0.23901166\n",
      "Iteration 23, loss = 0.23941137\n",
      "Iteration 24, loss = 0.23859321\n",
      "Iteration 25, loss = 0.23758739\n",
      "Iteration 26, loss = 0.23744555\n",
      "Iteration 27, loss = 0.23708779\n",
      "Iteration 28, loss = 0.23494356\n",
      "Iteration 29, loss = 0.23609597\n",
      "Iteration 30, loss = 0.23520735\n",
      "Iteration 31, loss = 0.23317871\n",
      "Iteration 32, loss = 0.23329527\n",
      "Iteration 33, loss = 0.23293507\n",
      "Iteration 34, loss = 0.23311152\n",
      "Iteration 35, loss = 0.23245946\n",
      "Iteration 36, loss = 0.23148699\n",
      "Iteration 37, loss = 0.23102111\n",
      "Iteration 38, loss = 0.23101816\n",
      "Iteration 39, loss = 0.23199740\n",
      "Iteration 40, loss = 0.23075268\n",
      "Iteration 41, loss = 0.23016401\n",
      "Iteration 42, loss = 0.23069988\n",
      "Iteration 43, loss = 0.22872542\n",
      "Iteration 44, loss = 0.22884127\n",
      "Iteration 45, loss = 0.22736247\n",
      "Iteration 46, loss = 0.22747319\n",
      "Iteration 47, loss = 0.22661209\n",
      "Iteration 48, loss = 0.22808316\n",
      "Iteration 49, loss = 0.22692810\n",
      "Iteration 50, loss = 0.22603149\n",
      "Iteration 1, loss = 0.33882872\n",
      "Iteration 2, loss = 0.27267998\n",
      "Iteration 3, loss = 0.26523489\n",
      "Iteration 4, loss = 0.26141392\n",
      "Iteration 5, loss = 0.25850264\n",
      "Iteration 6, loss = 0.25529457\n",
      "Iteration 7, loss = 0.25372312\n",
      "Iteration 8, loss = 0.25108126\n",
      "Iteration 9, loss = 0.25146437\n",
      "Iteration 10, loss = 0.24933631\n",
      "Iteration 11, loss = 0.24922787\n",
      "Iteration 12, loss = 0.24766349\n",
      "Iteration 13, loss = 0.24587075\n",
      "Iteration 14, loss = 0.24642041\n",
      "Iteration 15, loss = 0.24391459\n",
      "Iteration 16, loss = 0.24459204\n",
      "Iteration 17, loss = 0.24269719\n",
      "Iteration 18, loss = 0.24216259\n",
      "Iteration 19, loss = 0.24124290\n",
      "Iteration 20, loss = 0.24036120\n",
      "Iteration 21, loss = 0.24030728\n",
      "Iteration 22, loss = 0.23935767\n",
      "Iteration 23, loss = 0.24083705\n",
      "Iteration 24, loss = 0.23811953\n",
      "Iteration 25, loss = 0.23788776\n",
      "Iteration 26, loss = 0.23793750\n",
      "Iteration 27, loss = 0.23691971\n",
      "Iteration 28, loss = 0.23677687\n",
      "Iteration 29, loss = 0.23615187\n",
      "Iteration 30, loss = 0.23519601\n",
      "Iteration 31, loss = 0.23493065\n",
      "Iteration 32, loss = 0.23520322\n",
      "Iteration 33, loss = 0.23398225\n",
      "Iteration 34, loss = 0.23344797\n",
      "Iteration 35, loss = 0.23349236\n",
      "Iteration 36, loss = 0.23287164\n",
      "Iteration 37, loss = 0.23163745\n",
      "Iteration 38, loss = 0.23216295\n",
      "Iteration 39, loss = 0.23211947\n",
      "Iteration 40, loss = 0.23148120\n",
      "Iteration 41, loss = 0.23092686\n",
      "Iteration 42, loss = 0.23084534\n",
      "Iteration 43, loss = 0.23002363\n",
      "Iteration 44, loss = 0.22964069\n",
      "Iteration 45, loss = 0.22956371\n",
      "Iteration 46, loss = 0.22907459\n",
      "Iteration 47, loss = 0.22794950\n",
      "Iteration 48, loss = 0.22834090\n",
      "Iteration 49, loss = 0.22887812\n",
      "Iteration 50, loss = 0.22739969\n",
      "Iteration 1, loss = 0.33470013\n",
      "Iteration 2, loss = 0.26971143\n",
      "Iteration 3, loss = 0.26517808\n",
      "Iteration 4, loss = 0.26087821\n",
      "Iteration 5, loss = 0.25863401\n",
      "Iteration 6, loss = 0.25588025\n",
      "Iteration 7, loss = 0.25501807\n",
      "Iteration 8, loss = 0.25348245\n",
      "Iteration 9, loss = 0.25166299\n",
      "Iteration 10, loss = 0.24993755\n",
      "Iteration 11, loss = 0.24751965\n",
      "Iteration 12, loss = 0.24692977\n",
      "Iteration 13, loss = 0.24621372\n",
      "Iteration 14, loss = 0.24526498\n",
      "Iteration 15, loss = 0.24355852\n",
      "Iteration 16, loss = 0.24304556\n",
      "Iteration 17, loss = 0.24313542\n",
      "Iteration 18, loss = 0.24120610\n",
      "Iteration 19, loss = 0.24070505\n",
      "Iteration 20, loss = 0.24022687\n",
      "Iteration 21, loss = 0.23954512\n",
      "Iteration 22, loss = 0.23975059\n",
      "Iteration 23, loss = 0.23815362\n",
      "Iteration 24, loss = 0.23714459\n",
      "Iteration 25, loss = 0.23715938\n",
      "Iteration 26, loss = 0.23650193\n",
      "Iteration 27, loss = 0.23741677\n",
      "Iteration 28, loss = 0.23590673\n",
      "Iteration 29, loss = 0.23594012\n",
      "Iteration 30, loss = 0.23332031\n",
      "Iteration 31, loss = 0.23388263\n",
      "Iteration 32, loss = 0.23391327\n",
      "Iteration 33, loss = 0.23330519\n",
      "Iteration 34, loss = 0.23281038\n",
      "Iteration 35, loss = 0.23133093\n",
      "Iteration 36, loss = 0.23231003\n",
      "Iteration 37, loss = 0.23114008\n",
      "Iteration 38, loss = 0.23029024\n",
      "Iteration 39, loss = 0.23067216\n",
      "Iteration 40, loss = 0.22952142\n",
      "Iteration 41, loss = 0.23004802\n",
      "Iteration 42, loss = 0.22932080\n",
      "Iteration 43, loss = 0.22893951\n",
      "Iteration 44, loss = 0.22874407\n",
      "Iteration 45, loss = 0.22789809\n",
      "Iteration 46, loss = 0.22794383\n",
      "Iteration 47, loss = 0.22653829\n",
      "Iteration 48, loss = 0.22819013\n",
      "Iteration 49, loss = 0.22547497\n",
      "Iteration 50, loss = 0.22490673\n",
      "Iteration 1, loss = 0.33701659\n",
      "Iteration 2, loss = 0.26754814\n",
      "Iteration 3, loss = 0.26112880\n",
      "Iteration 4, loss = 0.25816454\n",
      "Iteration 5, loss = 0.25590199\n",
      "Iteration 6, loss = 0.25335520\n",
      "Iteration 7, loss = 0.25234487\n",
      "Iteration 8, loss = 0.24885811\n",
      "Iteration 9, loss = 0.24706966\n",
      "Iteration 10, loss = 0.24683032\n",
      "Iteration 11, loss = 0.24517729\n",
      "Iteration 12, loss = 0.24419221\n",
      "Iteration 13, loss = 0.24207607\n",
      "Iteration 14, loss = 0.24198835\n",
      "Iteration 15, loss = 0.24233686\n",
      "Iteration 16, loss = 0.24127751\n",
      "Iteration 17, loss = 0.24141809\n",
      "Iteration 18, loss = 0.23930588\n",
      "Iteration 19, loss = 0.23816806\n",
      "Iteration 20, loss = 0.23714882\n",
      "Iteration 21, loss = 0.23732130\n",
      "Iteration 22, loss = 0.23643671\n",
      "Iteration 23, loss = 0.23618814\n",
      "Iteration 24, loss = 0.23560200\n",
      "Iteration 25, loss = 0.23541643\n",
      "Iteration 26, loss = 0.23468253\n",
      "Iteration 27, loss = 0.23379767\n",
      "Iteration 28, loss = 0.23257221\n",
      "Iteration 29, loss = 0.23315473\n",
      "Iteration 30, loss = 0.23253867\n",
      "Iteration 31, loss = 0.23181220\n",
      "Iteration 32, loss = 0.23085902\n",
      "Iteration 33, loss = 0.23016517\n",
      "Iteration 34, loss = 0.23071275\n",
      "Iteration 35, loss = 0.22969648\n",
      "Iteration 36, loss = 0.22929700\n",
      "Iteration 37, loss = 0.22893079\n",
      "Iteration 38, loss = 0.22828696\n",
      "Iteration 39, loss = 0.22901478\n",
      "Iteration 40, loss = 0.22822826\n",
      "Iteration 41, loss = 0.22719315\n",
      "Iteration 42, loss = 0.22642012\n",
      "Iteration 43, loss = 0.22690737\n",
      "Iteration 44, loss = 0.22609940\n",
      "Iteration 45, loss = 0.22550514\n",
      "Iteration 46, loss = 0.22475089\n",
      "Iteration 47, loss = 0.22479332\n",
      "Iteration 48, loss = 0.22523428\n",
      "Iteration 49, loss = 0.22424415\n",
      "Iteration 50, loss = 0.22591427\n",
      "Iteration 1, loss = 0.33884480\n",
      "Iteration 2, loss = 0.27198125\n",
      "Iteration 3, loss = 0.26597437\n",
      "Iteration 4, loss = 0.26304177\n",
      "Iteration 5, loss = 0.26150804\n",
      "Iteration 6, loss = 0.25720825\n",
      "Iteration 7, loss = 0.25402124\n",
      "Iteration 8, loss = 0.25314123\n",
      "Iteration 9, loss = 0.25209092\n",
      "Iteration 10, loss = 0.25089687\n",
      "Iteration 11, loss = 0.24933555\n",
      "Iteration 12, loss = 0.24815095\n",
      "Iteration 13, loss = 0.24844146\n",
      "Iteration 14, loss = 0.24600352\n",
      "Iteration 15, loss = 0.24456628\n",
      "Iteration 16, loss = 0.24378734\n",
      "Iteration 17, loss = 0.24281211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.24345727\n",
      "Iteration 19, loss = 0.24181638\n",
      "Iteration 20, loss = 0.24169396\n",
      "Iteration 21, loss = 0.24153527\n",
      "Iteration 22, loss = 0.23984359\n",
      "Iteration 23, loss = 0.23956956\n",
      "Iteration 24, loss = 0.23828327\n",
      "Iteration 25, loss = 0.23878121\n",
      "Iteration 26, loss = 0.23773575\n",
      "Iteration 27, loss = 0.23645545\n",
      "Iteration 28, loss = 0.23527482\n",
      "Iteration 29, loss = 0.23488919\n",
      "Iteration 30, loss = 0.23522236\n",
      "Iteration 31, loss = 0.23455251\n",
      "Iteration 32, loss = 0.23410063\n",
      "Iteration 33, loss = 0.23500366\n",
      "Iteration 34, loss = 0.23364313\n",
      "Iteration 35, loss = 0.23277322\n",
      "Iteration 36, loss = 0.23306614\n",
      "Iteration 37, loss = 0.23229376\n",
      "Iteration 38, loss = 0.23140453\n",
      "Iteration 39, loss = 0.23177898\n",
      "Iteration 40, loss = 0.22970474\n",
      "Iteration 41, loss = 0.23062848\n",
      "Iteration 42, loss = 0.22980869\n",
      "Iteration 43, loss = 0.22909986\n",
      "Iteration 44, loss = 0.22871277\n",
      "Iteration 45, loss = 0.22907234\n",
      "Iteration 46, loss = 0.22938337\n",
      "Iteration 47, loss = 0.22811781\n",
      "Iteration 48, loss = 0.22774198\n",
      "Iteration 49, loss = 0.22622181\n",
      "Iteration 50, loss = 0.22645173\n",
      "Train Accuracy: 0.8842\n",
      "Running Memory: 0.02 MB\n",
      "Running Time: 112.97 Seconds\n",
      "Test  Accuracy: 0.8751\n",
      "Test  Presion:  0.8384\n",
      "Test  Recall:   0.9392\n",
      "Test  F1-score: 0.8860\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS1_tfidf_2, memory_MLP_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS1_tfidf_2, acc_MLP_DS1_tfidf_2, pre_MLP_DS1_tfidf_2, rec_MLP_DS1_tfidf_2, f1_MLP_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS1_tfidf_2, model_3, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48527600\n",
      "Iteration 2, loss = 0.44272121\n",
      "Iteration 3, loss = 0.44056342\n",
      "Iteration 4, loss = 0.43731293\n",
      "Iteration 5, loss = 0.43603489\n",
      "Iteration 6, loss = 0.43550246\n",
      "Iteration 7, loss = 0.43406713\n",
      "Iteration 8, loss = 0.43348951\n",
      "Iteration 9, loss = 0.43275815\n",
      "Iteration 10, loss = 0.43214333\n",
      "Iteration 11, loss = 0.43311915\n",
      "Iteration 12, loss = 0.43281469\n",
      "Iteration 13, loss = 0.43135479\n",
      "Iteration 14, loss = 0.43086245\n",
      "Iteration 15, loss = 0.43213663\n",
      "Iteration 16, loss = 0.43039965\n",
      "Iteration 17, loss = 0.43036798\n",
      "Iteration 18, loss = 0.43022279\n",
      "Iteration 19, loss = 0.42966176\n",
      "Iteration 20, loss = 0.42955720\n",
      "Iteration 21, loss = 0.42939623\n",
      "Iteration 22, loss = 0.42900958\n",
      "Iteration 23, loss = 0.42987004\n",
      "Iteration 24, loss = 0.42872438\n",
      "Iteration 25, loss = 0.42955300\n",
      "Iteration 26, loss = 0.42781317\n",
      "Iteration 27, loss = 0.42846007\n",
      "Iteration 28, loss = 0.42863550\n",
      "Iteration 29, loss = 0.42800852\n",
      "Iteration 30, loss = 0.42803255\n",
      "Iteration 31, loss = 0.42846897\n",
      "Iteration 32, loss = 0.42686595\n",
      "Iteration 33, loss = 0.42738936\n",
      "Iteration 34, loss = 0.42728900\n",
      "Iteration 35, loss = 0.42833187\n",
      "Iteration 36, loss = 0.42686589\n",
      "Iteration 37, loss = 0.42755634\n",
      "Iteration 38, loss = 0.42713061\n",
      "Iteration 39, loss = 0.42744133\n",
      "Iteration 40, loss = 0.42678117\n",
      "Iteration 41, loss = 0.42719143\n",
      "Iteration 42, loss = 0.42701621\n",
      "Iteration 43, loss = 0.42643222\n",
      "Iteration 44, loss = 0.42627685\n",
      "Iteration 45, loss = 0.42606786\n",
      "Iteration 46, loss = 0.42622927\n",
      "Iteration 47, loss = 0.42664648\n",
      "Iteration 48, loss = 0.42590441\n",
      "Iteration 49, loss = 0.42587946\n",
      "Iteration 50, loss = 0.42588554\n",
      "Iteration 1, loss = 0.49056829\n",
      "Iteration 2, loss = 0.44326478\n",
      "Iteration 3, loss = 0.44022573\n",
      "Iteration 4, loss = 0.43889247\n",
      "Iteration 5, loss = 0.43826209\n",
      "Iteration 6, loss = 0.43645869\n",
      "Iteration 7, loss = 0.43616717\n",
      "Iteration 8, loss = 0.43487571\n",
      "Iteration 9, loss = 0.43310963\n",
      "Iteration 10, loss = 0.43505049\n",
      "Iteration 11, loss = 0.43332289\n",
      "Iteration 12, loss = 0.43218278\n",
      "Iteration 13, loss = 0.43248189\n",
      "Iteration 14, loss = 0.43209865\n",
      "Iteration 15, loss = 0.43257480\n",
      "Iteration 16, loss = 0.43152554\n",
      "Iteration 17, loss = 0.43191523\n",
      "Iteration 18, loss = 0.43115841\n",
      "Iteration 19, loss = 0.43070009\n",
      "Iteration 20, loss = 0.43089715\n",
      "Iteration 21, loss = 0.43098156\n",
      "Iteration 22, loss = 0.43075572\n",
      "Iteration 23, loss = 0.43022288\n",
      "Iteration 24, loss = 0.43047678\n",
      "Iteration 25, loss = 0.43040968\n",
      "Iteration 26, loss = 0.43064352\n",
      "Iteration 27, loss = 0.42975529\n",
      "Iteration 28, loss = 0.42957937\n",
      "Iteration 29, loss = 0.42977421\n",
      "Iteration 30, loss = 0.42906236\n",
      "Iteration 31, loss = 0.42949312\n",
      "Iteration 32, loss = 0.42830560\n",
      "Iteration 33, loss = 0.42854147\n",
      "Iteration 34, loss = 0.42876144\n",
      "Iteration 35, loss = 0.42893559\n",
      "Iteration 36, loss = 0.42864291\n",
      "Iteration 37, loss = 0.42851683\n",
      "Iteration 38, loss = 0.42866994\n",
      "Iteration 39, loss = 0.42828947\n",
      "Iteration 40, loss = 0.42875027\n",
      "Iteration 41, loss = 0.42843655\n",
      "Iteration 42, loss = 0.42812926\n",
      "Iteration 43, loss = 0.42790300\n",
      "Iteration 44, loss = 0.42760865\n",
      "Iteration 45, loss = 0.42750115\n",
      "Iteration 46, loss = 0.42769538\n",
      "Iteration 47, loss = 0.42735059\n",
      "Iteration 48, loss = 0.42781352\n",
      "Iteration 49, loss = 0.42765192\n",
      "Iteration 50, loss = 0.42736069\n",
      "Iteration 1, loss = 0.48930420\n",
      "Iteration 2, loss = 0.44438980\n",
      "Iteration 3, loss = 0.44080152\n",
      "Iteration 4, loss = 0.43763521\n",
      "Iteration 5, loss = 0.43460890\n",
      "Iteration 6, loss = 0.43498810\n",
      "Iteration 7, loss = 0.43371819\n",
      "Iteration 8, loss = 0.43563210\n",
      "Iteration 9, loss = 0.43365373\n",
      "Iteration 10, loss = 0.43386953\n",
      "Iteration 11, loss = 0.43252387\n",
      "Iteration 12, loss = 0.43194829\n",
      "Iteration 13, loss = 0.43079598\n",
      "Iteration 14, loss = 0.43142607\n",
      "Iteration 15, loss = 0.43073788\n",
      "Iteration 16, loss = 0.42979126\n",
      "Iteration 17, loss = 0.43041967\n",
      "Iteration 18, loss = 0.42963546\n",
      "Iteration 19, loss = 0.42986753\n",
      "Iteration 20, loss = 0.42993868\n",
      "Iteration 21, loss = 0.42929083\n",
      "Iteration 22, loss = 0.42930655\n",
      "Iteration 23, loss = 0.42892042\n",
      "Iteration 24, loss = 0.42882013\n",
      "Iteration 25, loss = 0.42974518\n",
      "Iteration 26, loss = 0.42940938\n",
      "Iteration 27, loss = 0.42903269\n",
      "Iteration 28, loss = 0.42827012\n",
      "Iteration 29, loss = 0.42898949\n",
      "Iteration 30, loss = 0.42804553\n",
      "Iteration 31, loss = 0.42715009\n",
      "Iteration 32, loss = 0.42816566\n",
      "Iteration 33, loss = 0.42807392\n",
      "Iteration 34, loss = 0.42798692\n",
      "Iteration 35, loss = 0.42721995\n",
      "Iteration 36, loss = 0.42695152\n",
      "Iteration 37, loss = 0.42770316\n",
      "Iteration 38, loss = 0.42765407\n",
      "Iteration 39, loss = 0.42718219\n",
      "Iteration 40, loss = 0.42639653\n",
      "Iteration 41, loss = 0.42696887\n",
      "Iteration 42, loss = 0.42684670\n",
      "Iteration 43, loss = 0.42691386\n",
      "Iteration 44, loss = 0.42663892\n",
      "Iteration 45, loss = 0.42642197\n",
      "Iteration 46, loss = 0.42619617\n",
      "Iteration 47, loss = 0.42578617\n",
      "Iteration 48, loss = 0.42635825\n",
      "Iteration 49, loss = 0.42614350\n",
      "Iteration 50, loss = 0.42689628\n",
      "Iteration 1, loss = 0.49001405\n",
      "Iteration 2, loss = 0.44489802\n",
      "Iteration 3, loss = 0.44050974\n",
      "Iteration 4, loss = 0.43992338\n",
      "Iteration 5, loss = 0.43765639\n",
      "Iteration 6, loss = 0.43629475\n",
      "Iteration 7, loss = 0.43513483\n",
      "Iteration 8, loss = 0.43471755\n",
      "Iteration 9, loss = 0.43430339\n",
      "Iteration 10, loss = 0.43326886\n",
      "Iteration 11, loss = 0.43264277\n",
      "Iteration 12, loss = 0.43222847\n",
      "Iteration 13, loss = 0.43315658\n",
      "Iteration 14, loss = 0.43206673\n",
      "Iteration 15, loss = 0.43188136\n",
      "Iteration 16, loss = 0.43036839\n",
      "Iteration 17, loss = 0.43161381\n",
      "Iteration 18, loss = 0.43009759\n",
      "Iteration 19, loss = 0.43043350\n",
      "Iteration 20, loss = 0.43031713\n",
      "Iteration 21, loss = 0.43187975\n",
      "Iteration 22, loss = 0.42898904\n",
      "Iteration 23, loss = 0.43042495\n",
      "Iteration 24, loss = 0.42991467\n",
      "Iteration 25, loss = 0.42947550\n",
      "Iteration 26, loss = 0.42958847\n",
      "Iteration 27, loss = 0.42890117\n",
      "Iteration 28, loss = 0.42901325\n",
      "Iteration 29, loss = 0.42871612\n",
      "Iteration 30, loss = 0.42894691\n",
      "Iteration 31, loss = 0.42826552\n",
      "Iteration 32, loss = 0.42804149\n",
      "Iteration 33, loss = 0.42763932\n",
      "Iteration 34, loss = 0.42800257\n",
      "Iteration 35, loss = 0.42744474\n",
      "Iteration 36, loss = 0.42869408\n",
      "Iteration 37, loss = 0.42851545\n",
      "Iteration 38, loss = 0.42808354\n",
      "Iteration 39, loss = 0.42686049\n",
      "Iteration 40, loss = 0.42719789\n",
      "Iteration 41, loss = 0.42772958\n",
      "Iteration 42, loss = 0.42742687\n",
      "Iteration 43, loss = 0.42743539\n",
      "Iteration 44, loss = 0.42786682\n",
      "Iteration 45, loss = 0.42694125\n",
      "Iteration 46, loss = 0.42703526\n",
      "Iteration 47, loss = 0.42728773\n",
      "Iteration 48, loss = 0.42697957\n",
      "Iteration 49, loss = 0.42722388\n",
      "Iteration 50, loss = 0.42653803\n",
      "Iteration 1, loss = 0.48969817\n",
      "Iteration 2, loss = 0.44384501\n",
      "Iteration 3, loss = 0.43884639\n",
      "Iteration 4, loss = 0.43917457\n",
      "Iteration 5, loss = 0.43558372\n",
      "Iteration 6, loss = 0.43324367\n",
      "Iteration 7, loss = 0.43340242\n",
      "Iteration 8, loss = 0.43343845\n",
      "Iteration 9, loss = 0.43257035\n",
      "Iteration 10, loss = 0.43170065\n",
      "Iteration 11, loss = 0.43132424\n",
      "Iteration 12, loss = 0.43111326\n",
      "Iteration 13, loss = 0.43113364\n",
      "Iteration 14, loss = 0.43115752\n",
      "Iteration 15, loss = 0.43027141\n",
      "Iteration 16, loss = 0.43013611\n",
      "Iteration 17, loss = 0.42929902\n",
      "Iteration 18, loss = 0.43044029\n",
      "Iteration 19, loss = 0.42897847\n",
      "Iteration 20, loss = 0.42847927\n",
      "Iteration 21, loss = 0.42863330\n",
      "Iteration 22, loss = 0.42871279\n",
      "Iteration 23, loss = 0.42959992\n",
      "Iteration 24, loss = 0.42794297\n",
      "Iteration 25, loss = 0.42840701\n",
      "Iteration 26, loss = 0.42735199\n",
      "Iteration 27, loss = 0.42780710\n",
      "Iteration 28, loss = 0.42743007\n",
      "Iteration 29, loss = 0.42801570\n",
      "Iteration 30, loss = 0.42734622\n",
      "Iteration 31, loss = 0.42678603\n",
      "Iteration 32, loss = 0.42685531\n",
      "Iteration 33, loss = 0.42691963\n",
      "Iteration 34, loss = 0.42707387\n",
      "Iteration 35, loss = 0.42704747\n",
      "Iteration 36, loss = 0.42669802\n",
      "Iteration 37, loss = 0.42627669\n",
      "Iteration 38, loss = 0.42659272\n",
      "Iteration 39, loss = 0.42624542\n",
      "Iteration 40, loss = 0.42588643\n",
      "Iteration 41, loss = 0.42572842\n",
      "Iteration 42, loss = 0.42594954\n",
      "Iteration 43, loss = 0.42568412\n",
      "Iteration 44, loss = 0.42584731\n",
      "Iteration 45, loss = 0.42524452\n",
      "Iteration 46, loss = 0.42626411\n",
      "Iteration 47, loss = 0.42530702\n",
      "Iteration 48, loss = 0.42549136\n",
      "Iteration 49, loss = 0.42595957\n",
      "Iteration 50, loss = 0.42462187\n",
      "Iteration 1, loss = 0.48926577\n",
      "Iteration 2, loss = 0.44094463\n",
      "Iteration 3, loss = 0.43772712\n",
      "Iteration 4, loss = 0.43781983\n",
      "Iteration 5, loss = 0.43510205\n",
      "Iteration 6, loss = 0.43604685\n",
      "Iteration 7, loss = 0.43481947\n",
      "Iteration 8, loss = 0.43260952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.43368389\n",
      "Iteration 10, loss = 0.43221611\n",
      "Iteration 11, loss = 0.43237702\n",
      "Iteration 12, loss = 0.43136263\n",
      "Iteration 13, loss = 0.43101907\n",
      "Iteration 14, loss = 0.43198775\n",
      "Iteration 15, loss = 0.42997543\n",
      "Iteration 16, loss = 0.43012937\n",
      "Iteration 17, loss = 0.43003118\n",
      "Iteration 18, loss = 0.43029123\n",
      "Iteration 19, loss = 0.42942091\n",
      "Iteration 20, loss = 0.42949004\n",
      "Iteration 21, loss = 0.42932114\n",
      "Iteration 22, loss = 0.42867187\n",
      "Iteration 23, loss = 0.42783789\n",
      "Iteration 24, loss = 0.42867215\n",
      "Iteration 25, loss = 0.42859311\n",
      "Iteration 26, loss = 0.42846345\n",
      "Iteration 27, loss = 0.42785053\n",
      "Iteration 28, loss = 0.42819339\n",
      "Iteration 29, loss = 0.42732968\n",
      "Iteration 30, loss = 0.42730363\n",
      "Iteration 31, loss = 0.42764442\n",
      "Iteration 32, loss = 0.42720620\n",
      "Iteration 33, loss = 0.42778542\n",
      "Iteration 34, loss = 0.42683936\n",
      "Iteration 35, loss = 0.42737709\n",
      "Iteration 36, loss = 0.42692820\n",
      "Iteration 37, loss = 0.42645579\n",
      "Iteration 38, loss = 0.42675073\n",
      "Iteration 39, loss = 0.42643626\n",
      "Iteration 40, loss = 0.42590986\n",
      "Iteration 41, loss = 0.42596952\n",
      "Iteration 42, loss = 0.42596528\n",
      "Iteration 43, loss = 0.42588694\n",
      "Iteration 44, loss = 0.42619445\n",
      "Iteration 45, loss = 0.42573400\n",
      "Iteration 46, loss = 0.42600583\n",
      "Iteration 47, loss = 0.42587959\n",
      "Iteration 48, loss = 0.42541467\n",
      "Iteration 49, loss = 0.42526857\n",
      "Iteration 50, loss = 0.42507024\n",
      "Iteration 1, loss = 0.48987457\n",
      "Iteration 2, loss = 0.44373633\n",
      "Iteration 3, loss = 0.44030188\n",
      "Iteration 4, loss = 0.43727021\n",
      "Iteration 5, loss = 0.43750778\n",
      "Iteration 6, loss = 0.43504338\n",
      "Iteration 7, loss = 0.43401533\n",
      "Iteration 8, loss = 0.43447729\n",
      "Iteration 9, loss = 0.43405153\n",
      "Iteration 10, loss = 0.43392095\n",
      "Iteration 11, loss = 0.43241073\n",
      "Iteration 12, loss = 0.43144835\n",
      "Iteration 13, loss = 0.43155367\n",
      "Iteration 14, loss = 0.43098606\n",
      "Iteration 15, loss = 0.43080989\n",
      "Iteration 16, loss = 0.43137137\n",
      "Iteration 17, loss = 0.43177584\n",
      "Iteration 18, loss = 0.43012163\n",
      "Iteration 19, loss = 0.42979339\n",
      "Iteration 20, loss = 0.43026063\n",
      "Iteration 21, loss = 0.42936576\n",
      "Iteration 22, loss = 0.42972089\n",
      "Iteration 23, loss = 0.42988342\n",
      "Iteration 24, loss = 0.43032693\n",
      "Iteration 25, loss = 0.42935499\n",
      "Iteration 26, loss = 0.42875423\n",
      "Iteration 27, loss = 0.42941839\n",
      "Iteration 28, loss = 0.42847906\n",
      "Iteration 29, loss = 0.42970086\n",
      "Iteration 30, loss = 0.42847859\n",
      "Iteration 31, loss = 0.42791253\n",
      "Iteration 32, loss = 0.42791956\n",
      "Iteration 33, loss = 0.42773080\n",
      "Iteration 34, loss = 0.42871898\n",
      "Iteration 35, loss = 0.42789483\n",
      "Iteration 36, loss = 0.42801033\n",
      "Iteration 37, loss = 0.42801169\n",
      "Iteration 38, loss = 0.42891662\n",
      "Iteration 39, loss = 0.42754559\n",
      "Iteration 40, loss = 0.42784958\n",
      "Iteration 41, loss = 0.42796685\n",
      "Iteration 42, loss = 0.42716065\n",
      "Iteration 43, loss = 0.42744216\n",
      "Iteration 44, loss = 0.42747279\n",
      "Iteration 45, loss = 0.42684677\n",
      "Iteration 46, loss = 0.42732314\n",
      "Iteration 47, loss = 0.42732834\n",
      "Iteration 48, loss = 0.42788257\n",
      "Iteration 49, loss = 0.42652448\n",
      "Iteration 50, loss = 0.42662885\n",
      "Iteration 1, loss = 0.48913316\n",
      "Iteration 2, loss = 0.44398430\n",
      "Iteration 3, loss = 0.43932213\n",
      "Iteration 4, loss = 0.43915375\n",
      "Iteration 5, loss = 0.43557778\n",
      "Iteration 6, loss = 0.43468721\n",
      "Iteration 7, loss = 0.43328748\n",
      "Iteration 8, loss = 0.43324577\n",
      "Iteration 9, loss = 0.43366977\n",
      "Iteration 10, loss = 0.43181907\n",
      "Iteration 11, loss = 0.43313672\n",
      "Iteration 12, loss = 0.43114678\n",
      "Iteration 13, loss = 0.43141840\n",
      "Iteration 14, loss = 0.43173044\n",
      "Iteration 15, loss = 0.43038943\n",
      "Iteration 16, loss = 0.43061417\n",
      "Iteration 17, loss = 0.42998254\n",
      "Iteration 18, loss = 0.42930666\n",
      "Iteration 19, loss = 0.42933517\n",
      "Iteration 20, loss = 0.42993143\n",
      "Iteration 21, loss = 0.42890941\n",
      "Iteration 22, loss = 0.42945778\n",
      "Iteration 23, loss = 0.42896198\n",
      "Iteration 24, loss = 0.42898661\n",
      "Iteration 25, loss = 0.42924002\n",
      "Iteration 26, loss = 0.42836195\n",
      "Iteration 27, loss = 0.42799895\n",
      "Iteration 28, loss = 0.42820893\n",
      "Iteration 29, loss = 0.42795700\n",
      "Iteration 30, loss = 0.42779155\n",
      "Iteration 31, loss = 0.42752632\n",
      "Iteration 32, loss = 0.42735953\n",
      "Iteration 33, loss = 0.42771731\n",
      "Iteration 34, loss = 0.42726577\n",
      "Iteration 35, loss = 0.42646785\n",
      "Iteration 36, loss = 0.42680399\n",
      "Iteration 37, loss = 0.42703136\n",
      "Iteration 38, loss = 0.42703972\n",
      "Iteration 39, loss = 0.42672354\n",
      "Iteration 40, loss = 0.42630212\n",
      "Iteration 41, loss = 0.42656163\n",
      "Iteration 42, loss = 0.42696391\n",
      "Iteration 43, loss = 0.42706639\n",
      "Iteration 44, loss = 0.42624134\n",
      "Iteration 45, loss = 0.42623010\n",
      "Iteration 46, loss = 0.42617890\n",
      "Iteration 47, loss = 0.42560433\n",
      "Iteration 48, loss = 0.42620267\n",
      "Iteration 49, loss = 0.42684012\n",
      "Iteration 50, loss = 0.42576255\n",
      "Iteration 1, loss = 0.48783112\n",
      "Iteration 2, loss = 0.44176252\n",
      "Iteration 3, loss = 0.43739923\n",
      "Iteration 4, loss = 0.43563335\n",
      "Iteration 5, loss = 0.43660969\n",
      "Iteration 6, loss = 0.43308397\n",
      "Iteration 7, loss = 0.43319229\n",
      "Iteration 8, loss = 0.43299629\n",
      "Iteration 9, loss = 0.43126184\n",
      "Iteration 10, loss = 0.43154516\n",
      "Iteration 11, loss = 0.43070904\n",
      "Iteration 12, loss = 0.43091648\n",
      "Iteration 13, loss = 0.42922927\n",
      "Iteration 14, loss = 0.42955233\n",
      "Iteration 15, loss = 0.42996990\n",
      "Iteration 16, loss = 0.42868621\n",
      "Iteration 17, loss = 0.42898373\n",
      "Iteration 18, loss = 0.42882908\n",
      "Iteration 19, loss = 0.42837130\n",
      "Iteration 20, loss = 0.42888563\n",
      "Iteration 21, loss = 0.42824536\n",
      "Iteration 22, loss = 0.42770905\n",
      "Iteration 23, loss = 0.42738169\n",
      "Iteration 24, loss = 0.42765564\n",
      "Iteration 25, loss = 0.42751858\n",
      "Iteration 26, loss = 0.42657910\n",
      "Iteration 27, loss = 0.42757581\n",
      "Iteration 28, loss = 0.42715951\n",
      "Iteration 29, loss = 0.42748266\n",
      "Iteration 30, loss = 0.42686849\n",
      "Iteration 31, loss = 0.42661505\n",
      "Iteration 32, loss = 0.42660537\n",
      "Iteration 33, loss = 0.42623596\n",
      "Iteration 34, loss = 0.42615667\n",
      "Iteration 35, loss = 0.42691189\n",
      "Iteration 36, loss = 0.42604998\n",
      "Iteration 37, loss = 0.42544086\n",
      "Iteration 38, loss = 0.42538480\n",
      "Iteration 39, loss = 0.42518073\n",
      "Iteration 40, loss = 0.42485096\n",
      "Iteration 41, loss = 0.42568272\n",
      "Iteration 42, loss = 0.42552439\n",
      "Iteration 43, loss = 0.42558601\n",
      "Iteration 44, loss = 0.42459741\n",
      "Iteration 45, loss = 0.42476611\n",
      "Iteration 46, loss = 0.42492767\n",
      "Iteration 47, loss = 0.42469340\n",
      "Iteration 48, loss = 0.42401191\n",
      "Iteration 49, loss = 0.42433052\n",
      "Iteration 50, loss = 0.42405107\n",
      "Iteration 1, loss = 0.49154228\n",
      "Iteration 2, loss = 0.44447081\n",
      "Iteration 3, loss = 0.43965712\n",
      "Iteration 4, loss = 0.43831737\n",
      "Iteration 5, loss = 0.43778003\n",
      "Iteration 6, loss = 0.43558070\n",
      "Iteration 7, loss = 0.43513557\n",
      "Iteration 8, loss = 0.43419936\n",
      "Iteration 9, loss = 0.43329285\n",
      "Iteration 10, loss = 0.43269895\n",
      "Iteration 11, loss = 0.43233357\n",
      "Iteration 12, loss = 0.43208575\n",
      "Iteration 13, loss = 0.43170191\n",
      "Iteration 14, loss = 0.43133253\n",
      "Iteration 15, loss = 0.43162352\n",
      "Iteration 16, loss = 0.43115107\n",
      "Iteration 17, loss = 0.43110101\n",
      "Iteration 18, loss = 0.43078105\n",
      "Iteration 19, loss = 0.43137092\n",
      "Iteration 20, loss = 0.42934840\n",
      "Iteration 21, loss = 0.42978250\n",
      "Iteration 22, loss = 0.42944212\n",
      "Iteration 23, loss = 0.42967617\n",
      "Iteration 24, loss = 0.42926173\n",
      "Iteration 25, loss = 0.42969441\n",
      "Iteration 26, loss = 0.42899284\n",
      "Iteration 27, loss = 0.42919487\n",
      "Iteration 28, loss = 0.42857374\n",
      "Iteration 29, loss = 0.42811056\n",
      "Iteration 30, loss = 0.42848648\n",
      "Iteration 31, loss = 0.42836813\n",
      "Iteration 32, loss = 0.42738329\n",
      "Iteration 33, loss = 0.42769523\n",
      "Iteration 34, loss = 0.42798217\n",
      "Iteration 35, loss = 0.42779845\n",
      "Iteration 36, loss = 0.42806442\n",
      "Iteration 37, loss = 0.42785396\n",
      "Iteration 38, loss = 0.42773542\n",
      "Iteration 39, loss = 0.42715044\n",
      "Iteration 40, loss = 0.42727312\n",
      "Iteration 41, loss = 0.42663235\n",
      "Iteration 42, loss = 0.42685311\n",
      "Iteration 43, loss = 0.42681357\n",
      "Iteration 44, loss = 0.42673233\n",
      "Iteration 45, loss = 0.42694720\n",
      "Iteration 46, loss = 0.42673860\n",
      "Iteration 47, loss = 0.42679978\n",
      "Iteration 48, loss = 0.42663454\n",
      "Iteration 49, loss = 0.42679231\n",
      "Iteration 50, loss = 0.42658891\n",
      "Iteration 1, loss = 0.48823951\n",
      "Iteration 2, loss = 0.44445983\n",
      "Iteration 3, loss = 0.43973964\n",
      "Iteration 4, loss = 0.43880811\n",
      "Iteration 5, loss = 0.43783704\n",
      "Iteration 6, loss = 0.43566718\n",
      "Iteration 7, loss = 0.43514664\n",
      "Iteration 8, loss = 0.43419673\n",
      "Iteration 9, loss = 0.43347769\n",
      "Iteration 10, loss = 0.43253147\n",
      "Iteration 11, loss = 0.43300666\n",
      "Iteration 12, loss = 0.43296255\n",
      "Iteration 13, loss = 0.43211739\n",
      "Iteration 14, loss = 0.43199526\n",
      "Iteration 15, loss = 0.43057736\n",
      "Iteration 16, loss = 0.43054979\n",
      "Iteration 17, loss = 0.43058418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.42979854\n",
      "Iteration 19, loss = 0.43038225\n",
      "Iteration 20, loss = 0.43053757\n",
      "Iteration 21, loss = 0.43020784\n",
      "Iteration 22, loss = 0.42911532\n",
      "Iteration 23, loss = 0.43018895\n",
      "Iteration 24, loss = 0.42890403\n",
      "Iteration 25, loss = 0.43033329\n",
      "Iteration 26, loss = 0.42929037\n",
      "Iteration 27, loss = 0.42944435\n",
      "Iteration 28, loss = 0.42948944\n",
      "Iteration 29, loss = 0.42830639\n",
      "Iteration 30, loss = 0.42861187\n",
      "Iteration 31, loss = 0.42917564\n",
      "Iteration 32, loss = 0.42784359\n",
      "Iteration 33, loss = 0.42863984\n",
      "Iteration 34, loss = 0.42819345\n",
      "Iteration 35, loss = 0.42799075\n",
      "Iteration 36, loss = 0.42807805\n",
      "Iteration 37, loss = 0.42778768\n",
      "Iteration 38, loss = 0.42754537\n",
      "Iteration 39, loss = 0.42711522\n",
      "Iteration 40, loss = 0.42713326\n",
      "Iteration 41, loss = 0.42695135\n",
      "Iteration 42, loss = 0.42725782\n",
      "Iteration 43, loss = 0.42680421\n",
      "Iteration 44, loss = 0.42679777\n",
      "Iteration 45, loss = 0.42694324\n",
      "Iteration 46, loss = 0.42646305\n",
      "Iteration 47, loss = 0.42679180\n",
      "Iteration 48, loss = 0.42615184\n",
      "Iteration 49, loss = 0.42669704\n",
      "Iteration 50, loss = 0.42674075\n",
      "Train Accuracy: 0.7241\n",
      "Running Memory: 0.03 MB\n",
      "Running Time: 104.99 Seconds\n",
      "Test  Accuracy: 0.7135\n",
      "Test  Presion:  0.6556\n",
      "Test  Recall:   0.9389\n",
      "Test  F1-score: 0.7721\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS1_tfidf_3, memory_MLP_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS1_tfidf_3, acc_MLP_DS1_tfidf_3, pre_MLP_DS1_tfidf_3, rec_MLP_DS1_tfidf_3, f1_MLP_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS1_tfidf_3, model_3, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8989\n",
      "Running Memory: 0.02 MB\n",
      "Running Time: 19.94 Seconds\n",
      "Test  Accuracy: 0.9031\n",
      "Test  Presion:  0.9426\n",
      "Test  Recall:   0.8652\n",
      "Test  F1-score: 0.9023\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS1_bow_1, memory_KNN_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS1_bow_1, acc_KNN_DS1_bow_1, pre_KNN_DS1_bow_1, rec_KNN_DS1_bow_1, f1_KNN_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS1_bow_1, model_4, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8558\n",
      "Running Memory: 0.45 MB\n",
      "Running Time: 21.50 Seconds\n",
      "Test  Accuracy: 0.8561\n",
      "Test  Presion:  0.8263\n",
      "Test  Recall:   0.9137\n",
      "Test  F1-score: 0.8678\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS1_bow_2, memory_KNN_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS1_bow_2, acc_KNN_DS1_bow_2, pre_KNN_DS1_bow_2, rec_KNN_DS1_bow_2, f1_KNN_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS1_bow_2, model_4, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7207\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 19.12 Seconds\n",
      "Test  Accuracy: 0.7127\n",
      "Test  Presion:  0.6550\n",
      "Test  Recall:   0.9384\n",
      "Test  F1-score: 0.7715\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS1_bow_3, memory_KNN_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS1_bow_3, acc_KNN_DS1_bow_3, pre_KNN_DS1_bow_3, rec_KNN_DS1_bow_3, f1_KNN_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS1_bow_3, model_4, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8907\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 20.92 Seconds\n",
      "Test  Accuracy: 0.8965\n",
      "Test  Presion:  0.9349\n",
      "Test  Recall:   0.8596\n",
      "Test  F1-score: 0.8957\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS1_tfidf_1, memory_KNN_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS1_tfidf_1, acc_KNN_DS1_tfidf_1, pre_KNN_DS1_tfidf_1, rec_KNN_DS1_tfidf_1, f1_KNN_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS1_tfidf_1, model_4, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8493\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 20.95 Seconds\n",
      "Test  Accuracy: 0.8606\n",
      "Test  Presion:  0.8427\n",
      "Test  Recall:   0.8979\n",
      "Test  F1-score: 0.8694\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS1_tfidf_2, memory_KNN_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS1_tfidf_2, acc_KNN_DS1_tfidf_2, pre_KNN_DS1_tfidf_2, rec_KNN_DS1_tfidf_2, f1_KNN_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS1_tfidf_2, model_4, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7110\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 19.71 Seconds\n",
      "Test  Accuracy: 0.7027\n",
      "Test  Presion:  0.6465\n",
      "Test  Recall:   0.9372\n",
      "Test  F1-score: 0.7652\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS1_tfidf_3, memory_KNN_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS1_tfidf_3, acc_KNN_DS1_tfidf_3, pre_KNN_DS1_tfidf_3, rec_KNN_DS1_tfidf_3, f1_KNN_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS1_tfidf_3, model_4, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9948\n",
      "Running Memory: 1.22 MB\n",
      "Running Time: 27.45 Seconds\n",
      "Test  Accuracy: 0.9955\n",
      "Test  Presion:  0.9984\n",
      "Test  Recall:   0.9928\n",
      "Test  F1-score: 0.9956\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS1_bow_1, memory_RF_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS1_bow_1, acc_RF_DS1_bow_1, pre_RF_DS1_bow_1, rec_RF_DS1_bow_1, f1_RF_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS1_bow_1, model_5, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8943\n",
      "Running Memory: 2.34 MB\n",
      "Running Time: 24.95 Seconds\n",
      "Test  Accuracy: 0.8896\n",
      "Test  Presion:  0.8614\n",
      "Test  Recall:   0.9372\n",
      "Test  F1-score: 0.8977\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS1_bow_2, memory_RF_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS1_bow_2, acc_RF_DS1_bow_2, pre_RF_DS1_bow_2, rec_RF_DS1_bow_2, f1_RF_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS1_bow_2, model_5, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7315\n",
      "Running Memory: 0.03 MB\n",
      "Running Time: 22.63 Seconds\n",
      "Test  Accuracy: 0.7371\n",
      "Test  Presion:  0.9521\n",
      "Test  Recall:   0.5173\n",
      "Test  F1-score: 0.6704\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS1_bow_3, memory_RF_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS1_bow_3, acc_RF_DS1_bow_3, pre_RF_DS1_bow_3, rec_RF_DS1_bow_3, f1_RF_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS1_bow_3, model_5, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9947\n",
      "Running Memory: 12.70 MB\n",
      "Running Time: 59.89 Seconds\n",
      "Test  Accuracy: 0.9956\n",
      "Test  Presion:  0.9984\n",
      "Test  Recall:   0.9931\n",
      "Test  F1-score: 0.9958\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS1_tfidf_1, memory_RF_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS1_tfidf_1, acc_RF_DS1_tfidf_1, pre_RF_DS1_tfidf_1, rec_RF_DS1_tfidf_1, f1_RF_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS1_tfidf_1, model_5, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8947\n",
      "Running Memory: 27.76 MB\n",
      "Running Time: 36.63 Seconds\n",
      "Test  Accuracy: 0.8898\n",
      "Test  Presion:  0.8644\n",
      "Test  Recall:   0.9331\n",
      "Test  F1-score: 0.8974\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS1_tfidf_2, memory_RF_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS1_tfidf_2, acc_RF_DS1_tfidf_2, pre_RF_DS1_tfidf_2, rec_RF_DS1_tfidf_2, f1_RF_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS1_tfidf_2, model_5, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7316\n",
      "Running Memory: 14.38 MB\n",
      "Running Time: 29.85 Seconds\n",
      "Test  Accuracy: 0.7367\n",
      "Test  Presion:  0.9547\n",
      "Test  Recall:   0.5149\n",
      "Test  F1-score: 0.6690\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS1_tfidf_3, memory_RF_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS1_tfidf_3, acc_RF_DS1_tfidf_3, pre_RF_DS1_tfidf_3, rec_RF_DS1_tfidf_3, f1_RF_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS1_tfidf_3, model_5, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier (RF,LR,KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9934\n",
      "Running Memory: 18.86 MB\n",
      "Running Time: 60.93 Seconds\n",
      "Test  Accuracy: 0.9939\n",
      "Test  Presion:  0.9980\n",
      "Test  Recall:   0.9902\n",
      "Test  F1-score: 0.9941\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS1_bow_1, memory_VOT_1_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS1_bow_1, acc_VOT_1_DS1_bow_1, pre_VOT_1_DS1_bow_1, rec_VOT_1_DS1_bow_1, f1_VOT_1_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS1_bow_1, model_6, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8870\n",
      "Running Memory: 35.91 MB\n",
      "Running Time: 65.10 Seconds\n",
      "Test  Accuracy: 0.8825\n",
      "Test  Presion:  0.8523\n",
      "Test  Recall:   0.9345\n",
      "Test  F1-score: 0.8915\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS1_bow_2, memory_VOT_1_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS1_bow_2, acc_VOT_1_DS1_bow_2, pre_VOT_1_DS1_bow_2, rec_VOT_1_DS1_bow_2, f1_VOT_1_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS1_bow_2, model_6, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7311\n",
      "Running Memory: 43.04 MB\n",
      "Running Time: 52.30 Seconds\n",
      "Test  Accuracy: 0.7366\n",
      "Test  Presion:  0.9521\n",
      "Test  Recall:   0.5163\n",
      "Test  F1-score: 0.6695\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS1_bow_3, memory_VOT_1_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS1_bow_3, acc_VOT_1_DS1_bow_3, pre_VOT_1_DS1_bow_3, rec_VOT_1_DS1_bow_3, f1_VOT_1_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS1_bow_3, model_6, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9872\n",
      "Running Memory: 57.05 MB\n",
      "Running Time: 100.26 Seconds\n",
      "Test  Accuracy: 0.9878\n",
      "Test  Presion:  0.9888\n",
      "Test  Recall:   0.9875\n",
      "Test  F1-score: 0.9881\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS1_tfidf_1, memory_VOT_1_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS1_tfidf_1, acc_VOT_1_DS1_tfidf_1, pre_VOT_1_DS1_tfidf_1, rec_VOT_1_DS1_tfidf_1, f1_VOT_1_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS1_tfidf_1, model_6, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8869\n",
      "Running Memory: 31.31 MB\n",
      "Running Time: 68.85 Seconds\n",
      "Test  Accuracy: 0.8823\n",
      "Test  Presion:  0.8627\n",
      "Test  Recall:   0.9185\n",
      "Test  F1-score: 0.8897\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS1_tfidf_2, memory_VOT_1_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS1_tfidf_2, acc_VOT_1_DS1_tfidf_2, pre_VOT_1_DS1_tfidf_2, rec_VOT_1_DS1_tfidf_2, f1_VOT_1_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS1_tfidf_2, model_6, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7296\n",
      "Running Memory: 2.27 MB\n",
      "Running Time: 54.27 Seconds\n",
      "Test  Accuracy: 0.7341\n",
      "Test  Presion:  0.9605\n",
      "Test  Recall:   0.5062\n",
      "Test  F1-score: 0.6630\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS1_tfidf_3, memory_VOT_1_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS1_tfidf_3, acc_VOT_1_DS1_tfidf_3, pre_VOT_1_DS1_tfidf_3, rec_VOT_1_DS1_tfidf_3, f1_VOT_1_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS1_tfidf_3, model_6, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier (LR, LSVM, CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9926\n",
      "Running Memory: 44.74 MB\n",
      "Running Time: 38.35 Seconds\n",
      "Test  Accuracy: 0.9925\n",
      "Test  Presion:  0.9977\n",
      "Test  Recall:   0.9878\n",
      "Test  F1-score: 0.9927\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS1_bow_1, memory_VOT_2_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS1_bow_1, acc_VOT_2_DS1_bow_1, pre_VOT_2_DS1_bow_1, rec_VOT_2_DS1_bow_1, f1_VOT_2_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS1_bow_1, model_7, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8734\n",
      "Running Memory: 43.13 MB\n",
      "Running Time: 14.94 Seconds\n",
      "Test  Accuracy: 0.8682\n",
      "Test  Presion:  0.8401\n",
      "Test  Recall:   0.9201\n",
      "Test  F1-score: 0.8783\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS1_bow_2, memory_VOT_2_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS1_bow_2, acc_VOT_2_DS1_bow_2, pre_VOT_2_DS1_bow_2, rec_VOT_2_DS1_bow_2, f1_VOT_2_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS1_bow_2, model_7, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7272\n",
      "Running Memory: 43.18 MB\n",
      "Running Time: 12.85 Seconds\n",
      "Test  Accuracy: 0.7287\n",
      "Test  Presion:  0.9592\n",
      "Test  Recall:   0.4960\n",
      "Test  F1-score: 0.6539\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS1_bow_3, memory_VOT_2_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS1_bow_3, acc_VOT_2_DS1_bow_3, pre_VOT_2_DS1_bow_3, rec_VOT_2_DS1_bow_3, f1_VOT_2_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS1_bow_3, model_7, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9882\n",
      "Running Memory: 44.68 MB\n",
      "Running Time: 11.56 Seconds\n",
      "Test  Accuracy: 0.9871\n",
      "Test  Presion:  0.9861\n",
      "Test  Recall:   0.9889\n",
      "Test  F1-score: 0.9875\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS1_tfidf_1, memory_VOT_2_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS1_tfidf_1, acc_VOT_2_DS1_tfidf_1, pre_VOT_2_DS1_tfidf_1, rec_VOT_2_DS1_tfidf_1, f1_VOT_2_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS1_tfidf_1, model_7, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8759\n",
      "Running Memory: 43.16 MB\n",
      "Running Time: 6.89 Seconds\n",
      "Test  Accuracy: 0.8716\n",
      "Test  Presion:  0.8578\n",
      "Test  Recall:   0.9007\n",
      "Test  F1-score: 0.8788\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS1_tfidf_2, memory_VOT_2_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS1_tfidf_2, acc_VOT_2_DS1_tfidf_2, pre_VOT_2_DS1_tfidf_2, rec_VOT_2_DS1_tfidf_2, f1_VOT_2_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS1_tfidf_2, model_7, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7253\n",
      "Running Memory: 43.05 MB\n",
      "Running Time: 6.97 Seconds\n",
      "Test  Accuracy: 0.7279\n",
      "Test  Presion:  0.9609\n",
      "Test  Recall:   0.4936\n",
      "Test  F1-score: 0.6522\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS1_tfidf_3, memory_VOT_2_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS1_tfidf_3, acc_VOT_2_DS1_tfidf_3, pre_VOT_2_DS1_tfidf_3, rec_VOT_2_DS1_tfidf_3, f1_VOT_2_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS1_tfidf_3, model_7, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging classifier (decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9946\n",
      "Running Memory: 5.36 MB\n",
      "Running Time: 198.24 Seconds\n",
      "Test  Accuracy: 0.9955\n",
      "Test  Presion:  0.9981\n",
      "Test  Recall:   0.9932\n",
      "Test  F1-score: 0.9957\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS1_bow_1, memory_BAGG_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS1_bow_1, acc_BAGG_DS1_bow_1, pre_BAGG_DS1_bow_1, rec_BAGG_DS1_bow_1, f1_BAGG_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS1_bow_1, model_8, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8884\n",
      "Running Memory: 17.77 MB\n",
      "Running Time: 93.11 Seconds\n",
      "Test  Accuracy: 0.8837\n",
      "Test  Presion:  0.8547\n",
      "Test  Recall:   0.9336\n",
      "Test  F1-score: 0.8924\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS1_bow_2, memory_BAGG_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS1_bow_2, acc_BAGG_DS1_bow_2, pre_BAGG_DS1_bow_2, rec_BAGG_DS1_bow_2, f1_BAGG_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS1_bow_2, model_8, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7314\n",
      "Running Memory: 16.04 MB\n",
      "Running Time: 88.21 Seconds\n",
      "Test  Accuracy: 0.7360\n",
      "Test  Presion:  0.9481\n",
      "Test  Recall:   0.5175\n",
      "Test  F1-score: 0.6695\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS1_bow_3, memory_BAGG_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS1_bow_3, acc_BAGG_DS1_bow_3, pre_BAGG_DS1_bow_3, rec_BAGG_DS1_bow_3, f1_BAGG_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS1_bow_3, model_8, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9946\n",
      "Running Memory: 2.73 MB\n",
      "Running Time: 476.38 Seconds\n",
      "Test  Accuracy: 0.9955\n",
      "Test  Presion:  0.9981\n",
      "Test  Recall:   0.9931\n",
      "Test  F1-score: 0.9956\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS1_tfidf_1, memory_BAGG_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS1_tfidf_1, acc_BAGG_DS1_tfidf_1, pre_BAGG_DS1_tfidf_1, rec_BAGG_DS1_tfidf_1, f1_BAGG_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS1_tfidf_1, model_8, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8915\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 174.29 Seconds\n",
      "Test  Accuracy: 0.8878\n",
      "Test  Presion:  0.8617\n",
      "Test  Recall:   0.9326\n",
      "Test  F1-score: 0.8958\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS1_tfidf_2, memory_BAGG_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS1_tfidf_2, acc_BAGG_DS1_tfidf_2, pre_BAGG_DS1_tfidf_2, rec_BAGG_DS1_tfidf_2, f1_BAGG_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS1_tfidf_2, model_8, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7313\n",
      "Running Memory: 0.67 MB\n",
      "Running Time: 101.94 Seconds\n",
      "Test  Accuracy: 0.7362\n",
      "Test  Presion:  0.9527\n",
      "Test  Recall:   0.5152\n",
      "Test  F1-score: 0.6687\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS1_tfidf_3, memory_BAGG_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS1_tfidf_3, acc_BAGG_DS1_tfidf_3, pre_BAGG_DS1_tfidf_3, rec_BAGG_DS1_tfidf_3, f1_BAGG_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS1_tfidf_3, model_8, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost (Boosting classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9927\n",
      "Running Memory: 0.15 MB\n",
      "Running Time: 22.41 Seconds\n",
      "Test  Accuracy: 0.9923\n",
      "Test  Presion:  0.9974\n",
      "Test  Recall:   0.9876\n",
      "Test  F1-score: 0.9925\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS1_bow_1, memory_ADA_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS1_bow_1, acc_ADA_DS1_bow_1, pre_ADA_DS1_bow_1, rec_ADA_DS1_bow_1, f1_ADA_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS1_bow_1, model_9, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8724\n",
      "Running Memory: 0.04 MB\n",
      "Running Time: 15.57 Seconds\n",
      "Test  Accuracy: 0.8665\n",
      "Test  Presion:  0.8346\n",
      "Test  Recall:   0.9250\n",
      "Test  F1-score: 0.8775\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS1_bow_2, memory_ADA_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS1_bow_2, acc_ADA_DS1_bow_2, pre_ADA_DS1_bow_2, rec_ADA_DS1_bow_2, f1_ADA_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS1_bow_2, model_9, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7260\n",
      "Running Memory: 0.41 MB\n",
      "Running Time: 15.24 Seconds\n",
      "Test  Accuracy: 0.7261\n",
      "Test  Presion:  0.9557\n",
      "Test  Recall:   0.4929\n",
      "Test  F1-score: 0.6504\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS1_bow_3, memory_ADA_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS1_bow_3, acc_ADA_DS1_bow_3, pre_ADA_DS1_bow_3, rec_ADA_DS1_bow_3, f1_ADA_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS1_bow_3, model_9, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9925\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 53.21 Seconds\n",
      "Test  Accuracy: 0.9918\n",
      "Test  Presion:  0.9971\n",
      "Test  Recall:   0.9869\n",
      "Test  F1-score: 0.9920\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS1_tfidf_1, memory_ADA_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS1_tfidf_1, acc_ADA_DS1_tfidf_1, pre_ADA_DS1_tfidf_1, rec_ADA_DS1_tfidf_1, f1_ADA_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS1_tfidf_1, model_9, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8775\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 29.07 Seconds\n",
      "Test  Accuracy: 0.8722\n",
      "Test  Presion:  0.8512\n",
      "Test  Recall:   0.9121\n",
      "Test  F1-score: 0.8806\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS1_tfidf_2, memory_ADA_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS1_tfidf_2, acc_ADA_DS1_tfidf_2, pre_ADA_DS1_tfidf_2, rec_ADA_DS1_tfidf_2, f1_ADA_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS1_tfidf_2, model_9, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7293\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 22.31 Seconds\n",
      "Test  Accuracy: 0.7316\n",
      "Test  Presion:  0.9601\n",
      "Test  Recall:   0.5014\n",
      "Test  F1-score: 0.6587\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS1_tfidf_3, memory_ADA_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS1_tfidf_3, acc_ADA_DS1_tfidf_3, pre_ADA_DS1_tfidf_3, rec_ADA_DS1_tfidf_3, f1_ADA_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS1_tfidf_3, model_9, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Boosting classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9949\n",
      "Running Memory: 8.98 MB\n",
      "Running Time: 35.99 Seconds\n",
      "Test  Accuracy: 0.9953\n",
      "Test  Presion:  0.9971\n",
      "Test  Recall:   0.9938\n",
      "Test  F1-score: 0.9955\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS1_bow_1, memory_XG_DS1_bow_1 = train(DS1_bow_1_x_train, DS1_bow_1_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS1_bow_1, acc_XG_DS1_bow_1, pre_XG_DS1_bow_1, rec_XG_DS1_bow_1, f1_XG_DS1_bow_1 = predict(DS1_bow_1_x_test, DS1_bow_1_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS1_bow_1, model_10, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8893\n",
      "Running Memory: 0.41 MB\n",
      "Running Time: 29.30 Seconds\n",
      "Test  Accuracy: 0.8837\n",
      "Test  Presion:  0.8570\n",
      "Test  Recall:   0.9300\n",
      "Test  F1-score: 0.8920\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS1_bow_2, memory_XG_DS1_bow_2 = train(DS1_bow_2_x_train, DS1_bow_2_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS1_bow_2, acc_XG_DS1_bow_2, pre_XG_DS1_bow_2, rec_XG_DS1_bow_2, f1_XG_DS1_bow_2 = predict(DS1_bow_2_x_test, DS1_bow_2_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS1_bow_2, model_10, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7313\n",
      "Running Memory: 0.14 MB\n",
      "Running Time: 25.48 Seconds\n",
      "Test  Accuracy: 0.7350\n",
      "Test  Presion:  0.9559\n",
      "Test  Recall:   0.5107\n",
      "Test  F1-score: 0.6657\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS1_bow_3, memory_XG_DS1_bow_3 = train(DS1_bow_3_x_train, DS1_bow_3_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS1_bow_3, acc_XG_DS1_bow_3, pre_XG_DS1_bow_3, rec_XG_DS1_bow_3, f1_XG_DS1_bow_3 = predict(DS1_bow_3_x_test, DS1_bow_3_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS1_bow_3, model_10, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9948\n",
      "Running Memory: 0.18 MB\n",
      "Running Time: 61.53 Seconds\n",
      "Test  Accuracy: 0.9955\n",
      "Test  Presion:  0.9978\n",
      "Test  Recall:   0.9934\n",
      "Test  F1-score: 0.9956\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS1_tfidf_1, memory_XG_DS1_tfidf_1 = train(DS1_tfidf_1_x_train, DS1_tfidf_1_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS1_tfidf_1, acc_XG_DS1_tfidf_1, pre_XG_DS1_tfidf_1, rec_XG_DS1_tfidf_1, f1_XG_DS1_tfidf_1 = predict(DS1_tfidf_1_x_test, DS1_tfidf_1_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS1_tfidf_1, model_10, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8902\n",
      "Running Memory: 4.77 MB\n",
      "Running Time: 38.39 Seconds\n",
      "Test  Accuracy: 0.8840\n",
      "Test  Presion:  0.8563\n",
      "Test  Recall:   0.9320\n",
      "Test  F1-score: 0.8926\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS1_tfidf_2, memory_XG_DS1_tfidf_2 = train(DS1_tfidf_2_x_train, DS1_tfidf_2_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS1_tfidf_2, acc_XG_DS1_tfidf_2, pre_XG_DS1_tfidf_2, rec_XG_DS1_tfidf_2, f1_XG_DS1_tfidf_2 = predict(DS1_tfidf_2_x_test, DS1_tfidf_2_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS1_tfidf_2, model_10, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7315\n",
      "Running Memory: 0.05 MB\n",
      "Running Time: 25.31 Seconds\n",
      "Test  Accuracy: 0.7362\n",
      "Test  Presion:  0.9551\n",
      "Test  Recall:   0.5136\n",
      "Test  F1-score: 0.6680\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS1_tfidf_3, memory_XG_DS1_tfidf_3 = train(DS1_tfidf_3_x_train, DS1_tfidf_3_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS1_tfidf_3, acc_XG_DS1_tfidf_3, pre_XG_DS1_tfidf_3, rec_XG_DS1_tfidf_3, f1_XG_DS1_tfidf_3 = predict(DS1_tfidf_3_x_test, DS1_tfidf_3_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS1_tfidf_3, model_10, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7716\n",
      "Running Memory: 44.49 MB\n",
      "Running Time: 1.07 Seconds\n",
      "Test  Accuracy: 0.7704\n",
      "Test  Presion:  0.7145\n",
      "Test  Recall:   0.8835\n",
      "Test  F1-score: 0.7901\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS2_bow_1, memory_LR_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS2_bow_1, acc_LR_DS2_bow_1, pre_LR_DS2_bow_1, rec_LR_DS2_bow_1, f1_LR_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS2_bow_1, model_1, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8201\n",
      "Running Memory: 4.15 MB\n",
      "Running Time: 0.87 Seconds\n",
      "Test  Accuracy: 0.8125\n",
      "Test  Presion:  0.7610\n",
      "Test  Recall:   0.8989\n",
      "Test  F1-score: 0.8242\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS2_bow_2, memory_LR_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS2_bow_2, acc_LR_DS2_bow_2, pre_LR_DS2_bow_2, rec_LR_DS2_bow_2, f1_LR_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS2_bow_2, model_1, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8184\n",
      "Running Memory: 4.17 MB\n",
      "Running Time: 1.18 Seconds\n",
      "Test  Accuracy: 0.8167\n",
      "Test  Presion:  0.7621\n",
      "Test  Recall:   0.9088\n",
      "Test  F1-score: 0.8290\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS2_bow_3, memory_LR_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS2_bow_3, acc_LR_DS2_bow_3, pre_LR_DS2_bow_3, rec_LR_DS2_bow_3, f1_LR_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS2_bow_3, model_1, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7756\n",
      "Running Memory: 4.30 MB\n",
      "Running Time: 0.68 Seconds\n",
      "Test  Accuracy: 0.7790\n",
      "Test  Presion:  0.7563\n",
      "Test  Recall:   0.8085\n",
      "Test  F1-score: 0.7815\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS2_tfidf_1, memory_LR_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS2_tfidf_1, acc_LR_DS2_tfidf_1, pre_LR_DS2_tfidf_1, rec_LR_DS2_tfidf_1, f1_LR_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS2_tfidf_1, model_1, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8128\n",
      "Running Memory: 5.41 MB\n",
      "Running Time: 0.85 Seconds\n",
      "Test  Accuracy: 0.8056\n",
      "Test  Presion:  0.7832\n",
      "Test  Recall:   0.8331\n",
      "Test  F1-score: 0.8074\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS2_tfidf_2, memory_LR_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS2_tfidf_2, acc_LR_DS2_tfidf_2, pre_LR_DS2_tfidf_2, rec_LR_DS2_tfidf_2, f1_LR_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS2_tfidf_2, model_1, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8115\n",
      "Running Memory: 5.41 MB\n",
      "Running Time: 1.30 Seconds\n",
      "Test  Accuracy: 0.8097\n",
      "Test  Presion:  0.7652\n",
      "Test  Recall:   0.8813\n",
      "Test  F1-score: 0.8192\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS2_tfidf_3, memory_LR_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS2_tfidf_3, acc_LR_DS2_tfidf_3, pre_LR_DS2_tfidf_3, rec_LR_DS2_tfidf_3, f1_LR_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS2_tfidf_3, model_1, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7655\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 25.00 Seconds\n",
      "Test  Accuracy: 0.7633\n",
      "Test  Presion:  0.7157\n",
      "Test  Recall:   0.8560\n",
      "Test  F1-score: 0.7796\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS2_bow_1, memory_LSVM_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS2_bow_1, acc_LSVM_DS2_bow_1, pre_LSVM_DS2_bow_1, rec_LSVM_DS2_bow_1, f1_LSVM_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS2_bow_1, model_2, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8169\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 9.59 Seconds\n",
      "Test  Accuracy: 0.8090\n",
      "Test  Presion:  0.7532\n",
      "Test  Recall:   0.9064\n",
      "Test  F1-score: 0.8228\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS2_bow_2, memory_LSVM_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS2_bow_2, acc_LSVM_DS2_bow_2, pre_LSVM_DS2_bow_2, rec_LSVM_DS2_bow_2, f1_LSVM_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS2_bow_2, model_2, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8182\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 7.03 Seconds\n",
      "Test  Accuracy: 0.8166\n",
      "Test  Presion:  0.7606\n",
      "Test  Recall:   0.9118\n",
      "Test  F1-score: 0.8294\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS2_bow_3, memory_LSVM_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS2_bow_3, acc_LSVM_DS2_bow_3, pre_LSVM_DS2_bow_3, rec_LSVM_DS2_bow_3, f1_LSVM_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS2_bow_3, model_2, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7749\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 2.02 Seconds\n",
      "Test  Accuracy: 0.7777\n",
      "Test  Presion:  0.7541\n",
      "Test  Recall:   0.8091\n",
      "Test  F1-score: 0.7806\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS2_tfidf_1, memory_LSVM_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS2_tfidf_1, acc_LSVM_DS2_tfidf_1, pre_LSVM_DS2_tfidf_1, rec_LSVM_DS2_tfidf_1, f1_LSVM_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS2_tfidf_1, model_2, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8125\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.98 Seconds\n",
      "Test  Accuracy: 0.8055\n",
      "Test  Presion:  0.7824\n",
      "Test  Recall:   0.8341\n",
      "Test  F1-score: 0.8075\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS2_tfidf_2, memory_LSVM_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS2_tfidf_2, acc_LSVM_DS2_tfidf_2, pre_LSVM_DS2_tfidf_2, rec_LSVM_DS2_tfidf_2, f1_LSVM_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS2_tfidf_2, model_2, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8104\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.74 Seconds\n",
      "Test  Accuracy: 0.8093\n",
      "Test  Presion:  0.7652\n",
      "Test  Recall:   0.8800\n",
      "Test  F1-score: 0.8186\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS2_tfidf_3, memory_LSVM_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS2_tfidf_3, acc_LSVM_DS2_tfidf_3, pre_LSVM_DS2_tfidf_3, rec_LSVM_DS2_tfidf_3, f1_LSVM_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS2_tfidf_3, model_2, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49286846\n",
      "Iteration 2, loss = 0.43596222\n",
      "Iteration 3, loss = 0.42248841\n",
      "Iteration 4, loss = 0.41295007\n",
      "Iteration 5, loss = 0.40640708\n",
      "Iteration 6, loss = 0.40219454\n",
      "Iteration 7, loss = 0.39525478\n",
      "Iteration 8, loss = 0.39128523\n",
      "Iteration 9, loss = 0.38869556\n",
      "Iteration 10, loss = 0.38435690\n",
      "Iteration 11, loss = 0.38170424\n",
      "Iteration 12, loss = 0.37703445\n",
      "Iteration 13, loss = 0.37568740\n",
      "Iteration 14, loss = 0.37611140\n",
      "Iteration 15, loss = 0.37134342\n",
      "Iteration 16, loss = 0.36643067\n",
      "Iteration 17, loss = 0.36202070\n",
      "Iteration 18, loss = 0.36229654\n",
      "Iteration 19, loss = 0.35919322\n",
      "Iteration 20, loss = 0.35799593\n",
      "Iteration 21, loss = 0.35204145\n",
      "Iteration 22, loss = 0.35339653\n",
      "Iteration 23, loss = 0.34617554\n",
      "Iteration 24, loss = 0.38032664\n",
      "Iteration 25, loss = 0.36399593\n",
      "Iteration 26, loss = 0.35273659\n",
      "Iteration 27, loss = 0.34853372\n",
      "Iteration 28, loss = 0.34135863\n",
      "Iteration 29, loss = 0.33870144\n",
      "Iteration 30, loss = 0.33440728\n",
      "Iteration 31, loss = 0.33625291\n",
      "Iteration 32, loss = 0.33174523\n",
      "Iteration 33, loss = 0.33210131\n",
      "Iteration 34, loss = 0.32746420\n",
      "Iteration 35, loss = 0.32600672\n",
      "Iteration 36, loss = 0.32513539\n",
      "Iteration 37, loss = 0.31945692\n",
      "Iteration 38, loss = 0.31721016\n",
      "Iteration 39, loss = 0.31359037\n",
      "Iteration 40, loss = 0.31237148\n",
      "Iteration 41, loss = 0.30852150\n",
      "Iteration 42, loss = 0.30866654\n",
      "Iteration 43, loss = 0.30863179\n",
      "Iteration 44, loss = 0.30465649\n",
      "Iteration 45, loss = 0.30428167\n",
      "Iteration 46, loss = 0.30389633\n",
      "Iteration 47, loss = 0.30090288\n",
      "Iteration 48, loss = 0.29828917\n",
      "Iteration 49, loss = 0.30234267\n",
      "Iteration 50, loss = 0.29033741\n",
      "Iteration 1, loss = 0.49792209\n",
      "Iteration 2, loss = 0.43748916\n",
      "Iteration 3, loss = 0.42672327\n",
      "Iteration 4, loss = 0.41348872\n",
      "Iteration 5, loss = 0.40656056\n",
      "Iteration 6, loss = 0.40204415\n",
      "Iteration 7, loss = 0.39526448\n",
      "Iteration 8, loss = 0.39126228\n",
      "Iteration 9, loss = 0.38888317\n",
      "Iteration 10, loss = 0.38442372\n",
      "Iteration 11, loss = 0.37885737\n",
      "Iteration 12, loss = 0.37523102\n",
      "Iteration 13, loss = 0.37501613\n",
      "Iteration 14, loss = 0.37098482\n",
      "Iteration 15, loss = 0.36830981\n",
      "Iteration 16, loss = 0.36166572\n",
      "Iteration 17, loss = 0.35814842\n",
      "Iteration 18, loss = 0.35782717\n",
      "Iteration 19, loss = 0.35198114\n",
      "Iteration 20, loss = 0.35043155\n",
      "Iteration 21, loss = 0.34933432\n",
      "Iteration 22, loss = 0.34539013\n",
      "Iteration 23, loss = 0.34099904\n",
      "Iteration 24, loss = 0.33900399\n",
      "Iteration 25, loss = 0.33552337\n",
      "Iteration 26, loss = 0.34998326\n",
      "Iteration 27, loss = 0.33373126\n",
      "Iteration 28, loss = 0.33505584\n",
      "Iteration 29, loss = 0.33011616\n",
      "Iteration 30, loss = 0.32679129\n",
      "Iteration 31, loss = 0.32866351\n",
      "Iteration 32, loss = 0.32154230\n",
      "Iteration 33, loss = 0.31483931\n",
      "Iteration 34, loss = 0.32288665\n",
      "Iteration 35, loss = 0.32004735\n",
      "Iteration 36, loss = 0.31689064\n",
      "Iteration 37, loss = 0.30588506\n",
      "Iteration 38, loss = 0.30664400\n",
      "Iteration 39, loss = 0.30628191\n",
      "Iteration 40, loss = 0.30282027\n",
      "Iteration 41, loss = 0.30173224\n",
      "Iteration 42, loss = 0.29566810\n",
      "Iteration 43, loss = 0.29176138\n",
      "Iteration 44, loss = 0.29652464\n",
      "Iteration 45, loss = 0.29338120\n",
      "Iteration 46, loss = 0.29950476\n",
      "Iteration 47, loss = 0.29254474\n",
      "Iteration 48, loss = 0.28637768\n",
      "Iteration 49, loss = 0.28485468\n",
      "Iteration 50, loss = 0.28429635\n",
      "Iteration 1, loss = 0.49915391\n",
      "Iteration 2, loss = 0.43508795\n",
      "Iteration 3, loss = 0.42111971\n",
      "Iteration 4, loss = 0.41115624\n",
      "Iteration 5, loss = 0.40803581\n",
      "Iteration 6, loss = 0.39872979\n",
      "Iteration 7, loss = 0.39412780\n",
      "Iteration 8, loss = 0.38980656\n",
      "Iteration 9, loss = 0.38384154\n",
      "Iteration 10, loss = 0.38326999\n",
      "Iteration 11, loss = 0.37899261\n",
      "Iteration 12, loss = 0.37740431\n",
      "Iteration 13, loss = 0.37117931\n",
      "Iteration 14, loss = 0.36739581\n",
      "Iteration 15, loss = 0.36302642\n",
      "Iteration 16, loss = 0.35787717\n",
      "Iteration 17, loss = 0.35583844\n",
      "Iteration 18, loss = 0.35503837\n",
      "Iteration 19, loss = 0.35480748\n",
      "Iteration 20, loss = 0.35225979\n",
      "Iteration 21, loss = 0.34344939\n",
      "Iteration 22, loss = 0.34401597\n",
      "Iteration 23, loss = 0.34293983\n",
      "Iteration 24, loss = 0.33546685\n",
      "Iteration 25, loss = 0.33367376\n",
      "Iteration 26, loss = 0.33323587\n",
      "Iteration 27, loss = 0.32933452\n",
      "Iteration 28, loss = 0.32793709\n",
      "Iteration 29, loss = 0.32433898\n",
      "Iteration 30, loss = 0.33425692\n",
      "Iteration 31, loss = 0.32602890\n",
      "Iteration 32, loss = 0.31805330\n",
      "Iteration 33, loss = 0.30962144\n",
      "Iteration 34, loss = 0.30668223\n",
      "Iteration 35, loss = 0.31002227\n",
      "Iteration 36, loss = 0.31064008\n",
      "Iteration 37, loss = 0.29802848\n",
      "Iteration 38, loss = 0.30336888\n",
      "Iteration 39, loss = 0.30603734\n",
      "Iteration 40, loss = 0.32302280\n",
      "Iteration 41, loss = 0.30430423\n",
      "Iteration 42, loss = 0.29778536\n",
      "Iteration 43, loss = 0.29864873\n",
      "Iteration 44, loss = 0.29670350\n",
      "Iteration 45, loss = 0.28695149\n",
      "Iteration 46, loss = 0.28320831\n",
      "Iteration 47, loss = 0.28205826\n",
      "Iteration 48, loss = 0.28892557\n",
      "Iteration 49, loss = 0.27938418\n",
      "Iteration 50, loss = 0.27793394\n",
      "Iteration 1, loss = 0.50070110\n",
      "Iteration 2, loss = 0.44092609\n",
      "Iteration 3, loss = 0.42726646\n",
      "Iteration 4, loss = 0.41437806\n",
      "Iteration 5, loss = 0.40962602\n",
      "Iteration 6, loss = 0.40282404\n",
      "Iteration 7, loss = 0.40015583\n",
      "Iteration 8, loss = 0.39324644\n",
      "Iteration 9, loss = 0.39139805\n",
      "Iteration 10, loss = 0.38333658\n",
      "Iteration 11, loss = 0.38526473\n",
      "Iteration 12, loss = 0.37756845\n",
      "Iteration 13, loss = 0.37518424\n",
      "Iteration 14, loss = 0.37070932\n",
      "Iteration 15, loss = 0.36856284\n",
      "Iteration 16, loss = 0.36498440\n",
      "Iteration 17, loss = 0.35754319\n",
      "Iteration 18, loss = 0.35815828\n",
      "Iteration 19, loss = 0.35234741\n",
      "Iteration 20, loss = 0.34919912\n",
      "Iteration 21, loss = 0.34761685\n",
      "Iteration 22, loss = 0.34941479\n",
      "Iteration 23, loss = 0.34186615\n",
      "Iteration 24, loss = 0.33745908\n",
      "Iteration 25, loss = 0.33334549\n",
      "Iteration 26, loss = 0.33341286\n",
      "Iteration 27, loss = 0.33306534\n",
      "Iteration 28, loss = 0.34133580\n",
      "Iteration 29, loss = 0.32711594\n",
      "Iteration 30, loss = 0.32513327\n",
      "Iteration 31, loss = 0.32297050\n",
      "Iteration 32, loss = 0.32443288\n",
      "Iteration 33, loss = 0.32062829\n",
      "Iteration 34, loss = 0.31600172\n",
      "Iteration 35, loss = 0.30996356\n",
      "Iteration 36, loss = 0.30885957\n",
      "Iteration 37, loss = 0.31086762\n",
      "Iteration 38, loss = 0.30416114\n",
      "Iteration 39, loss = 0.30254738\n",
      "Iteration 40, loss = 0.29932435\n",
      "Iteration 41, loss = 0.29743377\n",
      "Iteration 42, loss = 0.29847864\n",
      "Iteration 43, loss = 0.30223700\n",
      "Iteration 44, loss = 0.31255862\n",
      "Iteration 45, loss = 0.29166381\n",
      "Iteration 46, loss = 0.28469572\n",
      "Iteration 47, loss = 0.29383554\n",
      "Iteration 48, loss = 0.28228286\n",
      "Iteration 49, loss = 0.28516786\n",
      "Iteration 50, loss = 0.30640103\n",
      "Iteration 1, loss = 0.51642589\n",
      "Iteration 2, loss = 0.43889106\n",
      "Iteration 3, loss = 0.42266758\n",
      "Iteration 4, loss = 0.41572715\n",
      "Iteration 5, loss = 0.40927974\n",
      "Iteration 6, loss = 0.40294205\n",
      "Iteration 7, loss = 0.39560663\n",
      "Iteration 8, loss = 0.39247863\n",
      "Iteration 9, loss = 0.38484912\n",
      "Iteration 10, loss = 0.38509688\n",
      "Iteration 11, loss = 0.38212697\n",
      "Iteration 12, loss = 0.38121961\n",
      "Iteration 13, loss = 0.37821690\n",
      "Iteration 14, loss = 0.37599527\n",
      "Iteration 15, loss = 0.36874102\n",
      "Iteration 16, loss = 0.37419759\n",
      "Iteration 17, loss = 0.36811343\n",
      "Iteration 18, loss = 0.36387501\n",
      "Iteration 19, loss = 0.36181936\n",
      "Iteration 20, loss = 0.35551510\n",
      "Iteration 21, loss = 0.35783477\n",
      "Iteration 22, loss = 0.35158728\n",
      "Iteration 23, loss = 0.34603073\n",
      "Iteration 24, loss = 0.34711869\n",
      "Iteration 25, loss = 0.34300514\n",
      "Iteration 26, loss = 0.34087585\n",
      "Iteration 27, loss = 0.33572956\n",
      "Iteration 28, loss = 0.33508693\n",
      "Iteration 29, loss = 0.33423314\n",
      "Iteration 30, loss = 0.33162070\n",
      "Iteration 31, loss = 0.32855166\n",
      "Iteration 32, loss = 0.32601687\n",
      "Iteration 33, loss = 0.33054967\n",
      "Iteration 34, loss = 0.32278933\n",
      "Iteration 35, loss = 0.31846538\n",
      "Iteration 36, loss = 0.31930283\n",
      "Iteration 37, loss = 0.32044574\n",
      "Iteration 38, loss = 0.31711568\n",
      "Iteration 39, loss = 0.31425994\n",
      "Iteration 40, loss = 0.30984199\n",
      "Iteration 41, loss = 0.30877516\n",
      "Iteration 42, loss = 0.30694358\n",
      "Iteration 43, loss = 0.30435615\n",
      "Iteration 44, loss = 0.32557919\n",
      "Iteration 45, loss = 0.31183275\n",
      "Iteration 46, loss = 0.30671304\n",
      "Iteration 47, loss = 0.29966751\n",
      "Iteration 48, loss = 0.30459427\n",
      "Iteration 49, loss = 0.29839615\n",
      "Iteration 50, loss = 0.29625454\n",
      "Iteration 1, loss = 0.50870055\n",
      "Iteration 2, loss = 0.44216287\n",
      "Iteration 3, loss = 0.42277853\n",
      "Iteration 4, loss = 0.41533188\n",
      "Iteration 5, loss = 0.40736610\n",
      "Iteration 6, loss = 0.40336195\n",
      "Iteration 7, loss = 0.39643283\n",
      "Iteration 8, loss = 0.39384189\n",
      "Iteration 9, loss = 0.38850534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.38428267\n",
      "Iteration 11, loss = 0.37915438\n",
      "Iteration 12, loss = 0.37497606\n",
      "Iteration 13, loss = 0.37379059\n",
      "Iteration 14, loss = 0.37123984\n",
      "Iteration 15, loss = 0.37211488\n",
      "Iteration 16, loss = 0.36412462\n",
      "Iteration 17, loss = 0.35896821\n",
      "Iteration 18, loss = 0.35585634\n",
      "Iteration 19, loss = 0.35539981\n",
      "Iteration 20, loss = 0.35406675\n",
      "Iteration 21, loss = 0.35178627\n",
      "Iteration 22, loss = 0.34769946\n",
      "Iteration 23, loss = 0.34663888\n",
      "Iteration 24, loss = 0.34097175\n",
      "Iteration 25, loss = 0.33921507\n",
      "Iteration 26, loss = 0.33755432\n",
      "Iteration 27, loss = 0.33103976\n",
      "Iteration 28, loss = 0.33394151\n",
      "Iteration 29, loss = 0.33215394\n",
      "Iteration 30, loss = 0.32743115\n",
      "Iteration 31, loss = 0.32272835\n",
      "Iteration 32, loss = 0.31870293\n",
      "Iteration 33, loss = 0.32064739\n",
      "Iteration 34, loss = 0.31760951\n",
      "Iteration 35, loss = 0.31281922\n",
      "Iteration 36, loss = 0.31429524\n",
      "Iteration 37, loss = 0.30905420\n",
      "Iteration 38, loss = 0.30967593\n",
      "Iteration 39, loss = 0.30093206\n",
      "Iteration 40, loss = 0.30897519\n",
      "Iteration 41, loss = 0.30180977\n",
      "Iteration 42, loss = 0.30529085\n",
      "Iteration 43, loss = 0.30395219\n",
      "Iteration 44, loss = 0.34921595\n",
      "Iteration 45, loss = 0.31652861\n",
      "Iteration 46, loss = 0.30383980\n",
      "Iteration 47, loss = 0.30191040\n",
      "Iteration 48, loss = 0.28856512\n",
      "Iteration 49, loss = 0.28482742\n",
      "Iteration 50, loss = 0.28471066\n",
      "Iteration 1, loss = 0.51676193\n",
      "Iteration 2, loss = 0.43969682\n",
      "Iteration 3, loss = 0.42630393\n",
      "Iteration 4, loss = 0.41852928\n",
      "Iteration 5, loss = 0.40872019\n",
      "Iteration 6, loss = 0.40396554\n",
      "Iteration 7, loss = 0.39915281\n",
      "Iteration 8, loss = 0.39520836\n",
      "Iteration 9, loss = 0.38961405\n",
      "Iteration 10, loss = 0.38710605\n",
      "Iteration 11, loss = 0.38601373\n",
      "Iteration 12, loss = 0.37926425\n",
      "Iteration 13, loss = 0.37609105\n",
      "Iteration 14, loss = 0.37302850\n",
      "Iteration 15, loss = 0.36909043\n",
      "Iteration 16, loss = 0.36817104\n",
      "Iteration 17, loss = 0.36520657\n",
      "Iteration 18, loss = 0.36381139\n",
      "Iteration 19, loss = 0.36025084\n",
      "Iteration 20, loss = 0.36106611\n",
      "Iteration 21, loss = 0.35218610\n",
      "Iteration 22, loss = 0.35162317\n",
      "Iteration 23, loss = 0.35101673\n",
      "Iteration 24, loss = 0.34579447\n",
      "Iteration 25, loss = 0.34696159\n",
      "Iteration 26, loss = 0.34352733\n",
      "Iteration 27, loss = 0.33989258\n",
      "Iteration 28, loss = 0.33691844\n",
      "Iteration 29, loss = 0.33756207\n",
      "Iteration 30, loss = 0.33235384\n",
      "Iteration 31, loss = 0.33193507\n",
      "Iteration 32, loss = 0.33636797\n",
      "Iteration 33, loss = 0.32932766\n",
      "Iteration 34, loss = 0.32340274\n",
      "Iteration 35, loss = 0.32284041\n",
      "Iteration 36, loss = 0.32133513\n",
      "Iteration 37, loss = 0.31649818\n",
      "Iteration 38, loss = 0.32362646\n",
      "Iteration 39, loss = 0.32427893\n",
      "Iteration 40, loss = 0.32200928\n",
      "Iteration 41, loss = 0.31418332\n",
      "Iteration 42, loss = 0.30761949\n",
      "Iteration 43, loss = 0.30483360\n",
      "Iteration 44, loss = 0.30595527\n",
      "Iteration 45, loss = 0.30404048\n",
      "Iteration 46, loss = 0.30771819\n",
      "Iteration 47, loss = 0.30888340\n",
      "Iteration 48, loss = 0.30586971\n",
      "Iteration 49, loss = 0.29515708\n",
      "Iteration 50, loss = 0.29264760\n",
      "Iteration 1, loss = 0.53201590\n",
      "Iteration 2, loss = 0.44229849\n",
      "Iteration 3, loss = 0.42726376\n",
      "Iteration 4, loss = 0.42173247\n",
      "Iteration 5, loss = 0.41373395\n",
      "Iteration 6, loss = 0.40969663\n",
      "Iteration 7, loss = 0.40312330\n",
      "Iteration 8, loss = 0.39797549\n",
      "Iteration 9, loss = 0.39574436\n",
      "Iteration 10, loss = 0.39331359\n",
      "Iteration 11, loss = 0.38706912\n",
      "Iteration 12, loss = 0.38990149\n",
      "Iteration 13, loss = 0.38401975\n",
      "Iteration 14, loss = 0.38250880\n",
      "Iteration 15, loss = 0.38027858\n",
      "Iteration 16, loss = 0.38404313\n",
      "Iteration 17, loss = 0.37794285\n",
      "Iteration 18, loss = 0.37597427\n",
      "Iteration 19, loss = 0.37422181\n",
      "Iteration 20, loss = 0.37245861\n",
      "Iteration 21, loss = 0.36981420\n",
      "Iteration 22, loss = 0.36919020\n",
      "Iteration 23, loss = 0.37001871\n",
      "Iteration 24, loss = 0.36465476\n",
      "Iteration 25, loss = 0.36168258\n",
      "Iteration 26, loss = 0.36323377\n",
      "Iteration 27, loss = 0.36007253\n",
      "Iteration 28, loss = 0.35611404\n",
      "Iteration 29, loss = 0.35493356\n",
      "Iteration 30, loss = 0.35022213\n",
      "Iteration 31, loss = 0.35222954\n",
      "Iteration 32, loss = 0.34870576\n",
      "Iteration 33, loss = 0.34918837\n",
      "Iteration 34, loss = 0.34684920\n",
      "Iteration 35, loss = 0.34598740\n",
      "Iteration 36, loss = 0.34493639\n",
      "Iteration 37, loss = 0.34238659\n",
      "Iteration 38, loss = 0.33528634\n",
      "Iteration 39, loss = 0.33525525\n",
      "Iteration 40, loss = 0.33483973\n",
      "Iteration 41, loss = 0.33250055\n",
      "Iteration 42, loss = 0.33120344\n",
      "Iteration 43, loss = 0.33252564\n",
      "Iteration 44, loss = 0.33122941\n",
      "Iteration 45, loss = 0.32373175\n",
      "Iteration 46, loss = 0.32251321\n",
      "Iteration 47, loss = 0.32458543\n",
      "Iteration 48, loss = 0.31920284\n",
      "Iteration 49, loss = 0.31548954\n",
      "Iteration 50, loss = 0.32179281\n",
      "Iteration 1, loss = 0.50653381\n",
      "Iteration 2, loss = 0.43901144\n",
      "Iteration 3, loss = 0.42539801\n",
      "Iteration 4, loss = 0.41560650\n",
      "Iteration 5, loss = 0.40915132\n",
      "Iteration 6, loss = 0.40122607\n",
      "Iteration 7, loss = 0.39816797\n",
      "Iteration 8, loss = 0.39287615\n",
      "Iteration 9, loss = 0.39139414\n",
      "Iteration 10, loss = 0.38607019\n",
      "Iteration 11, loss = 0.38174474\n",
      "Iteration 12, loss = 0.37764485\n",
      "Iteration 13, loss = 0.37501152\n",
      "Iteration 14, loss = 0.37206821\n",
      "Iteration 15, loss = 0.36961554\n",
      "Iteration 16, loss = 0.36578480\n",
      "Iteration 17, loss = 0.35977046\n",
      "Iteration 18, loss = 0.35831187\n",
      "Iteration 19, loss = 0.35465217\n",
      "Iteration 20, loss = 0.35370051\n",
      "Iteration 21, loss = 0.35334898\n",
      "Iteration 22, loss = 0.34789002\n",
      "Iteration 23, loss = 0.34403527\n",
      "Iteration 24, loss = 0.34157089\n",
      "Iteration 25, loss = 0.33543731\n",
      "Iteration 26, loss = 0.36486504\n",
      "Iteration 27, loss = 0.35869532\n",
      "Iteration 28, loss = 0.34424747\n",
      "Iteration 29, loss = 0.33669369\n",
      "Iteration 30, loss = 0.33857269\n",
      "Iteration 31, loss = 0.33192319\n",
      "Iteration 32, loss = 0.32943403\n",
      "Iteration 33, loss = 0.32790025\n",
      "Iteration 34, loss = 0.32230623\n",
      "Iteration 35, loss = 0.32682526\n",
      "Iteration 36, loss = 0.31953279\n",
      "Iteration 37, loss = 0.31009047\n",
      "Iteration 38, loss = 0.30744626\n",
      "Iteration 39, loss = 0.30903260\n",
      "Iteration 40, loss = 0.31035135\n",
      "Iteration 41, loss = 0.31181197\n",
      "Iteration 42, loss = 0.30437779\n",
      "Iteration 43, loss = 0.29732615\n",
      "Iteration 44, loss = 0.29580850\n",
      "Iteration 45, loss = 0.28979907\n",
      "Iteration 46, loss = 0.29220376\n",
      "Iteration 47, loss = 0.29009161\n",
      "Iteration 48, loss = 0.30057219\n",
      "Iteration 49, loss = 0.30079734\n",
      "Iteration 50, loss = 0.28817094\n",
      "Iteration 1, loss = 0.52370737\n",
      "Iteration 2, loss = 0.44171328\n",
      "Iteration 3, loss = 0.42924494\n",
      "Iteration 4, loss = 0.41889967\n",
      "Iteration 5, loss = 0.41071944\n",
      "Iteration 6, loss = 0.40473761\n",
      "Iteration 7, loss = 0.40079495\n",
      "Iteration 8, loss = 0.39720618\n",
      "Iteration 9, loss = 0.39213843\n",
      "Iteration 10, loss = 0.38854236\n",
      "Iteration 11, loss = 0.38564932\n",
      "Iteration 12, loss = 0.38012285\n",
      "Iteration 13, loss = 0.37607868\n",
      "Iteration 14, loss = 0.37280491\n",
      "Iteration 15, loss = 0.37267281\n",
      "Iteration 16, loss = 0.36654131\n",
      "Iteration 17, loss = 0.36644768\n",
      "Iteration 18, loss = 0.37065255\n",
      "Iteration 19, loss = 0.36468747\n",
      "Iteration 20, loss = 0.35745004\n",
      "Iteration 21, loss = 0.35794360\n",
      "Iteration 22, loss = 0.35261704\n",
      "Iteration 23, loss = 0.34920422\n",
      "Iteration 24, loss = 0.34936084\n",
      "Iteration 25, loss = 0.34960856\n",
      "Iteration 26, loss = 0.34660117\n",
      "Iteration 27, loss = 0.34425121\n",
      "Iteration 28, loss = 0.33925726\n",
      "Iteration 29, loss = 0.33714456\n",
      "Iteration 30, loss = 0.33376936\n",
      "Iteration 31, loss = 0.33021682\n",
      "Iteration 32, loss = 0.32722862\n",
      "Iteration 33, loss = 0.32528712\n",
      "Iteration 34, loss = 0.32152833\n",
      "Iteration 35, loss = 0.32588538\n",
      "Iteration 36, loss = 0.32340252\n",
      "Iteration 37, loss = 0.31742483\n",
      "Iteration 38, loss = 0.31755165\n",
      "Iteration 39, loss = 0.31916505\n",
      "Iteration 40, loss = 0.31603714\n",
      "Iteration 41, loss = 0.30751715\n",
      "Iteration 42, loss = 0.31062492\n",
      "Iteration 43, loss = 0.31308712\n",
      "Iteration 44, loss = 0.31192258\n",
      "Iteration 45, loss = 0.30973673\n",
      "Iteration 46, loss = 0.30553902\n",
      "Iteration 47, loss = 0.30626855\n",
      "Iteration 48, loss = 0.30407056\n",
      "Iteration 49, loss = 0.29920135\n",
      "Iteration 50, loss = 0.29880619\n",
      "Iteration 1, loss = 0.52977920\n",
      "Iteration 2, loss = 0.44170053\n",
      "Iteration 3, loss = 0.42751169\n",
      "Iteration 4, loss = 0.41824860\n",
      "Iteration 5, loss = 0.41355587\n",
      "Iteration 6, loss = 0.40655894\n",
      "Iteration 7, loss = 0.40490352\n",
      "Iteration 8, loss = 0.40141877\n",
      "Iteration 9, loss = 0.40054471\n",
      "Iteration 10, loss = 0.39332644\n",
      "Iteration 11, loss = 0.39262403\n",
      "Iteration 12, loss = 0.38819962\n",
      "Iteration 13, loss = 0.38381326\n",
      "Iteration 14, loss = 0.38039702\n",
      "Iteration 15, loss = 0.37787880\n",
      "Iteration 16, loss = 0.37555520\n",
      "Iteration 17, loss = 0.37556186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.37081928\n",
      "Iteration 19, loss = 0.36671160\n",
      "Iteration 20, loss = 0.36656380\n",
      "Iteration 21, loss = 0.36414682\n",
      "Iteration 22, loss = 0.35973835\n",
      "Iteration 23, loss = 0.35836721\n",
      "Iteration 24, loss = 0.35958914\n",
      "Iteration 25, loss = 0.35767912\n",
      "Iteration 26, loss = 0.35512173\n",
      "Iteration 27, loss = 0.35059254\n",
      "Iteration 28, loss = 0.34858967\n",
      "Iteration 29, loss = 0.34767510\n",
      "Iteration 30, loss = 0.34278048\n",
      "Iteration 31, loss = 0.34215304\n",
      "Iteration 32, loss = 0.34434965\n",
      "Iteration 33, loss = 0.34072154\n",
      "Iteration 34, loss = 0.33696599\n",
      "Iteration 35, loss = 0.33505499\n",
      "Iteration 36, loss = 0.33378619\n",
      "Iteration 37, loss = 0.33124980\n",
      "Iteration 38, loss = 0.32549090\n",
      "Iteration 39, loss = 0.32736721\n",
      "Iteration 40, loss = 0.34212194\n",
      "Iteration 41, loss = 0.32971019\n",
      "Iteration 42, loss = 0.32535582\n",
      "Iteration 43, loss = 0.32231001\n",
      "Iteration 44, loss = 0.32798646\n",
      "Iteration 45, loss = 0.32001084\n",
      "Iteration 46, loss = 0.32182929\n",
      "Iteration 47, loss = 0.31514809\n",
      "Iteration 48, loss = 0.31618274\n",
      "Iteration 49, loss = 0.31035547\n",
      "Iteration 50, loss = 0.30925985\n",
      "Train Accuracy: 0.7794\n",
      "Running Memory: 1.48 MB\n",
      "Running Time: 68.33 Seconds\n",
      "Test  Accuracy: 0.7773\n",
      "Test  Presion:  0.7612\n",
      "Test  Recall:   0.7934\n",
      "Test  F1-score: 0.7770\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS2_bow_1, memory_MLP_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS2_bow_1, acc_MLP_DS2_bow_1, pre_MLP_DS2_bow_1, rec_MLP_DS2_bow_1, f1_MLP_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS2_bow_1, model_3, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.45370961\n",
      "Iteration 2, loss = 0.41120015\n",
      "Iteration 3, loss = 0.40216595\n",
      "Iteration 4, loss = 0.39767811\n",
      "Iteration 5, loss = 0.39356926\n",
      "Iteration 6, loss = 0.39056665\n",
      "Iteration 7, loss = 0.38720372\n",
      "Iteration 8, loss = 0.38646700\n",
      "Iteration 9, loss = 0.38325696\n",
      "Iteration 10, loss = 0.38308960\n",
      "Iteration 11, loss = 0.37887004\n",
      "Iteration 12, loss = 0.37839750\n",
      "Iteration 13, loss = 0.37683379\n",
      "Iteration 14, loss = 0.37511025\n",
      "Iteration 15, loss = 0.37225364\n",
      "Iteration 16, loss = 0.37201342\n",
      "Iteration 17, loss = 0.36976554\n",
      "Iteration 18, loss = 0.36886761\n",
      "Iteration 19, loss = 0.36813164\n",
      "Iteration 20, loss = 0.36625048\n",
      "Iteration 21, loss = 0.36413785\n",
      "Iteration 22, loss = 0.36348931\n",
      "Iteration 23, loss = 0.36404901\n",
      "Iteration 24, loss = 0.36054815\n",
      "Iteration 25, loss = 0.36156390\n",
      "Iteration 26, loss = 0.35732365\n",
      "Iteration 27, loss = 0.35928882\n",
      "Iteration 28, loss = 0.35358933\n",
      "Iteration 29, loss = 0.35499238\n",
      "Iteration 30, loss = 0.35290814\n",
      "Iteration 31, loss = 0.35752908\n",
      "Iteration 32, loss = 0.35079343\n",
      "Iteration 33, loss = 0.35118215\n",
      "Iteration 34, loss = 0.35042044\n",
      "Iteration 35, loss = 0.34766736\n",
      "Iteration 36, loss = 0.34809203\n",
      "Iteration 37, loss = 0.34724686\n",
      "Iteration 38, loss = 0.34480772\n",
      "Iteration 39, loss = 0.34580306\n",
      "Iteration 40, loss = 0.34252229\n",
      "Iteration 41, loss = 0.34236852\n",
      "Iteration 42, loss = 0.34134994\n",
      "Iteration 43, loss = 0.34181823\n",
      "Iteration 44, loss = 0.34038088\n",
      "Iteration 45, loss = 0.34228475\n",
      "Iteration 46, loss = 0.34107229\n",
      "Iteration 47, loss = 0.33593777\n",
      "Iteration 48, loss = 0.33871775\n",
      "Iteration 49, loss = 0.33472636\n",
      "Iteration 50, loss = 0.33300789\n",
      "Iteration 1, loss = 0.45853042\n",
      "Iteration 2, loss = 0.41381252\n",
      "Iteration 3, loss = 0.40426272\n",
      "Iteration 4, loss = 0.39791733\n",
      "Iteration 5, loss = 0.39271845\n",
      "Iteration 6, loss = 0.39103467\n",
      "Iteration 7, loss = 0.38883133\n",
      "Iteration 8, loss = 0.38680668\n",
      "Iteration 9, loss = 0.38385590\n",
      "Iteration 10, loss = 0.38271183\n",
      "Iteration 11, loss = 0.37945000\n",
      "Iteration 12, loss = 0.37911064\n",
      "Iteration 13, loss = 0.37745849\n",
      "Iteration 14, loss = 0.37677558\n",
      "Iteration 15, loss = 0.37375703\n",
      "Iteration 16, loss = 0.37252522\n",
      "Iteration 17, loss = 0.37314530\n",
      "Iteration 18, loss = 0.37009604\n",
      "Iteration 19, loss = 0.36793553\n",
      "Iteration 20, loss = 0.36672756\n",
      "Iteration 21, loss = 0.36498639\n",
      "Iteration 22, loss = 0.36412301\n",
      "Iteration 23, loss = 0.36235937\n",
      "Iteration 24, loss = 0.36251496\n",
      "Iteration 25, loss = 0.36047840\n",
      "Iteration 26, loss = 0.35931245\n",
      "Iteration 27, loss = 0.35878458\n",
      "Iteration 28, loss = 0.35618234\n",
      "Iteration 29, loss = 0.35603586\n",
      "Iteration 30, loss = 0.35522661\n",
      "Iteration 31, loss = 0.35445623\n",
      "Iteration 32, loss = 0.35043065\n",
      "Iteration 33, loss = 0.34942732\n",
      "Iteration 34, loss = 0.35110652\n",
      "Iteration 35, loss = 0.34933391\n",
      "Iteration 36, loss = 0.35102762\n",
      "Iteration 37, loss = 0.34783367\n",
      "Iteration 38, loss = 0.34814412\n",
      "Iteration 39, loss = 0.34450632\n",
      "Iteration 40, loss = 0.34087519\n",
      "Iteration 41, loss = 0.34421556\n",
      "Iteration 42, loss = 0.34110296\n",
      "Iteration 43, loss = 0.34120622\n",
      "Iteration 44, loss = 0.34178946\n",
      "Iteration 45, loss = 0.33932389\n",
      "Iteration 46, loss = 0.33669216\n",
      "Iteration 47, loss = 0.33669668\n",
      "Iteration 48, loss = 0.33715718\n",
      "Iteration 49, loss = 0.33737518\n",
      "Iteration 50, loss = 0.33289894\n",
      "Iteration 1, loss = 0.45905875\n",
      "Iteration 2, loss = 0.41341641\n",
      "Iteration 3, loss = 0.40306244\n",
      "Iteration 4, loss = 0.39676091\n",
      "Iteration 5, loss = 0.39343823\n",
      "Iteration 6, loss = 0.39101946\n",
      "Iteration 7, loss = 0.38747193\n",
      "Iteration 8, loss = 0.38623038\n",
      "Iteration 9, loss = 0.38435655\n",
      "Iteration 10, loss = 0.38346018\n",
      "Iteration 11, loss = 0.38048613\n",
      "Iteration 12, loss = 0.37872611\n",
      "Iteration 13, loss = 0.37707779\n",
      "Iteration 14, loss = 0.37489896\n",
      "Iteration 15, loss = 0.37408293\n",
      "Iteration 16, loss = 0.37237652\n",
      "Iteration 17, loss = 0.37124423\n",
      "Iteration 18, loss = 0.37005793\n",
      "Iteration 19, loss = 0.36804726\n",
      "Iteration 20, loss = 0.36666342\n",
      "Iteration 21, loss = 0.36490641\n",
      "Iteration 22, loss = 0.36476632\n",
      "Iteration 23, loss = 0.36348409\n",
      "Iteration 24, loss = 0.36169847\n",
      "Iteration 25, loss = 0.36025160\n",
      "Iteration 26, loss = 0.35937706\n",
      "Iteration 27, loss = 0.35814455\n",
      "Iteration 28, loss = 0.35889577\n",
      "Iteration 29, loss = 0.35645121\n",
      "Iteration 30, loss = 0.35464405\n",
      "Iteration 31, loss = 0.35060833\n",
      "Iteration 32, loss = 0.35354944\n",
      "Iteration 33, loss = 0.35035565\n",
      "Iteration 34, loss = 0.35159668\n",
      "Iteration 35, loss = 0.34887580\n",
      "Iteration 36, loss = 0.34527880\n",
      "Iteration 37, loss = 0.34596539\n",
      "Iteration 38, loss = 0.34743001\n",
      "Iteration 39, loss = 0.34240033\n",
      "Iteration 40, loss = 0.34388472\n",
      "Iteration 41, loss = 0.34270985\n",
      "Iteration 42, loss = 0.34145387\n",
      "Iteration 43, loss = 0.33952601\n",
      "Iteration 44, loss = 0.33967382\n",
      "Iteration 45, loss = 0.33777649\n",
      "Iteration 46, loss = 0.33938454\n",
      "Iteration 47, loss = 0.33575143\n",
      "Iteration 48, loss = 0.33698425\n",
      "Iteration 49, loss = 0.33015472\n",
      "Iteration 50, loss = 0.33155364\n",
      "Iteration 1, loss = 0.46042195\n",
      "Iteration 2, loss = 0.41538219\n",
      "Iteration 3, loss = 0.40405355\n",
      "Iteration 4, loss = 0.39779737\n",
      "Iteration 5, loss = 0.39459516\n",
      "Iteration 6, loss = 0.39020805\n",
      "Iteration 7, loss = 0.38914524\n",
      "Iteration 8, loss = 0.38629853\n",
      "Iteration 9, loss = 0.38461599\n",
      "Iteration 10, loss = 0.38127292\n",
      "Iteration 11, loss = 0.38101441\n",
      "Iteration 12, loss = 0.37758598\n",
      "Iteration 13, loss = 0.37558924\n",
      "Iteration 14, loss = 0.37551167\n",
      "Iteration 15, loss = 0.37350357\n",
      "Iteration 16, loss = 0.37319495\n",
      "Iteration 17, loss = 0.37047602\n",
      "Iteration 18, loss = 0.36914663\n",
      "Iteration 19, loss = 0.37034844\n",
      "Iteration 20, loss = 0.36650529\n",
      "Iteration 21, loss = 0.36512281\n",
      "Iteration 22, loss = 0.36563201\n",
      "Iteration 23, loss = 0.36268260\n",
      "Iteration 24, loss = 0.36448301\n",
      "Iteration 25, loss = 0.36109467\n",
      "Iteration 26, loss = 0.35831739\n",
      "Iteration 27, loss = 0.35719588\n",
      "Iteration 28, loss = 0.35604862\n",
      "Iteration 29, loss = 0.35647598\n",
      "Iteration 30, loss = 0.35554585\n",
      "Iteration 31, loss = 0.35483838\n",
      "Iteration 32, loss = 0.35231475\n",
      "Iteration 33, loss = 0.35162600\n",
      "Iteration 34, loss = 0.35046748\n",
      "Iteration 35, loss = 0.35015769\n",
      "Iteration 36, loss = 0.34781761\n",
      "Iteration 37, loss = 0.34662717\n",
      "Iteration 38, loss = 0.34719595\n",
      "Iteration 39, loss = 0.34513433\n",
      "Iteration 40, loss = 0.34685522\n",
      "Iteration 41, loss = 0.34335868\n",
      "Iteration 42, loss = 0.34256213\n",
      "Iteration 43, loss = 0.34256084\n",
      "Iteration 44, loss = 0.34091330\n",
      "Iteration 45, loss = 0.33739627\n",
      "Iteration 46, loss = 0.33455428\n",
      "Iteration 47, loss = 0.33512163\n",
      "Iteration 48, loss = 0.34449792\n",
      "Iteration 49, loss = 0.33746611\n",
      "Iteration 50, loss = 0.33603255\n",
      "Iteration 1, loss = 0.45928085\n",
      "Iteration 2, loss = 0.41045897\n",
      "Iteration 3, loss = 0.40137627\n",
      "Iteration 4, loss = 0.39463229\n",
      "Iteration 5, loss = 0.39228743\n",
      "Iteration 6, loss = 0.38826164\n",
      "Iteration 7, loss = 0.38645131\n",
      "Iteration 8, loss = 0.38275023\n",
      "Iteration 9, loss = 0.38009765\n",
      "Iteration 10, loss = 0.37809061\n",
      "Iteration 11, loss = 0.37685909\n",
      "Iteration 12, loss = 0.37599775\n",
      "Iteration 13, loss = 0.37476577\n",
      "Iteration 14, loss = 0.37316340\n",
      "Iteration 15, loss = 0.37110558\n",
      "Iteration 16, loss = 0.37031311\n",
      "Iteration 17, loss = 0.36939516\n",
      "Iteration 18, loss = 0.36710837\n",
      "Iteration 19, loss = 0.36645530\n",
      "Iteration 20, loss = 0.36519989\n",
      "Iteration 21, loss = 0.36270882\n",
      "Iteration 22, loss = 0.36112036\n",
      "Iteration 23, loss = 0.35652085\n",
      "Iteration 24, loss = 0.36037062\n",
      "Iteration 25, loss = 0.35660023\n",
      "Iteration 26, loss = 0.35716134\n",
      "Iteration 27, loss = 0.35496859\n",
      "Iteration 28, loss = 0.35665306\n",
      "Iteration 29, loss = 0.35349502\n",
      "Iteration 30, loss = 0.35350862\n",
      "Iteration 31, loss = 0.34863097\n",
      "Iteration 32, loss = 0.34853597\n",
      "Iteration 33, loss = 0.34810996\n",
      "Iteration 34, loss = 0.34536791\n",
      "Iteration 35, loss = 0.34580711\n",
      "Iteration 36, loss = 0.34492943\n",
      "Iteration 37, loss = 0.34256844\n",
      "Iteration 38, loss = 0.34304405\n",
      "Iteration 39, loss = 0.34367995\n",
      "Iteration 40, loss = 0.34347749\n",
      "Iteration 41, loss = 0.33910958\n",
      "Iteration 42, loss = 0.33819816\n",
      "Iteration 43, loss = 0.33665687\n",
      "Iteration 44, loss = 0.33343963\n",
      "Iteration 45, loss = 0.33608001\n",
      "Iteration 46, loss = 0.33644227\n",
      "Iteration 47, loss = 0.33577409\n",
      "Iteration 48, loss = 0.33278991\n",
      "Iteration 49, loss = 0.33115702\n",
      "Iteration 50, loss = 0.32927598\n",
      "Iteration 1, loss = 0.45793319\n",
      "Iteration 2, loss = 0.41366448\n",
      "Iteration 3, loss = 0.40281172\n",
      "Iteration 4, loss = 0.39875644\n",
      "Iteration 5, loss = 0.39402614\n",
      "Iteration 6, loss = 0.39024105\n",
      "Iteration 7, loss = 0.38870966\n",
      "Iteration 8, loss = 0.38705440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, loss = 0.38382838\n",
      "Iteration 10, loss = 0.38090829\n",
      "Iteration 11, loss = 0.37924121\n",
      "Iteration 12, loss = 0.37890136\n",
      "Iteration 13, loss = 0.37571946\n",
      "Iteration 14, loss = 0.37545666\n",
      "Iteration 15, loss = 0.37557721\n",
      "Iteration 16, loss = 0.37233539\n",
      "Iteration 17, loss = 0.36841270\n",
      "Iteration 18, loss = 0.36919430\n",
      "Iteration 19, loss = 0.36951001\n",
      "Iteration 20, loss = 0.36360401\n",
      "Iteration 21, loss = 0.36414137\n",
      "Iteration 22, loss = 0.36367967\n",
      "Iteration 23, loss = 0.36287182\n",
      "Iteration 24, loss = 0.36165573\n",
      "Iteration 25, loss = 0.35962042\n",
      "Iteration 26, loss = 0.35813835\n",
      "Iteration 27, loss = 0.35792377\n",
      "Iteration 28, loss = 0.35401178\n",
      "Iteration 29, loss = 0.35440404\n",
      "Iteration 30, loss = 0.35283363\n",
      "Iteration 31, loss = 0.35315808\n",
      "Iteration 32, loss = 0.35283227\n",
      "Iteration 33, loss = 0.34990489\n",
      "Iteration 34, loss = 0.34836988\n",
      "Iteration 35, loss = 0.34619488\n",
      "Iteration 36, loss = 0.34787203\n",
      "Iteration 37, loss = 0.34718210\n",
      "Iteration 38, loss = 0.34489599\n",
      "Iteration 39, loss = 0.34147397\n",
      "Iteration 40, loss = 0.34016394\n",
      "Iteration 41, loss = 0.34232367\n",
      "Iteration 42, loss = 0.34097966\n",
      "Iteration 43, loss = 0.34261515\n",
      "Iteration 44, loss = 0.33951150\n",
      "Iteration 45, loss = 0.33613324\n",
      "Iteration 46, loss = 0.33730002\n",
      "Iteration 47, loss = 0.33587384\n",
      "Iteration 48, loss = 0.33225714\n",
      "Iteration 49, loss = 0.33687496\n",
      "Iteration 50, loss = 0.33098094\n",
      "Iteration 1, loss = 0.45873520\n",
      "Iteration 2, loss = 0.41170279\n",
      "Iteration 3, loss = 0.40441112\n",
      "Iteration 4, loss = 0.39732907\n",
      "Iteration 5, loss = 0.39412784\n",
      "Iteration 6, loss = 0.39072506\n",
      "Iteration 7, loss = 0.38818149\n",
      "Iteration 8, loss = 0.38705333\n",
      "Iteration 9, loss = 0.38272106\n",
      "Iteration 10, loss = 0.38199943\n",
      "Iteration 11, loss = 0.37919363\n",
      "Iteration 12, loss = 0.37851021\n",
      "Iteration 13, loss = 0.37614923\n",
      "Iteration 14, loss = 0.37394032\n",
      "Iteration 15, loss = 0.37128772\n",
      "Iteration 16, loss = 0.37177014\n",
      "Iteration 17, loss = 0.37057907\n",
      "Iteration 18, loss = 0.36928080\n",
      "Iteration 19, loss = 0.36723621\n",
      "Iteration 20, loss = 0.36515311\n",
      "Iteration 21, loss = 0.36462069\n",
      "Iteration 22, loss = 0.36390861\n",
      "Iteration 23, loss = 0.36166817\n",
      "Iteration 24, loss = 0.36243271\n",
      "Iteration 25, loss = 0.35988663\n",
      "Iteration 26, loss = 0.35815150\n",
      "Iteration 27, loss = 0.35657417\n",
      "Iteration 28, loss = 0.35432172\n",
      "Iteration 29, loss = 0.35615789\n",
      "Iteration 30, loss = 0.35341286\n",
      "Iteration 31, loss = 0.35372108\n",
      "Iteration 32, loss = 0.35168938\n",
      "Iteration 33, loss = 0.35020981\n",
      "Iteration 34, loss = 0.34963089\n",
      "Iteration 35, loss = 0.34857007\n",
      "Iteration 36, loss = 0.34582971\n",
      "Iteration 37, loss = 0.34499970\n",
      "Iteration 38, loss = 0.34581465\n",
      "Iteration 39, loss = 0.34483535\n",
      "Iteration 40, loss = 0.34121321\n",
      "Iteration 41, loss = 0.34280438\n",
      "Iteration 42, loss = 0.34010934\n",
      "Iteration 43, loss = 0.34291967\n",
      "Iteration 44, loss = 0.33990896\n",
      "Iteration 45, loss = 0.33577022\n",
      "Iteration 46, loss = 0.33710783\n",
      "Iteration 47, loss = 0.33763689\n",
      "Iteration 48, loss = 0.33315374\n",
      "Iteration 49, loss = 0.33404780\n",
      "Iteration 50, loss = 0.33186555\n",
      "Iteration 1, loss = 0.45798437\n",
      "Iteration 2, loss = 0.41210367\n",
      "Iteration 3, loss = 0.40141240\n",
      "Iteration 4, loss = 0.39509291\n",
      "Iteration 5, loss = 0.39131853\n",
      "Iteration 6, loss = 0.38873853\n",
      "Iteration 7, loss = 0.38550032\n",
      "Iteration 8, loss = 0.38405831\n",
      "Iteration 9, loss = 0.38190785\n",
      "Iteration 10, loss = 0.38017797\n",
      "Iteration 11, loss = 0.37744081\n",
      "Iteration 12, loss = 0.37589462\n",
      "Iteration 13, loss = 0.37227085\n",
      "Iteration 14, loss = 0.37297208\n",
      "Iteration 15, loss = 0.37123519\n",
      "Iteration 16, loss = 0.36932835\n",
      "Iteration 17, loss = 0.36916945\n",
      "Iteration 18, loss = 0.36617825\n",
      "Iteration 19, loss = 0.36544441\n",
      "Iteration 20, loss = 0.36492180\n",
      "Iteration 21, loss = 0.36396024\n",
      "Iteration 22, loss = 0.36410907\n",
      "Iteration 23, loss = 0.35750289\n",
      "Iteration 24, loss = 0.35904496\n",
      "Iteration 25, loss = 0.35943670\n",
      "Iteration 26, loss = 0.35865725\n",
      "Iteration 27, loss = 0.35937094\n",
      "Iteration 28, loss = 0.35513962\n",
      "Iteration 29, loss = 0.35229648\n",
      "Iteration 30, loss = 0.35576588\n",
      "Iteration 31, loss = 0.34937894\n",
      "Iteration 32, loss = 0.35326627\n",
      "Iteration 33, loss = 0.34813688\n",
      "Iteration 34, loss = 0.34607839\n",
      "Iteration 35, loss = 0.34413712\n",
      "Iteration 36, loss = 0.34423868\n",
      "Iteration 37, loss = 0.34438098\n",
      "Iteration 38, loss = 0.34459879\n",
      "Iteration 39, loss = 0.34269682\n",
      "Iteration 40, loss = 0.34296598\n",
      "Iteration 41, loss = 0.34233648\n",
      "Iteration 42, loss = 0.33802070\n",
      "Iteration 43, loss = 0.33757395\n",
      "Iteration 44, loss = 0.33845408\n",
      "Iteration 45, loss = 0.33730959\n",
      "Iteration 46, loss = 0.33763057\n",
      "Iteration 47, loss = 0.33831033\n",
      "Iteration 48, loss = 0.33416863\n",
      "Iteration 49, loss = 0.33203920\n",
      "Iteration 50, loss = 0.33499355\n",
      "Iteration 1, loss = 0.45857267\n",
      "Iteration 2, loss = 0.41259110\n",
      "Iteration 3, loss = 0.40301360\n",
      "Iteration 4, loss = 0.39604174\n",
      "Iteration 5, loss = 0.39141126\n",
      "Iteration 6, loss = 0.38974585\n",
      "Iteration 7, loss = 0.38651968\n",
      "Iteration 8, loss = 0.38503608\n",
      "Iteration 9, loss = 0.38288161\n",
      "Iteration 10, loss = 0.37963916\n",
      "Iteration 11, loss = 0.37687852\n",
      "Iteration 12, loss = 0.37573440\n",
      "Iteration 13, loss = 0.37491717\n",
      "Iteration 14, loss = 0.37319143\n",
      "Iteration 15, loss = 0.37214248\n",
      "Iteration 16, loss = 0.36870437\n",
      "Iteration 17, loss = 0.36863036\n",
      "Iteration 18, loss = 0.36727542\n",
      "Iteration 19, loss = 0.36623323\n",
      "Iteration 20, loss = 0.36444268\n",
      "Iteration 21, loss = 0.36071994\n",
      "Iteration 22, loss = 0.36198441\n",
      "Iteration 23, loss = 0.36061635\n",
      "Iteration 24, loss = 0.35676396\n",
      "Iteration 25, loss = 0.35504090\n",
      "Iteration 26, loss = 0.35708293\n",
      "Iteration 27, loss = 0.35581540\n",
      "Iteration 28, loss = 0.35230203\n",
      "Iteration 29, loss = 0.35391484\n",
      "Iteration 30, loss = 0.35298877\n",
      "Iteration 31, loss = 0.34967988\n",
      "Iteration 32, loss = 0.34793179\n",
      "Iteration 33, loss = 0.34829567\n",
      "Iteration 34, loss = 0.34652251\n",
      "Iteration 35, loss = 0.34712075\n",
      "Iteration 36, loss = 0.34489863\n",
      "Iteration 37, loss = 0.34977312\n",
      "Iteration 38, loss = 0.34391413\n",
      "Iteration 39, loss = 0.34283659\n",
      "Iteration 40, loss = 0.34055761\n",
      "Iteration 41, loss = 0.33722792\n",
      "Iteration 42, loss = 0.33983395\n",
      "Iteration 43, loss = 0.33789802\n",
      "Iteration 44, loss = 0.33433505\n",
      "Iteration 45, loss = 0.33598786\n",
      "Iteration 46, loss = 0.33167640\n",
      "Iteration 47, loss = 0.33313694\n",
      "Iteration 48, loss = 0.33281134\n",
      "Iteration 49, loss = 0.33071724\n",
      "Iteration 50, loss = 0.33517662\n",
      "Iteration 1, loss = 0.45827620\n",
      "Iteration 2, loss = 0.41101880\n",
      "Iteration 3, loss = 0.40251345\n",
      "Iteration 4, loss = 0.39609625\n",
      "Iteration 5, loss = 0.39309892\n",
      "Iteration 6, loss = 0.38835934\n",
      "Iteration 7, loss = 0.38608782\n",
      "Iteration 8, loss = 0.38392934\n",
      "Iteration 9, loss = 0.38214519\n",
      "Iteration 10, loss = 0.38025653\n",
      "Iteration 11, loss = 0.37882030\n",
      "Iteration 12, loss = 0.37704345\n",
      "Iteration 13, loss = 0.37391292\n",
      "Iteration 14, loss = 0.37476949\n",
      "Iteration 15, loss = 0.37209108\n",
      "Iteration 16, loss = 0.37057169\n",
      "Iteration 17, loss = 0.36818102\n",
      "Iteration 18, loss = 0.36663910\n",
      "Iteration 19, loss = 0.36614621\n",
      "Iteration 20, loss = 0.36373847\n",
      "Iteration 21, loss = 0.36045293\n",
      "Iteration 22, loss = 0.36307087\n",
      "Iteration 23, loss = 0.36262751\n",
      "Iteration 24, loss = 0.35891423\n",
      "Iteration 25, loss = 0.35646986\n",
      "Iteration 26, loss = 0.35799184\n",
      "Iteration 27, loss = 0.35575698\n",
      "Iteration 28, loss = 0.35428927\n",
      "Iteration 29, loss = 0.35326218\n",
      "Iteration 30, loss = 0.35158362\n",
      "Iteration 31, loss = 0.34953709\n",
      "Iteration 32, loss = 0.34920532\n",
      "Iteration 33, loss = 0.34907137\n",
      "Iteration 34, loss = 0.34777700\n",
      "Iteration 35, loss = 0.34702662\n",
      "Iteration 36, loss = 0.34633387\n",
      "Iteration 37, loss = 0.34494576\n",
      "Iteration 38, loss = 0.34649046\n",
      "Iteration 39, loss = 0.34032331\n",
      "Iteration 40, loss = 0.34050852\n",
      "Iteration 41, loss = 0.33956847\n",
      "Iteration 42, loss = 0.34203310\n",
      "Iteration 43, loss = 0.33793732\n",
      "Iteration 44, loss = 0.33916516\n",
      "Iteration 45, loss = 0.33653875\n",
      "Iteration 46, loss = 0.33377650\n",
      "Iteration 47, loss = 0.33461483\n",
      "Iteration 48, loss = 0.33403247\n",
      "Iteration 49, loss = 0.33337598\n",
      "Iteration 50, loss = 0.33143485\n",
      "Iteration 1, loss = 0.46207523\n",
      "Iteration 2, loss = 0.41548174\n",
      "Iteration 3, loss = 0.40433239\n",
      "Iteration 4, loss = 0.39884490\n",
      "Iteration 5, loss = 0.39470560\n",
      "Iteration 6, loss = 0.39271906\n",
      "Iteration 7, loss = 0.39093785\n",
      "Iteration 8, loss = 0.38603702\n",
      "Iteration 9, loss = 0.38444503\n",
      "Iteration 10, loss = 0.38264919\n",
      "Iteration 11, loss = 0.38189568\n",
      "Iteration 12, loss = 0.37843645\n",
      "Iteration 13, loss = 0.37792402\n",
      "Iteration 14, loss = 0.37490857\n",
      "Iteration 15, loss = 0.37497124\n",
      "Iteration 16, loss = 0.37227130\n",
      "Iteration 17, loss = 0.37117603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.36937583\n",
      "Iteration 19, loss = 0.36668818\n",
      "Iteration 20, loss = 0.36825086\n",
      "Iteration 21, loss = 0.36628206\n",
      "Iteration 22, loss = 0.36347517\n",
      "Iteration 23, loss = 0.36211026\n",
      "Iteration 24, loss = 0.36179864\n",
      "Iteration 25, loss = 0.35996454\n",
      "Iteration 26, loss = 0.35905378\n",
      "Iteration 27, loss = 0.35777538\n",
      "Iteration 28, loss = 0.35935541\n",
      "Iteration 29, loss = 0.35531538\n",
      "Iteration 30, loss = 0.35064697\n",
      "Iteration 31, loss = 0.35425313\n",
      "Iteration 32, loss = 0.35129508\n",
      "Iteration 33, loss = 0.35197358\n",
      "Iteration 34, loss = 0.34698854\n",
      "Iteration 35, loss = 0.34768932\n",
      "Iteration 36, loss = 0.34914887\n",
      "Iteration 37, loss = 0.34474348\n",
      "Iteration 38, loss = 0.34251143\n",
      "Iteration 39, loss = 0.34128119\n",
      "Iteration 40, loss = 0.34171239\n",
      "Iteration 41, loss = 0.34102166\n",
      "Iteration 42, loss = 0.33808410\n",
      "Iteration 43, loss = 0.33667485\n",
      "Iteration 44, loss = 0.33996427\n",
      "Iteration 45, loss = 0.33577819\n",
      "Iteration 46, loss = 0.33635572\n",
      "Iteration 47, loss = 0.33408226\n",
      "Iteration 48, loss = 0.33175101\n",
      "Iteration 49, loss = 0.33017396\n",
      "Iteration 50, loss = 0.33133215\n",
      "Train Accuracy: 0.8199\n",
      "Running Memory: 0.06 MB\n",
      "Running Time: 65.31 Seconds\n",
      "Test  Accuracy: 0.8043\n",
      "Test  Presion:  0.7807\n",
      "Test  Recall:   0.8341\n",
      "Test  F1-score: 0.8065\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS2_bow_2, memory_MLP_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS2_bow_2, acc_MLP_DS2_bow_2, pre_MLP_DS2_bow_2, rec_MLP_DS2_bow_2, f1_MLP_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS2_bow_2, model_3, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47753852\n",
      "Iteration 2, loss = 0.44324696\n",
      "Iteration 3, loss = 0.44119607\n",
      "Iteration 4, loss = 0.43946549\n",
      "Iteration 5, loss = 0.43776882\n",
      "Iteration 6, loss = 0.43598234\n",
      "Iteration 7, loss = 0.43608790\n",
      "Iteration 8, loss = 0.43472733\n",
      "Iteration 9, loss = 0.43387599\n",
      "Iteration 10, loss = 0.43311297\n",
      "Iteration 11, loss = 0.43259092\n",
      "Iteration 12, loss = 0.43257797\n",
      "Iteration 13, loss = 0.43166879\n",
      "Iteration 14, loss = 0.43214815\n",
      "Iteration 15, loss = 0.43070145\n",
      "Iteration 16, loss = 0.43031713\n",
      "Iteration 17, loss = 0.43026131\n",
      "Iteration 18, loss = 0.42938011\n",
      "Iteration 19, loss = 0.42869938\n",
      "Iteration 20, loss = 0.42870399\n",
      "Iteration 21, loss = 0.42708439\n",
      "Iteration 22, loss = 0.42736849\n",
      "Iteration 23, loss = 0.42792996\n",
      "Iteration 24, loss = 0.42725614\n",
      "Iteration 25, loss = 0.42720497\n",
      "Iteration 26, loss = 0.42580337\n",
      "Iteration 27, loss = 0.42672936\n",
      "Iteration 28, loss = 0.42571084\n",
      "Iteration 29, loss = 0.42571262\n",
      "Iteration 30, loss = 0.42410222\n",
      "Iteration 31, loss = 0.42617161\n",
      "Iteration 32, loss = 0.42442266\n",
      "Iteration 33, loss = 0.42434611\n",
      "Iteration 34, loss = 0.42331590\n",
      "Iteration 35, loss = 0.42274711\n",
      "Iteration 36, loss = 0.42331638\n",
      "Iteration 37, loss = 0.42331809\n",
      "Iteration 38, loss = 0.42293284\n",
      "Iteration 39, loss = 0.42272658\n",
      "Iteration 40, loss = 0.42212917\n",
      "Iteration 41, loss = 0.42201603\n",
      "Iteration 42, loss = 0.42244329\n",
      "Iteration 43, loss = 0.42097749\n",
      "Iteration 44, loss = 0.41999970\n",
      "Iteration 45, loss = 0.42022073\n",
      "Iteration 46, loss = 0.42055278\n",
      "Iteration 47, loss = 0.41985160\n",
      "Iteration 48, loss = 0.41972803\n",
      "Iteration 49, loss = 0.41865414\n",
      "Iteration 50, loss = 0.41944416\n",
      "Iteration 1, loss = 0.48104527\n",
      "Iteration 2, loss = 0.44433729\n",
      "Iteration 3, loss = 0.44280379\n",
      "Iteration 4, loss = 0.43931003\n",
      "Iteration 5, loss = 0.43732256\n",
      "Iteration 6, loss = 0.43665287\n",
      "Iteration 7, loss = 0.43515338\n",
      "Iteration 8, loss = 0.43537187\n",
      "Iteration 9, loss = 0.43344814\n",
      "Iteration 10, loss = 0.43377401\n",
      "Iteration 11, loss = 0.43235086\n",
      "Iteration 12, loss = 0.43210071\n",
      "Iteration 13, loss = 0.43109783\n",
      "Iteration 14, loss = 0.43142931\n",
      "Iteration 15, loss = 0.43165456\n",
      "Iteration 16, loss = 0.43089029\n",
      "Iteration 17, loss = 0.42940433\n",
      "Iteration 18, loss = 0.42926217\n",
      "Iteration 19, loss = 0.42857857\n",
      "Iteration 20, loss = 0.42837547\n",
      "Iteration 21, loss = 0.42762635\n",
      "Iteration 22, loss = 0.42791543\n",
      "Iteration 23, loss = 0.42713697\n",
      "Iteration 24, loss = 0.42786529\n",
      "Iteration 25, loss = 0.42794695\n",
      "Iteration 26, loss = 0.42705519\n",
      "Iteration 27, loss = 0.42683923\n",
      "Iteration 28, loss = 0.42574755\n",
      "Iteration 29, loss = 0.42581356\n",
      "Iteration 30, loss = 0.42594276\n",
      "Iteration 31, loss = 0.42450441\n",
      "Iteration 32, loss = 0.42447625\n",
      "Iteration 33, loss = 0.42360633\n",
      "Iteration 34, loss = 0.42390288\n",
      "Iteration 35, loss = 0.42359922\n",
      "Iteration 36, loss = 0.42296673\n",
      "Iteration 37, loss = 0.42323691\n",
      "Iteration 38, loss = 0.42322654\n",
      "Iteration 39, loss = 0.42247887\n",
      "Iteration 40, loss = 0.42088062\n",
      "Iteration 41, loss = 0.42236511\n",
      "Iteration 42, loss = 0.42092745\n",
      "Iteration 43, loss = 0.42103435\n",
      "Iteration 44, loss = 0.42044159\n",
      "Iteration 45, loss = 0.42056451\n",
      "Iteration 46, loss = 0.42003297\n",
      "Iteration 47, loss = 0.41972197\n",
      "Iteration 48, loss = 0.41923167\n",
      "Iteration 49, loss = 0.42030905\n",
      "Iteration 50, loss = 0.42014959\n",
      "Iteration 1, loss = 0.47898651\n",
      "Iteration 2, loss = 0.44335822\n",
      "Iteration 3, loss = 0.43996204\n",
      "Iteration 4, loss = 0.43776188\n",
      "Iteration 5, loss = 0.43660980\n",
      "Iteration 6, loss = 0.43582669\n",
      "Iteration 7, loss = 0.43384792\n",
      "Iteration 8, loss = 0.43481341\n",
      "Iteration 9, loss = 0.43289338\n",
      "Iteration 10, loss = 0.43310390\n",
      "Iteration 11, loss = 0.43078460\n",
      "Iteration 12, loss = 0.43107594\n",
      "Iteration 13, loss = 0.43033453\n",
      "Iteration 14, loss = 0.43022278\n",
      "Iteration 15, loss = 0.42915511\n",
      "Iteration 16, loss = 0.42922140\n",
      "Iteration 17, loss = 0.42817590\n",
      "Iteration 18, loss = 0.42849760\n",
      "Iteration 19, loss = 0.42745286\n",
      "Iteration 20, loss = 0.42715257\n",
      "Iteration 21, loss = 0.42705416\n",
      "Iteration 22, loss = 0.42615089\n",
      "Iteration 23, loss = 0.42557939\n",
      "Iteration 24, loss = 0.42611843\n",
      "Iteration 25, loss = 0.42479071\n",
      "Iteration 26, loss = 0.42535171\n",
      "Iteration 27, loss = 0.42416408\n",
      "Iteration 28, loss = 0.42460186\n",
      "Iteration 29, loss = 0.42362430\n",
      "Iteration 30, loss = 0.42307480\n",
      "Iteration 31, loss = 0.42184295\n",
      "Iteration 32, loss = 0.42315360\n",
      "Iteration 33, loss = 0.42244513\n",
      "Iteration 34, loss = 0.42123862\n",
      "Iteration 35, loss = 0.42289165\n",
      "Iteration 36, loss = 0.42082616\n",
      "Iteration 37, loss = 0.42225909\n",
      "Iteration 38, loss = 0.42037975\n",
      "Iteration 39, loss = 0.42082999\n",
      "Iteration 40, loss = 0.42031475\n",
      "Iteration 41, loss = 0.41950493\n",
      "Iteration 42, loss = 0.41923050\n",
      "Iteration 43, loss = 0.41909970\n",
      "Iteration 44, loss = 0.41979513\n",
      "Iteration 45, loss = 0.41752740\n",
      "Iteration 46, loss = 0.41728480\n",
      "Iteration 47, loss = 0.41798823\n",
      "Iteration 48, loss = 0.41881795\n",
      "Iteration 49, loss = 0.41740157\n",
      "Iteration 50, loss = 0.41705229\n",
      "Iteration 1, loss = 0.48099164\n",
      "Iteration 2, loss = 0.44340445\n",
      "Iteration 3, loss = 0.43997969\n",
      "Iteration 4, loss = 0.43852393\n",
      "Iteration 5, loss = 0.43657452\n",
      "Iteration 6, loss = 0.43630050\n",
      "Iteration 7, loss = 0.43535104\n",
      "Iteration 8, loss = 0.43507038\n",
      "Iteration 9, loss = 0.43380400\n",
      "Iteration 10, loss = 0.43307661\n",
      "Iteration 11, loss = 0.43204486\n",
      "Iteration 12, loss = 0.43171076\n",
      "Iteration 13, loss = 0.43103695\n",
      "Iteration 14, loss = 0.42991934\n",
      "Iteration 15, loss = 0.43018008\n",
      "Iteration 16, loss = 0.43023105\n",
      "Iteration 17, loss = 0.42924351\n",
      "Iteration 18, loss = 0.42966507\n",
      "Iteration 19, loss = 0.42899616\n",
      "Iteration 20, loss = 0.42708803\n",
      "Iteration 21, loss = 0.42732258\n",
      "Iteration 22, loss = 0.42734257\n",
      "Iteration 23, loss = 0.42671719\n",
      "Iteration 24, loss = 0.42663352\n",
      "Iteration 25, loss = 0.42692032\n",
      "Iteration 26, loss = 0.42658958\n",
      "Iteration 27, loss = 0.42500084\n",
      "Iteration 28, loss = 0.42515661\n",
      "Iteration 29, loss = 0.42582650\n",
      "Iteration 30, loss = 0.42458455\n",
      "Iteration 31, loss = 0.42466757\n",
      "Iteration 32, loss = 0.42444243\n",
      "Iteration 33, loss = 0.42356454\n",
      "Iteration 34, loss = 0.42431891\n",
      "Iteration 35, loss = 0.42402438\n",
      "Iteration 36, loss = 0.42291841\n",
      "Iteration 37, loss = 0.42224709\n",
      "Iteration 38, loss = 0.42253937\n",
      "Iteration 39, loss = 0.42229682\n",
      "Iteration 40, loss = 0.42255336\n",
      "Iteration 41, loss = 0.42119578\n",
      "Iteration 42, loss = 0.42119366\n",
      "Iteration 43, loss = 0.42017284\n",
      "Iteration 44, loss = 0.42083387\n",
      "Iteration 45, loss = 0.42076745\n",
      "Iteration 46, loss = 0.41900961\n",
      "Iteration 47, loss = 0.41954055\n",
      "Iteration 48, loss = 0.42030325\n",
      "Iteration 49, loss = 0.42007674\n",
      "Iteration 50, loss = 0.41914130\n",
      "Iteration 1, loss = 0.48084695\n",
      "Iteration 2, loss = 0.44303579\n",
      "Iteration 3, loss = 0.43965333\n",
      "Iteration 4, loss = 0.43762438\n",
      "Iteration 5, loss = 0.43700223\n",
      "Iteration 6, loss = 0.43465586\n",
      "Iteration 7, loss = 0.43422138\n",
      "Iteration 8, loss = 0.43303773\n",
      "Iteration 9, loss = 0.43212942\n",
      "Iteration 10, loss = 0.43151430\n",
      "Iteration 11, loss = 0.43093281\n",
      "Iteration 12, loss = 0.43129452\n",
      "Iteration 13, loss = 0.42984047\n",
      "Iteration 14, loss = 0.42976658\n",
      "Iteration 15, loss = 0.42889700\n",
      "Iteration 16, loss = 0.42985396\n",
      "Iteration 17, loss = 0.42873261\n",
      "Iteration 18, loss = 0.42774208\n",
      "Iteration 19, loss = 0.42753544\n",
      "Iteration 20, loss = 0.42630911\n",
      "Iteration 21, loss = 0.42581886\n",
      "Iteration 22, loss = 0.42630795\n",
      "Iteration 23, loss = 0.42471418\n",
      "Iteration 24, loss = 0.42579170\n",
      "Iteration 25, loss = 0.42475298\n",
      "Iteration 26, loss = 0.42394021\n",
      "Iteration 27, loss = 0.42455451\n",
      "Iteration 28, loss = 0.42348810\n",
      "Iteration 29, loss = 0.42348082\n",
      "Iteration 30, loss = 0.42297376\n",
      "Iteration 31, loss = 0.42151939\n",
      "Iteration 32, loss = 0.42228618\n",
      "Iteration 33, loss = 0.42144615\n",
      "Iteration 34, loss = 0.42160494\n",
      "Iteration 35, loss = 0.42140072\n",
      "Iteration 36, loss = 0.42148825\n",
      "Iteration 37, loss = 0.42046945\n",
      "Iteration 38, loss = 0.42064926\n",
      "Iteration 39, loss = 0.41996643\n",
      "Iteration 40, loss = 0.42042562\n",
      "Iteration 41, loss = 0.42021983\n",
      "Iteration 42, loss = 0.41958938\n",
      "Iteration 43, loss = 0.41979581\n",
      "Iteration 44, loss = 0.41845540\n",
      "Iteration 45, loss = 0.41733019\n",
      "Iteration 46, loss = 0.41821188\n",
      "Iteration 47, loss = 0.41864301\n",
      "Iteration 48, loss = 0.41749293\n",
      "Iteration 49, loss = 0.41725189\n",
      "Iteration 50, loss = 0.41745315\n",
      "Iteration 1, loss = 0.48227562\n",
      "Iteration 2, loss = 0.44812608\n",
      "Iteration 3, loss = 0.44217274\n",
      "Iteration 4, loss = 0.44189441\n",
      "Iteration 5, loss = 0.43994928\n",
      "Iteration 6, loss = 0.43795976\n",
      "Iteration 7, loss = 0.43719364\n",
      "Iteration 8, loss = 0.43644021\n",
      "Iteration 9, loss = 0.43536921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.43416974\n",
      "Iteration 11, loss = 0.43385923\n",
      "Iteration 12, loss = 0.43291238\n",
      "Iteration 13, loss = 0.43274681\n",
      "Iteration 14, loss = 0.43320724\n",
      "Iteration 15, loss = 0.43146786\n",
      "Iteration 16, loss = 0.43202816\n",
      "Iteration 17, loss = 0.43109253\n",
      "Iteration 18, loss = 0.43054737\n",
      "Iteration 19, loss = 0.43029839\n",
      "Iteration 20, loss = 0.43013125\n",
      "Iteration 21, loss = 0.42920495\n",
      "Iteration 22, loss = 0.42987209\n",
      "Iteration 23, loss = 0.42871381\n",
      "Iteration 24, loss = 0.42893576\n",
      "Iteration 25, loss = 0.42856103\n",
      "Iteration 26, loss = 0.42686786\n",
      "Iteration 27, loss = 0.42788617\n",
      "Iteration 28, loss = 0.42683067\n",
      "Iteration 29, loss = 0.42634243\n",
      "Iteration 30, loss = 0.42590586\n",
      "Iteration 31, loss = 0.42598455\n",
      "Iteration 32, loss = 0.42606186\n",
      "Iteration 33, loss = 0.42499874\n",
      "Iteration 34, loss = 0.42546049\n",
      "Iteration 35, loss = 0.42583483\n",
      "Iteration 36, loss = 0.42434631\n",
      "Iteration 37, loss = 0.42402328\n",
      "Iteration 38, loss = 0.42312458\n",
      "Iteration 39, loss = 0.42332880\n",
      "Iteration 40, loss = 0.42477251\n",
      "Iteration 41, loss = 0.42225989\n",
      "Iteration 42, loss = 0.42290416\n",
      "Iteration 43, loss = 0.42228237\n",
      "Iteration 44, loss = 0.42140187\n",
      "Iteration 45, loss = 0.42183050\n",
      "Iteration 46, loss = 0.42107622\n",
      "Iteration 47, loss = 0.42121072\n",
      "Iteration 48, loss = 0.42092075\n",
      "Iteration 49, loss = 0.42009215\n",
      "Iteration 50, loss = 0.42053072\n",
      "Iteration 1, loss = 0.48330145\n",
      "Iteration 2, loss = 0.44444671\n",
      "Iteration 3, loss = 0.44239221\n",
      "Iteration 4, loss = 0.44018889\n",
      "Iteration 5, loss = 0.43954643\n",
      "Iteration 6, loss = 0.43692312\n",
      "Iteration 7, loss = 0.43652189\n",
      "Iteration 8, loss = 0.43554163\n",
      "Iteration 9, loss = 0.43486414\n",
      "Iteration 10, loss = 0.43330121\n",
      "Iteration 11, loss = 0.43407271\n",
      "Iteration 12, loss = 0.43273893\n",
      "Iteration 13, loss = 0.43195936\n",
      "Iteration 14, loss = 0.43142979\n",
      "Iteration 15, loss = 0.43043943\n",
      "Iteration 16, loss = 0.43064157\n",
      "Iteration 17, loss = 0.43010869\n",
      "Iteration 18, loss = 0.42981679\n",
      "Iteration 19, loss = 0.42945515\n",
      "Iteration 20, loss = 0.42995779\n",
      "Iteration 21, loss = 0.42863206\n",
      "Iteration 22, loss = 0.42840931\n",
      "Iteration 23, loss = 0.42794322\n",
      "Iteration 24, loss = 0.42677819\n",
      "Iteration 25, loss = 0.42702153\n",
      "Iteration 26, loss = 0.42686937\n",
      "Iteration 27, loss = 0.42632103\n",
      "Iteration 28, loss = 0.42478934\n",
      "Iteration 29, loss = 0.42593870\n",
      "Iteration 30, loss = 0.42508546\n",
      "Iteration 31, loss = 0.42544179\n",
      "Iteration 32, loss = 0.42475149\n",
      "Iteration 33, loss = 0.42497067\n",
      "Iteration 34, loss = 0.42398634\n",
      "Iteration 35, loss = 0.42411004\n",
      "Iteration 36, loss = 0.42377344\n",
      "Iteration 37, loss = 0.42252247\n",
      "Iteration 38, loss = 0.42175995\n",
      "Iteration 39, loss = 0.42248578\n",
      "Iteration 40, loss = 0.42135402\n",
      "Iteration 41, loss = 0.42153793\n",
      "Iteration 42, loss = 0.42089555\n",
      "Iteration 43, loss = 0.42106576\n",
      "Iteration 44, loss = 0.42003315\n",
      "Iteration 45, loss = 0.41969811\n",
      "Iteration 46, loss = 0.41998737\n",
      "Iteration 47, loss = 0.42124225\n",
      "Iteration 48, loss = 0.41969091\n",
      "Iteration 49, loss = 0.41929379\n",
      "Iteration 50, loss = 0.41842869\n",
      "Iteration 1, loss = 0.48338238\n",
      "Iteration 2, loss = 0.44384100\n",
      "Iteration 3, loss = 0.44055418\n",
      "Iteration 4, loss = 0.43861169\n",
      "Iteration 5, loss = 0.43784506\n",
      "Iteration 6, loss = 0.43701231\n",
      "Iteration 7, loss = 0.43622158\n",
      "Iteration 8, loss = 0.43414860\n",
      "Iteration 9, loss = 0.43339652\n",
      "Iteration 10, loss = 0.43308871\n",
      "Iteration 11, loss = 0.43321126\n",
      "Iteration 12, loss = 0.43181273\n",
      "Iteration 13, loss = 0.43079858\n",
      "Iteration 14, loss = 0.43077881\n",
      "Iteration 15, loss = 0.43009339\n",
      "Iteration 16, loss = 0.43023892\n",
      "Iteration 17, loss = 0.42942729\n",
      "Iteration 18, loss = 0.42872553\n",
      "Iteration 19, loss = 0.42925595\n",
      "Iteration 20, loss = 0.42887444\n",
      "Iteration 21, loss = 0.42781780\n",
      "Iteration 22, loss = 0.42824669\n",
      "Iteration 23, loss = 0.42702639\n",
      "Iteration 24, loss = 0.42676806\n",
      "Iteration 25, loss = 0.42755427\n",
      "Iteration 26, loss = 0.42611850\n",
      "Iteration 27, loss = 0.42646446\n",
      "Iteration 28, loss = 0.42514426\n",
      "Iteration 29, loss = 0.42531755\n",
      "Iteration 30, loss = 0.42555554\n",
      "Iteration 31, loss = 0.42449885\n",
      "Iteration 32, loss = 0.42481403\n",
      "Iteration 33, loss = 0.42383123\n",
      "Iteration 34, loss = 0.42363559\n",
      "Iteration 35, loss = 0.42389159\n",
      "Iteration 36, loss = 0.42326212\n",
      "Iteration 37, loss = 0.42285112\n",
      "Iteration 38, loss = 0.42184798\n",
      "Iteration 39, loss = 0.42233721\n",
      "Iteration 40, loss = 0.42187277\n",
      "Iteration 41, loss = 0.42175176\n",
      "Iteration 42, loss = 0.42127107\n",
      "Iteration 43, loss = 0.42067755\n",
      "Iteration 44, loss = 0.42073065\n",
      "Iteration 45, loss = 0.41989354\n",
      "Iteration 46, loss = 0.41908023\n",
      "Iteration 47, loss = 0.42013507\n",
      "Iteration 48, loss = 0.41980048\n",
      "Iteration 49, loss = 0.41981377\n",
      "Iteration 50, loss = 0.41826521\n",
      "Iteration 1, loss = 0.48197203\n",
      "Iteration 2, loss = 0.44433406\n",
      "Iteration 3, loss = 0.44166235\n",
      "Iteration 4, loss = 0.43924677\n",
      "Iteration 5, loss = 0.43757527\n",
      "Iteration 6, loss = 0.43682090\n",
      "Iteration 7, loss = 0.43532990\n",
      "Iteration 8, loss = 0.43533964\n",
      "Iteration 9, loss = 0.43438971\n",
      "Iteration 10, loss = 0.43223550\n",
      "Iteration 11, loss = 0.43331155\n",
      "Iteration 12, loss = 0.43200401\n",
      "Iteration 13, loss = 0.43145267\n",
      "Iteration 14, loss = 0.43092588\n",
      "Iteration 15, loss = 0.43097969\n",
      "Iteration 16, loss = 0.42934077\n",
      "Iteration 17, loss = 0.42954154\n",
      "Iteration 18, loss = 0.42915963\n",
      "Iteration 19, loss = 0.42909422\n",
      "Iteration 20, loss = 0.42833608\n",
      "Iteration 21, loss = 0.42807221\n",
      "Iteration 22, loss = 0.42825990\n",
      "Iteration 23, loss = 0.42776872\n",
      "Iteration 24, loss = 0.42756401\n",
      "Iteration 25, loss = 0.42551547\n",
      "Iteration 26, loss = 0.42596519\n",
      "Iteration 27, loss = 0.42610967\n",
      "Iteration 28, loss = 0.42545210\n",
      "Iteration 29, loss = 0.42504303\n",
      "Iteration 30, loss = 0.42378845\n",
      "Iteration 31, loss = 0.42494033\n",
      "Iteration 32, loss = 0.42444684\n",
      "Iteration 33, loss = 0.42342420\n",
      "Iteration 34, loss = 0.42302118\n",
      "Iteration 35, loss = 0.42272843\n",
      "Iteration 36, loss = 0.42298306\n",
      "Iteration 37, loss = 0.42278260\n",
      "Iteration 38, loss = 0.42269343\n",
      "Iteration 39, loss = 0.42173396\n",
      "Iteration 40, loss = 0.42201707\n",
      "Iteration 41, loss = 0.42093703\n",
      "Iteration 42, loss = 0.42081454\n",
      "Iteration 43, loss = 0.42046377\n",
      "Iteration 44, loss = 0.41997235\n",
      "Iteration 45, loss = 0.42007842\n",
      "Iteration 46, loss = 0.41970667\n",
      "Iteration 47, loss = 0.41937911\n",
      "Iteration 48, loss = 0.41906703\n",
      "Iteration 49, loss = 0.41890691\n",
      "Iteration 50, loss = 0.41882047\n",
      "Iteration 1, loss = 0.48061004\n",
      "Iteration 2, loss = 0.44316658\n",
      "Iteration 3, loss = 0.44006096\n",
      "Iteration 4, loss = 0.43753111\n",
      "Iteration 5, loss = 0.43701262\n",
      "Iteration 6, loss = 0.43475799\n",
      "Iteration 7, loss = 0.43468149\n",
      "Iteration 8, loss = 0.43291574\n",
      "Iteration 9, loss = 0.43280871\n",
      "Iteration 10, loss = 0.43262623\n",
      "Iteration 11, loss = 0.43076674\n",
      "Iteration 12, loss = 0.43091303\n",
      "Iteration 13, loss = 0.43003584\n",
      "Iteration 14, loss = 0.43039460\n",
      "Iteration 15, loss = 0.42979084\n",
      "Iteration 16, loss = 0.42953002\n",
      "Iteration 17, loss = 0.42770101\n",
      "Iteration 18, loss = 0.42842072\n",
      "Iteration 19, loss = 0.42779803\n",
      "Iteration 20, loss = 0.42797908\n",
      "Iteration 21, loss = 0.42674841\n",
      "Iteration 22, loss = 0.42738894\n",
      "Iteration 23, loss = 0.42630038\n",
      "Iteration 24, loss = 0.42528168\n",
      "Iteration 25, loss = 0.42566493\n",
      "Iteration 26, loss = 0.42532562\n",
      "Iteration 27, loss = 0.42458056\n",
      "Iteration 28, loss = 0.42404782\n",
      "Iteration 29, loss = 0.42444219\n",
      "Iteration 30, loss = 0.42497648\n",
      "Iteration 31, loss = 0.42397675\n",
      "Iteration 32, loss = 0.42272002\n",
      "Iteration 33, loss = 0.42316292\n",
      "Iteration 34, loss = 0.42238892\n",
      "Iteration 35, loss = 0.42214218\n",
      "Iteration 36, loss = 0.42187558\n",
      "Iteration 37, loss = 0.42197259\n",
      "Iteration 38, loss = 0.42167201\n",
      "Iteration 39, loss = 0.42261738\n",
      "Iteration 40, loss = 0.41956616\n",
      "Iteration 41, loss = 0.42030762\n",
      "Iteration 42, loss = 0.41983238\n",
      "Iteration 43, loss = 0.41927787\n",
      "Iteration 44, loss = 0.41893538\n",
      "Iteration 45, loss = 0.42103415\n",
      "Iteration 46, loss = 0.41828543\n",
      "Iteration 47, loss = 0.41892010\n",
      "Iteration 48, loss = 0.41823802\n",
      "Iteration 49, loss = 0.41892471\n",
      "Iteration 50, loss = 0.41876478\n",
      "Iteration 1, loss = 0.48651485\n",
      "Iteration 2, loss = 0.44668354\n",
      "Iteration 3, loss = 0.44388854\n",
      "Iteration 4, loss = 0.44085109\n",
      "Iteration 5, loss = 0.43960051\n",
      "Iteration 6, loss = 0.43963088\n",
      "Iteration 7, loss = 0.43855505\n",
      "Iteration 8, loss = 0.43672819\n",
      "Iteration 9, loss = 0.43633310\n",
      "Iteration 10, loss = 0.43547631\n",
      "Iteration 11, loss = 0.43542465\n",
      "Iteration 12, loss = 0.43463083\n",
      "Iteration 13, loss = 0.43354601\n",
      "Iteration 14, loss = 0.43245818\n",
      "Iteration 15, loss = 0.43331538\n",
      "Iteration 16, loss = 0.43234346\n",
      "Iteration 17, loss = 0.43183392\n",
      "Iteration 18, loss = 0.43206637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, loss = 0.43047260\n",
      "Iteration 20, loss = 0.43104156\n",
      "Iteration 21, loss = 0.43105757\n",
      "Iteration 22, loss = 0.43090630\n",
      "Iteration 23, loss = 0.42978353\n",
      "Iteration 24, loss = 0.42902046\n",
      "Iteration 25, loss = 0.42877531\n",
      "Iteration 26, loss = 0.42811773\n",
      "Iteration 27, loss = 0.42785011\n",
      "Iteration 28, loss = 0.42799820\n",
      "Iteration 29, loss = 0.42707308\n",
      "Iteration 30, loss = 0.42590397\n",
      "Iteration 31, loss = 0.42584457\n",
      "Iteration 32, loss = 0.42557846\n",
      "Iteration 33, loss = 0.42549301\n",
      "Iteration 34, loss = 0.42592469\n",
      "Iteration 35, loss = 0.42480101\n",
      "Iteration 36, loss = 0.42526307\n",
      "Iteration 37, loss = 0.42311321\n",
      "Iteration 38, loss = 0.42493431\n",
      "Iteration 39, loss = 0.42259804\n",
      "Iteration 40, loss = 0.42379618\n",
      "Iteration 41, loss = 0.42262190\n",
      "Iteration 42, loss = 0.42348015\n",
      "Iteration 43, loss = 0.42172936\n",
      "Iteration 44, loss = 0.42215774\n",
      "Iteration 45, loss = 0.42126459\n",
      "Iteration 46, loss = 0.42150333\n",
      "Iteration 47, loss = 0.42039326\n",
      "Iteration 48, loss = 0.42022107\n",
      "Iteration 49, loss = 0.42000037\n",
      "Iteration 50, loss = 0.42117681\n",
      "Train Accuracy: 0.8166\n",
      "Running Memory: 9.68 MB\n",
      "Running Time: 61.11 Seconds\n",
      "Test  Accuracy: 0.8196\n",
      "Test  Presion:  0.7640\n",
      "Test  Recall:   0.9132\n",
      "Test  F1-score: 0.8319\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS2_bow_3, memory_MLP_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS2_bow_3, acc_MLP_DS2_bow_3, pre_MLP_DS2_bow_3, rec_MLP_DS2_bow_3, f1_MLP_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS2_bow_3, model_3, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51156280\n",
      "Iteration 2, loss = 0.46109457\n",
      "Iteration 3, loss = 0.45814973\n",
      "Iteration 4, loss = 0.45297404\n",
      "Iteration 5, loss = 0.44814904\n",
      "Iteration 6, loss = 0.44116140\n",
      "Iteration 7, loss = 0.43804232\n",
      "Iteration 8, loss = 0.43273192\n",
      "Iteration 9, loss = 0.42984430\n",
      "Iteration 10, loss = 0.42608929\n",
      "Iteration 11, loss = 0.42436463\n",
      "Iteration 12, loss = 0.42235186\n",
      "Iteration 13, loss = 0.42051471\n",
      "Iteration 14, loss = 0.41757235\n",
      "Iteration 15, loss = 0.41665849\n",
      "Iteration 16, loss = 0.41304642\n",
      "Iteration 17, loss = 0.41125753\n",
      "Iteration 18, loss = 0.41003415\n",
      "Iteration 19, loss = 0.40733514\n",
      "Iteration 20, loss = 0.40696459\n",
      "Iteration 21, loss = 0.40483681\n",
      "Iteration 22, loss = 0.40302202\n",
      "Iteration 23, loss = 0.40268323\n",
      "Iteration 24, loss = 0.39910405\n",
      "Iteration 25, loss = 0.39907455\n",
      "Iteration 26, loss = 0.39554584\n",
      "Iteration 27, loss = 0.39740299\n",
      "Iteration 28, loss = 0.39470161\n",
      "Iteration 29, loss = 0.39218815\n",
      "Iteration 30, loss = 0.39136491\n",
      "Iteration 31, loss = 0.39057420\n",
      "Iteration 32, loss = 0.38913600\n",
      "Iteration 33, loss = 0.38929290\n",
      "Iteration 34, loss = 0.38603388\n",
      "Iteration 35, loss = 0.38458516\n",
      "Iteration 36, loss = 0.38504729\n",
      "Iteration 37, loss = 0.38298909\n",
      "Iteration 38, loss = 0.38065154\n",
      "Iteration 39, loss = 0.38118148\n",
      "Iteration 40, loss = 0.37831279\n",
      "Iteration 41, loss = 0.37771991\n",
      "Iteration 42, loss = 0.37719140\n",
      "Iteration 43, loss = 0.37656465\n",
      "Iteration 44, loss = 0.37434244\n",
      "Iteration 45, loss = 0.37392938\n",
      "Iteration 46, loss = 0.37158401\n",
      "Iteration 47, loss = 0.37244022\n",
      "Iteration 48, loss = 0.36879964\n",
      "Iteration 49, loss = 0.36938826\n",
      "Iteration 50, loss = 0.36886359\n",
      "Iteration 1, loss = 0.51740538\n",
      "Iteration 2, loss = 0.46053676\n",
      "Iteration 3, loss = 0.45961693\n",
      "Iteration 4, loss = 0.45409020\n",
      "Iteration 5, loss = 0.44859870\n",
      "Iteration 6, loss = 0.44355081\n",
      "Iteration 7, loss = 0.43934327\n",
      "Iteration 8, loss = 0.43589975\n",
      "Iteration 9, loss = 0.43265630\n",
      "Iteration 10, loss = 0.42929291\n",
      "Iteration 11, loss = 0.42738426\n",
      "Iteration 12, loss = 0.42467593\n",
      "Iteration 13, loss = 0.42419305\n",
      "Iteration 14, loss = 0.42083300\n",
      "Iteration 15, loss = 0.41903771\n",
      "Iteration 16, loss = 0.41697043\n",
      "Iteration 17, loss = 0.41478260\n",
      "Iteration 18, loss = 0.41249628\n",
      "Iteration 19, loss = 0.41154264\n",
      "Iteration 20, loss = 0.40967606\n",
      "Iteration 21, loss = 0.41026452\n",
      "Iteration 22, loss = 0.40704925\n",
      "Iteration 23, loss = 0.40422943\n",
      "Iteration 24, loss = 0.40507351\n",
      "Iteration 25, loss = 0.40168446\n",
      "Iteration 26, loss = 0.40109073\n",
      "Iteration 27, loss = 0.39779017\n",
      "Iteration 28, loss = 0.39662212\n",
      "Iteration 29, loss = 0.39596558\n",
      "Iteration 30, loss = 0.39371401\n",
      "Iteration 31, loss = 0.39098909\n",
      "Iteration 32, loss = 0.39130690\n",
      "Iteration 33, loss = 0.38846604\n",
      "Iteration 34, loss = 0.38715408\n",
      "Iteration 35, loss = 0.38782524\n",
      "Iteration 36, loss = 0.38639492\n",
      "Iteration 37, loss = 0.38164106\n",
      "Iteration 38, loss = 0.38291363\n",
      "Iteration 39, loss = 0.38149161\n",
      "Iteration 40, loss = 0.37849422\n",
      "Iteration 41, loss = 0.37892082\n",
      "Iteration 42, loss = 0.37495968\n",
      "Iteration 43, loss = 0.37605535\n",
      "Iteration 44, loss = 0.37276709\n",
      "Iteration 45, loss = 0.37231787\n",
      "Iteration 46, loss = 0.36897558\n",
      "Iteration 47, loss = 0.37042978\n",
      "Iteration 48, loss = 0.36815550\n",
      "Iteration 49, loss = 0.36638666\n",
      "Iteration 50, loss = 0.36515095\n",
      "Iteration 1, loss = 0.51680692\n",
      "Iteration 2, loss = 0.46156680\n",
      "Iteration 3, loss = 0.45685363\n",
      "Iteration 4, loss = 0.45227948\n",
      "Iteration 5, loss = 0.44861548\n",
      "Iteration 6, loss = 0.44182589\n",
      "Iteration 7, loss = 0.43707453\n",
      "Iteration 8, loss = 0.43321836\n",
      "Iteration 9, loss = 0.43030337\n",
      "Iteration 10, loss = 0.42631986\n",
      "Iteration 11, loss = 0.42186854\n",
      "Iteration 12, loss = 0.41920053\n",
      "Iteration 13, loss = 0.41799734\n",
      "Iteration 14, loss = 0.41447562\n",
      "Iteration 15, loss = 0.41406984\n",
      "Iteration 16, loss = 0.40971920\n",
      "Iteration 17, loss = 0.41021940\n",
      "Iteration 18, loss = 0.40704529\n",
      "Iteration 19, loss = 0.40620029\n",
      "Iteration 20, loss = 0.40452593\n",
      "Iteration 21, loss = 0.40246920\n",
      "Iteration 22, loss = 0.39895119\n",
      "Iteration 23, loss = 0.40081613\n",
      "Iteration 24, loss = 0.39741056\n",
      "Iteration 25, loss = 0.39453212\n",
      "Iteration 26, loss = 0.39443238\n",
      "Iteration 27, loss = 0.39445574\n",
      "Iteration 28, loss = 0.39191684\n",
      "Iteration 29, loss = 0.38999590\n",
      "Iteration 30, loss = 0.38770023\n",
      "Iteration 31, loss = 0.38807660\n",
      "Iteration 32, loss = 0.38735489\n",
      "Iteration 33, loss = 0.38432201\n",
      "Iteration 34, loss = 0.38270317\n",
      "Iteration 35, loss = 0.38105026\n",
      "Iteration 36, loss = 0.37936565\n",
      "Iteration 37, loss = 0.37904705\n",
      "Iteration 38, loss = 0.37823358\n",
      "Iteration 39, loss = 0.37691299\n",
      "Iteration 40, loss = 0.37490554\n",
      "Iteration 41, loss = 0.37325818\n",
      "Iteration 42, loss = 0.37386810\n",
      "Iteration 43, loss = 0.37103963\n",
      "Iteration 44, loss = 0.37072838\n",
      "Iteration 45, loss = 0.36847951\n",
      "Iteration 46, loss = 0.36701681\n",
      "Iteration 47, loss = 0.36452982\n",
      "Iteration 48, loss = 0.36335764\n",
      "Iteration 49, loss = 0.36492421\n",
      "Iteration 50, loss = 0.36252059\n",
      "Iteration 1, loss = 0.51371915\n",
      "Iteration 2, loss = 0.46254955\n",
      "Iteration 3, loss = 0.45858254\n",
      "Iteration 4, loss = 0.45294801\n",
      "Iteration 5, loss = 0.44789594\n",
      "Iteration 6, loss = 0.44345561\n",
      "Iteration 7, loss = 0.43792351\n",
      "Iteration 8, loss = 0.43443954\n",
      "Iteration 9, loss = 0.42889305\n",
      "Iteration 10, loss = 0.42723220\n",
      "Iteration 11, loss = 0.42325076\n",
      "Iteration 12, loss = 0.42338749\n",
      "Iteration 13, loss = 0.41972272\n",
      "Iteration 14, loss = 0.41776778\n",
      "Iteration 15, loss = 0.41608888\n",
      "Iteration 16, loss = 0.41217310\n",
      "Iteration 17, loss = 0.41183297\n",
      "Iteration 18, loss = 0.40958905\n",
      "Iteration 19, loss = 0.40769060\n",
      "Iteration 20, loss = 0.40572911\n",
      "Iteration 21, loss = 0.40468048\n",
      "Iteration 22, loss = 0.40195259\n",
      "Iteration 23, loss = 0.40129613\n",
      "Iteration 24, loss = 0.39748823\n",
      "Iteration 25, loss = 0.39784273\n",
      "Iteration 26, loss = 0.39572002\n",
      "Iteration 27, loss = 0.39347345\n",
      "Iteration 28, loss = 0.39246724\n",
      "Iteration 29, loss = 0.39049021\n",
      "Iteration 30, loss = 0.38855041\n",
      "Iteration 31, loss = 0.38752781\n",
      "Iteration 32, loss = 0.38533136\n",
      "Iteration 33, loss = 0.38453091\n",
      "Iteration 34, loss = 0.38286635\n",
      "Iteration 35, loss = 0.37955876\n",
      "Iteration 36, loss = 0.37791849\n",
      "Iteration 37, loss = 0.37934465\n",
      "Iteration 38, loss = 0.37643096\n",
      "Iteration 39, loss = 0.37338572\n",
      "Iteration 40, loss = 0.37447350\n",
      "Iteration 41, loss = 0.37252885\n",
      "Iteration 42, loss = 0.37046267\n",
      "Iteration 43, loss = 0.36941667\n",
      "Iteration 44, loss = 0.36685338\n",
      "Iteration 45, loss = 0.36690709\n",
      "Iteration 46, loss = 0.36382795\n",
      "Iteration 47, loss = 0.36559091\n",
      "Iteration 48, loss = 0.36147883\n",
      "Iteration 49, loss = 0.36284286\n",
      "Iteration 50, loss = 0.35970531\n",
      "Iteration 1, loss = 0.51308464\n",
      "Iteration 2, loss = 0.46050937\n",
      "Iteration 3, loss = 0.45654634\n",
      "Iteration 4, loss = 0.45262467\n",
      "Iteration 5, loss = 0.44723370\n",
      "Iteration 6, loss = 0.44260343\n",
      "Iteration 7, loss = 0.43733085\n",
      "Iteration 8, loss = 0.43251354\n",
      "Iteration 9, loss = 0.43013202\n",
      "Iteration 10, loss = 0.42716844\n",
      "Iteration 11, loss = 0.42548891\n",
      "Iteration 12, loss = 0.42181984\n",
      "Iteration 13, loss = 0.42092955\n",
      "Iteration 14, loss = 0.41795580\n",
      "Iteration 15, loss = 0.41412132\n",
      "Iteration 16, loss = 0.41396710\n",
      "Iteration 17, loss = 0.41243381\n",
      "Iteration 18, loss = 0.41068569\n",
      "Iteration 19, loss = 0.40769172\n",
      "Iteration 20, loss = 0.40614521\n",
      "Iteration 21, loss = 0.40404852\n",
      "Iteration 22, loss = 0.40324395\n",
      "Iteration 23, loss = 0.40092928\n",
      "Iteration 24, loss = 0.40124690\n",
      "Iteration 25, loss = 0.39790649\n",
      "Iteration 26, loss = 0.39468568\n",
      "Iteration 27, loss = 0.39451370\n",
      "Iteration 28, loss = 0.39298319\n",
      "Iteration 29, loss = 0.39189667\n",
      "Iteration 30, loss = 0.39086884\n",
      "Iteration 31, loss = 0.38708960\n",
      "Iteration 32, loss = 0.38619213\n",
      "Iteration 33, loss = 0.38712734\n",
      "Iteration 34, loss = 0.38430087\n",
      "Iteration 35, loss = 0.38346612\n",
      "Iteration 36, loss = 0.38126073\n",
      "Iteration 37, loss = 0.38082028\n",
      "Iteration 38, loss = 0.37852504\n",
      "Iteration 39, loss = 0.37655375\n",
      "Iteration 40, loss = 0.37573005\n",
      "Iteration 41, loss = 0.37501531\n",
      "Iteration 42, loss = 0.37357618\n",
      "Iteration 43, loss = 0.37191745\n",
      "Iteration 44, loss = 0.36905609\n",
      "Iteration 45, loss = 0.36938447\n",
      "Iteration 46, loss = 0.36923698\n",
      "Iteration 47, loss = 0.36716650\n",
      "Iteration 48, loss = 0.36602456\n",
      "Iteration 49, loss = 0.36155871\n",
      "Iteration 50, loss = 0.36405120\n",
      "Iteration 1, loss = 0.51841884\n",
      "Iteration 2, loss = 0.46690840\n",
      "Iteration 3, loss = 0.46003887\n",
      "Iteration 4, loss = 0.45767156\n",
      "Iteration 5, loss = 0.45279816\n",
      "Iteration 6, loss = 0.44657052\n",
      "Iteration 7, loss = 0.44224975\n",
      "Iteration 8, loss = 0.43761188\n",
      "Iteration 9, loss = 0.43278329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.42922216\n",
      "Iteration 11, loss = 0.42816084\n",
      "Iteration 12, loss = 0.42511030\n",
      "Iteration 13, loss = 0.42151872\n",
      "Iteration 14, loss = 0.41994772\n",
      "Iteration 15, loss = 0.41698091\n",
      "Iteration 16, loss = 0.41339056\n",
      "Iteration 17, loss = 0.41177016\n",
      "Iteration 18, loss = 0.40924489\n",
      "Iteration 19, loss = 0.40824486\n",
      "Iteration 20, loss = 0.40688914\n",
      "Iteration 21, loss = 0.40528737\n",
      "Iteration 22, loss = 0.40237960\n",
      "Iteration 23, loss = 0.40269921\n",
      "Iteration 24, loss = 0.40047124\n",
      "Iteration 25, loss = 0.39698461\n",
      "Iteration 26, loss = 0.39626686\n",
      "Iteration 27, loss = 0.39452740\n",
      "Iteration 28, loss = 0.39409547\n",
      "Iteration 29, loss = 0.39244273\n",
      "Iteration 30, loss = 0.38979818\n",
      "Iteration 31, loss = 0.38670058\n",
      "Iteration 32, loss = 0.38673709\n",
      "Iteration 33, loss = 0.38508392\n",
      "Iteration 34, loss = 0.38539858\n",
      "Iteration 35, loss = 0.38332700\n",
      "Iteration 36, loss = 0.38116173\n",
      "Iteration 37, loss = 0.38097444\n",
      "Iteration 38, loss = 0.38013838\n",
      "Iteration 39, loss = 0.37711366\n",
      "Iteration 40, loss = 0.37799901\n",
      "Iteration 41, loss = 0.37496912\n",
      "Iteration 42, loss = 0.37316169\n",
      "Iteration 43, loss = 0.37265972\n",
      "Iteration 44, loss = 0.37076827\n",
      "Iteration 45, loss = 0.37013895\n",
      "Iteration 46, loss = 0.36736983\n",
      "Iteration 47, loss = 0.36647199\n",
      "Iteration 48, loss = 0.36444934\n",
      "Iteration 49, loss = 0.36171643\n",
      "Iteration 50, loss = 0.36234410\n",
      "Iteration 1, loss = 0.51540557\n",
      "Iteration 2, loss = 0.46127736\n",
      "Iteration 3, loss = 0.45836147\n",
      "Iteration 4, loss = 0.45404606\n",
      "Iteration 5, loss = 0.44902173\n",
      "Iteration 6, loss = 0.44254452\n",
      "Iteration 7, loss = 0.43843448\n",
      "Iteration 8, loss = 0.43565162\n",
      "Iteration 9, loss = 0.43078738\n",
      "Iteration 10, loss = 0.42726847\n",
      "Iteration 11, loss = 0.42686030\n",
      "Iteration 12, loss = 0.42346590\n",
      "Iteration 13, loss = 0.42111531\n",
      "Iteration 14, loss = 0.41889692\n",
      "Iteration 15, loss = 0.41690544\n",
      "Iteration 16, loss = 0.41664249\n",
      "Iteration 17, loss = 0.41293744\n",
      "Iteration 18, loss = 0.41166825\n",
      "Iteration 19, loss = 0.40964614\n",
      "Iteration 20, loss = 0.40795424\n",
      "Iteration 21, loss = 0.40577377\n",
      "Iteration 22, loss = 0.40436331\n",
      "Iteration 23, loss = 0.40322074\n",
      "Iteration 24, loss = 0.40047180\n",
      "Iteration 25, loss = 0.39843784\n",
      "Iteration 26, loss = 0.39788272\n",
      "Iteration 27, loss = 0.39702133\n",
      "Iteration 28, loss = 0.39411684\n",
      "Iteration 29, loss = 0.39503792\n",
      "Iteration 30, loss = 0.39231061\n",
      "Iteration 31, loss = 0.39419315\n",
      "Iteration 32, loss = 0.38884922\n",
      "Iteration 33, loss = 0.38746086\n",
      "Iteration 34, loss = 0.38697035\n",
      "Iteration 35, loss = 0.38699451\n",
      "Iteration 36, loss = 0.38555847\n",
      "Iteration 37, loss = 0.38362495\n",
      "Iteration 38, loss = 0.38099560\n",
      "Iteration 39, loss = 0.38044305\n",
      "Iteration 40, loss = 0.37957883\n",
      "Iteration 41, loss = 0.37691534\n",
      "Iteration 42, loss = 0.37566071\n",
      "Iteration 43, loss = 0.37645406\n",
      "Iteration 44, loss = 0.37339835\n",
      "Iteration 45, loss = 0.37331360\n",
      "Iteration 46, loss = 0.37061490\n",
      "Iteration 47, loss = 0.36826277\n",
      "Iteration 48, loss = 0.36873139\n",
      "Iteration 49, loss = 0.36559046\n",
      "Iteration 50, loss = 0.36464780\n",
      "Iteration 1, loss = 0.51750393\n",
      "Iteration 2, loss = 0.46178477\n",
      "Iteration 3, loss = 0.45751199\n",
      "Iteration 4, loss = 0.45215181\n",
      "Iteration 5, loss = 0.44871329\n",
      "Iteration 6, loss = 0.44276256\n",
      "Iteration 7, loss = 0.43735100\n",
      "Iteration 8, loss = 0.43361879\n",
      "Iteration 9, loss = 0.43116704\n",
      "Iteration 10, loss = 0.42902094\n",
      "Iteration 11, loss = 0.42458770\n",
      "Iteration 12, loss = 0.42249246\n",
      "Iteration 13, loss = 0.42041460\n",
      "Iteration 14, loss = 0.41756846\n",
      "Iteration 15, loss = 0.41662925\n",
      "Iteration 16, loss = 0.41522172\n",
      "Iteration 17, loss = 0.41068373\n",
      "Iteration 18, loss = 0.41050858\n",
      "Iteration 19, loss = 0.40933740\n",
      "Iteration 20, loss = 0.40753914\n",
      "Iteration 21, loss = 0.40706638\n",
      "Iteration 22, loss = 0.40589930\n",
      "Iteration 23, loss = 0.40168170\n",
      "Iteration 24, loss = 0.40174342\n",
      "Iteration 25, loss = 0.40055899\n",
      "Iteration 26, loss = 0.39862877\n",
      "Iteration 27, loss = 0.39704490\n",
      "Iteration 28, loss = 0.39511492\n",
      "Iteration 29, loss = 0.39301742\n",
      "Iteration 30, loss = 0.39283287\n",
      "Iteration 31, loss = 0.39093959\n",
      "Iteration 32, loss = 0.39108513\n",
      "Iteration 33, loss = 0.38882264\n",
      "Iteration 34, loss = 0.38663019\n",
      "Iteration 35, loss = 0.38629978\n",
      "Iteration 36, loss = 0.38422040\n",
      "Iteration 37, loss = 0.38420407\n",
      "Iteration 38, loss = 0.38133901\n",
      "Iteration 39, loss = 0.38136386\n",
      "Iteration 40, loss = 0.38062916\n",
      "Iteration 41, loss = 0.38025052\n",
      "Iteration 42, loss = 0.37666248\n",
      "Iteration 43, loss = 0.37427211\n",
      "Iteration 44, loss = 0.37252076\n",
      "Iteration 45, loss = 0.37300551\n",
      "Iteration 46, loss = 0.37108577\n",
      "Iteration 47, loss = 0.36957921\n",
      "Iteration 48, loss = 0.37093553\n",
      "Iteration 49, loss = 0.36756576\n",
      "Iteration 50, loss = 0.36268524\n",
      "Iteration 1, loss = 0.51490879\n",
      "Iteration 2, loss = 0.46209655\n",
      "Iteration 3, loss = 0.45969529\n",
      "Iteration 4, loss = 0.45394671\n",
      "Iteration 5, loss = 0.44996018\n",
      "Iteration 6, loss = 0.44340037\n",
      "Iteration 7, loss = 0.43977802\n",
      "Iteration 8, loss = 0.43386816\n",
      "Iteration 9, loss = 0.43126407\n",
      "Iteration 10, loss = 0.42756765\n",
      "Iteration 11, loss = 0.42388078\n",
      "Iteration 12, loss = 0.42270705\n",
      "Iteration 13, loss = 0.41880764\n",
      "Iteration 14, loss = 0.41683849\n",
      "Iteration 15, loss = 0.41459857\n",
      "Iteration 16, loss = 0.41377117\n",
      "Iteration 17, loss = 0.41039819\n",
      "Iteration 18, loss = 0.41042330\n",
      "Iteration 19, loss = 0.40437965\n",
      "Iteration 20, loss = 0.40678792\n",
      "Iteration 21, loss = 0.40361220\n",
      "Iteration 22, loss = 0.40357941\n",
      "Iteration 23, loss = 0.39917887\n",
      "Iteration 24, loss = 0.39768637\n",
      "Iteration 25, loss = 0.39604022\n",
      "Iteration 26, loss = 0.39528528\n",
      "Iteration 27, loss = 0.39330659\n",
      "Iteration 28, loss = 0.39198718\n",
      "Iteration 29, loss = 0.39035554\n",
      "Iteration 30, loss = 0.38849522\n",
      "Iteration 31, loss = 0.38616062\n",
      "Iteration 32, loss = 0.38586315\n",
      "Iteration 33, loss = 0.38489446\n",
      "Iteration 34, loss = 0.38468124\n",
      "Iteration 35, loss = 0.38304800\n",
      "Iteration 36, loss = 0.38368544\n",
      "Iteration 37, loss = 0.38012193\n",
      "Iteration 38, loss = 0.37715886\n",
      "Iteration 39, loss = 0.37756898\n",
      "Iteration 40, loss = 0.37565777\n",
      "Iteration 41, loss = 0.37194045\n",
      "Iteration 42, loss = 0.37420626\n",
      "Iteration 43, loss = 0.37266312\n",
      "Iteration 44, loss = 0.37020347\n",
      "Iteration 45, loss = 0.36887611\n",
      "Iteration 46, loss = 0.36882449\n",
      "Iteration 47, loss = 0.36511518\n",
      "Iteration 48, loss = 0.36394452\n",
      "Iteration 49, loss = 0.36430007\n",
      "Iteration 50, loss = 0.36380959\n",
      "Iteration 1, loss = 0.51746358\n",
      "Iteration 2, loss = 0.45998162\n",
      "Iteration 3, loss = 0.45715609\n",
      "Iteration 4, loss = 0.45162072\n",
      "Iteration 5, loss = 0.44679711\n",
      "Iteration 6, loss = 0.44175558\n",
      "Iteration 7, loss = 0.43715443\n",
      "Iteration 8, loss = 0.43161211\n",
      "Iteration 9, loss = 0.42951622\n",
      "Iteration 10, loss = 0.42446512\n",
      "Iteration 11, loss = 0.42237830\n",
      "Iteration 12, loss = 0.41988416\n",
      "Iteration 13, loss = 0.41898983\n",
      "Iteration 14, loss = 0.41614247\n",
      "Iteration 15, loss = 0.41421482\n",
      "Iteration 16, loss = 0.41145408\n",
      "Iteration 17, loss = 0.40946196\n",
      "Iteration 18, loss = 0.40884140\n",
      "Iteration 19, loss = 0.40699788\n",
      "Iteration 20, loss = 0.40414374\n",
      "Iteration 21, loss = 0.40469907\n",
      "Iteration 22, loss = 0.40089925\n",
      "Iteration 23, loss = 0.39996522\n",
      "Iteration 24, loss = 0.40004307\n",
      "Iteration 25, loss = 0.39736425\n",
      "Iteration 26, loss = 0.39431500\n",
      "Iteration 27, loss = 0.39392903\n",
      "Iteration 28, loss = 0.39328898\n",
      "Iteration 29, loss = 0.39038129\n",
      "Iteration 30, loss = 0.38981291\n",
      "Iteration 31, loss = 0.38726525\n",
      "Iteration 32, loss = 0.38631892\n",
      "Iteration 33, loss = 0.38372450\n",
      "Iteration 34, loss = 0.38486479\n",
      "Iteration 35, loss = 0.38270721\n",
      "Iteration 36, loss = 0.38280728\n",
      "Iteration 37, loss = 0.38024459\n",
      "Iteration 38, loss = 0.37883017\n",
      "Iteration 39, loss = 0.37693387\n",
      "Iteration 40, loss = 0.37622727\n",
      "Iteration 41, loss = 0.37557747\n",
      "Iteration 42, loss = 0.37404505\n",
      "Iteration 43, loss = 0.37033542\n",
      "Iteration 44, loss = 0.36824663\n",
      "Iteration 45, loss = 0.36901185\n",
      "Iteration 46, loss = 0.36860082\n",
      "Iteration 47, loss = 0.36615018\n",
      "Iteration 48, loss = 0.36261559\n",
      "Iteration 49, loss = 0.36323190\n",
      "Iteration 50, loss = 0.36111877\n",
      "Iteration 1, loss = 0.52000073\n",
      "Iteration 2, loss = 0.46223007\n",
      "Iteration 3, loss = 0.45993074\n",
      "Iteration 4, loss = 0.45351888\n",
      "Iteration 5, loss = 0.45067046\n",
      "Iteration 6, loss = 0.44498122\n",
      "Iteration 7, loss = 0.44238837\n",
      "Iteration 8, loss = 0.43527934\n",
      "Iteration 9, loss = 0.43427222\n",
      "Iteration 10, loss = 0.42933353\n",
      "Iteration 11, loss = 0.42861352\n",
      "Iteration 12, loss = 0.42473595\n",
      "Iteration 13, loss = 0.42141820\n",
      "Iteration 14, loss = 0.42185973\n",
      "Iteration 15, loss = 0.41749147\n",
      "Iteration 16, loss = 0.41504010\n",
      "Iteration 17, loss = 0.41557791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.41061165\n",
      "Iteration 19, loss = 0.41126899\n",
      "Iteration 20, loss = 0.40862383\n",
      "Iteration 21, loss = 0.40675004\n",
      "Iteration 22, loss = 0.40499160\n",
      "Iteration 23, loss = 0.40352282\n",
      "Iteration 24, loss = 0.40301790\n",
      "Iteration 25, loss = 0.40163497\n",
      "Iteration 26, loss = 0.39973739\n",
      "Iteration 27, loss = 0.39816895\n",
      "Iteration 28, loss = 0.39597522\n",
      "Iteration 29, loss = 0.39539378\n",
      "Iteration 30, loss = 0.39303403\n",
      "Iteration 31, loss = 0.39249783\n",
      "Iteration 32, loss = 0.39066200\n",
      "Iteration 33, loss = 0.39066085\n",
      "Iteration 34, loss = 0.38891315\n",
      "Iteration 35, loss = 0.38769124\n",
      "Iteration 36, loss = 0.38838522\n",
      "Iteration 37, loss = 0.38263587\n",
      "Iteration 38, loss = 0.38119575\n",
      "Iteration 39, loss = 0.38037878\n",
      "Iteration 40, loss = 0.38032269\n",
      "Iteration 41, loss = 0.37950799\n",
      "Iteration 42, loss = 0.37755283\n",
      "Iteration 43, loss = 0.37548003\n",
      "Iteration 44, loss = 0.37469762\n",
      "Iteration 45, loss = 0.37301623\n",
      "Iteration 46, loss = 0.37082626\n",
      "Iteration 47, loss = 0.37171149\n",
      "Iteration 48, loss = 0.36829969\n",
      "Iteration 49, loss = 0.36660974\n",
      "Iteration 50, loss = 0.36662987\n",
      "Train Accuracy: 0.7878\n",
      "Running Memory: 17.45 MB\n",
      "Running Time: 67.31 Seconds\n",
      "Test  Accuracy: 0.7949\n",
      "Test  Presion:  0.7771\n",
      "Test  Recall:   0.8142\n",
      "Test  F1-score: 0.7952\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS2_tfidf_1, memory_MLP_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS2_tfidf_1, acc_MLP_DS2_tfidf_1, pre_MLP_DS2_tfidf_1, rec_MLP_DS2_tfidf_1, f1_MLP_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS2_tfidf_1, model_3, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49834711\n",
      "Iteration 2, loss = 0.42835554\n",
      "Iteration 3, loss = 0.42468172\n",
      "Iteration 4, loss = 0.41887955\n",
      "Iteration 5, loss = 0.41654325\n",
      "Iteration 6, loss = 0.41274253\n",
      "Iteration 7, loss = 0.41156645\n",
      "Iteration 8, loss = 0.40986678\n",
      "Iteration 9, loss = 0.40682196\n",
      "Iteration 10, loss = 0.40673906\n",
      "Iteration 11, loss = 0.40444009\n",
      "Iteration 12, loss = 0.40395451\n",
      "Iteration 13, loss = 0.40262960\n",
      "Iteration 14, loss = 0.40149535\n",
      "Iteration 15, loss = 0.40011296\n",
      "Iteration 16, loss = 0.39909146\n",
      "Iteration 17, loss = 0.39776973\n",
      "Iteration 18, loss = 0.39708867\n",
      "Iteration 19, loss = 0.39598347\n",
      "Iteration 20, loss = 0.39531964\n",
      "Iteration 21, loss = 0.39347534\n",
      "Iteration 22, loss = 0.39320120\n",
      "Iteration 23, loss = 0.39325956\n",
      "Iteration 24, loss = 0.39117883\n",
      "Iteration 25, loss = 0.39163784\n",
      "Iteration 26, loss = 0.38853769\n",
      "Iteration 27, loss = 0.39054692\n",
      "Iteration 28, loss = 0.38732504\n",
      "Iteration 29, loss = 0.38666246\n",
      "Iteration 30, loss = 0.38566629\n",
      "Iteration 31, loss = 0.38730989\n",
      "Iteration 32, loss = 0.38440528\n",
      "Iteration 33, loss = 0.38439201\n",
      "Iteration 34, loss = 0.38314951\n",
      "Iteration 35, loss = 0.38289767\n",
      "Iteration 36, loss = 0.38191450\n",
      "Iteration 37, loss = 0.38041032\n",
      "Iteration 38, loss = 0.37935188\n",
      "Iteration 39, loss = 0.38006144\n",
      "Iteration 40, loss = 0.37766635\n",
      "Iteration 41, loss = 0.37737990\n",
      "Iteration 42, loss = 0.37670341\n",
      "Iteration 43, loss = 0.37653245\n",
      "Iteration 44, loss = 0.37557155\n",
      "Iteration 45, loss = 0.37513846\n",
      "Iteration 46, loss = 0.37483182\n",
      "Iteration 47, loss = 0.37266739\n",
      "Iteration 48, loss = 0.37363175\n",
      "Iteration 49, loss = 0.37152913\n",
      "Iteration 50, loss = 0.37141271\n",
      "Iteration 1, loss = 0.50646802\n",
      "Iteration 2, loss = 0.43153846\n",
      "Iteration 3, loss = 0.42665954\n",
      "Iteration 4, loss = 0.42180309\n",
      "Iteration 5, loss = 0.41742625\n",
      "Iteration 6, loss = 0.41557418\n",
      "Iteration 7, loss = 0.41318434\n",
      "Iteration 8, loss = 0.41247437\n",
      "Iteration 9, loss = 0.40883257\n",
      "Iteration 10, loss = 0.40905227\n",
      "Iteration 11, loss = 0.40765028\n",
      "Iteration 12, loss = 0.40527730\n",
      "Iteration 13, loss = 0.40410988\n",
      "Iteration 14, loss = 0.40402502\n",
      "Iteration 15, loss = 0.40274331\n",
      "Iteration 16, loss = 0.40128003\n",
      "Iteration 17, loss = 0.39992339\n",
      "Iteration 18, loss = 0.39860544\n",
      "Iteration 19, loss = 0.39793887\n",
      "Iteration 20, loss = 0.39756412\n",
      "Iteration 21, loss = 0.39664886\n",
      "Iteration 22, loss = 0.39570938\n",
      "Iteration 23, loss = 0.39422387\n",
      "Iteration 24, loss = 0.39393459\n",
      "Iteration 25, loss = 0.39429973\n",
      "Iteration 26, loss = 0.39376724\n",
      "Iteration 27, loss = 0.39264156\n",
      "Iteration 28, loss = 0.39046440\n",
      "Iteration 29, loss = 0.38942841\n",
      "Iteration 30, loss = 0.38976488\n",
      "Iteration 31, loss = 0.38803620\n",
      "Iteration 32, loss = 0.38799469\n",
      "Iteration 33, loss = 0.38678987\n",
      "Iteration 34, loss = 0.38562052\n",
      "Iteration 35, loss = 0.38473059\n",
      "Iteration 36, loss = 0.38571801\n",
      "Iteration 37, loss = 0.38422539\n",
      "Iteration 38, loss = 0.38416681\n",
      "Iteration 39, loss = 0.38250589\n",
      "Iteration 40, loss = 0.37989451\n",
      "Iteration 41, loss = 0.38145545\n",
      "Iteration 42, loss = 0.37971084\n",
      "Iteration 43, loss = 0.37922587\n",
      "Iteration 44, loss = 0.37913831\n",
      "Iteration 45, loss = 0.37807054\n",
      "Iteration 46, loss = 0.37631584\n",
      "Iteration 47, loss = 0.37562583\n",
      "Iteration 48, loss = 0.37591896\n",
      "Iteration 49, loss = 0.37483799\n",
      "Iteration 50, loss = 0.37483248\n",
      "Iteration 1, loss = 0.50897167\n",
      "Iteration 2, loss = 0.43077542\n",
      "Iteration 3, loss = 0.42373842\n",
      "Iteration 4, loss = 0.41968471\n",
      "Iteration 5, loss = 0.41738877\n",
      "Iteration 6, loss = 0.41421378\n",
      "Iteration 7, loss = 0.41114283\n",
      "Iteration 8, loss = 0.41071744\n",
      "Iteration 9, loss = 0.40911387\n",
      "Iteration 10, loss = 0.40799100\n",
      "Iteration 11, loss = 0.40531789\n",
      "Iteration 12, loss = 0.40377325\n",
      "Iteration 13, loss = 0.40326896\n",
      "Iteration 14, loss = 0.40243966\n",
      "Iteration 15, loss = 0.40195983\n",
      "Iteration 16, loss = 0.40104292\n",
      "Iteration 17, loss = 0.39957611\n",
      "Iteration 18, loss = 0.39897287\n",
      "Iteration 19, loss = 0.39759890\n",
      "Iteration 20, loss = 0.39696938\n",
      "Iteration 21, loss = 0.39647639\n",
      "Iteration 22, loss = 0.39502718\n",
      "Iteration 23, loss = 0.39508610\n",
      "Iteration 24, loss = 0.39487130\n",
      "Iteration 25, loss = 0.39263878\n",
      "Iteration 26, loss = 0.39274912\n",
      "Iteration 27, loss = 0.39240712\n",
      "Iteration 28, loss = 0.39177418\n",
      "Iteration 29, loss = 0.39111650\n",
      "Iteration 30, loss = 0.38904444\n",
      "Iteration 31, loss = 0.38826779\n",
      "Iteration 32, loss = 0.38815745\n",
      "Iteration 33, loss = 0.38704510\n",
      "Iteration 34, loss = 0.38701157\n",
      "Iteration 35, loss = 0.38584276\n",
      "Iteration 36, loss = 0.38464664\n",
      "Iteration 37, loss = 0.38443151\n",
      "Iteration 38, loss = 0.38412325\n",
      "Iteration 39, loss = 0.38219994\n",
      "Iteration 40, loss = 0.38266041\n",
      "Iteration 41, loss = 0.38118820\n",
      "Iteration 42, loss = 0.38092709\n",
      "Iteration 43, loss = 0.37969985\n",
      "Iteration 44, loss = 0.38015805\n",
      "Iteration 45, loss = 0.37813207\n",
      "Iteration 46, loss = 0.37732403\n",
      "Iteration 47, loss = 0.37697841\n",
      "Iteration 48, loss = 0.37693614\n",
      "Iteration 49, loss = 0.37431298\n",
      "Iteration 50, loss = 0.37551358\n",
      "Iteration 1, loss = 0.50675089\n",
      "Iteration 2, loss = 0.43104154\n",
      "Iteration 3, loss = 0.42480006\n",
      "Iteration 4, loss = 0.42063066\n",
      "Iteration 5, loss = 0.41706139\n",
      "Iteration 6, loss = 0.41412132\n",
      "Iteration 7, loss = 0.41320174\n",
      "Iteration 8, loss = 0.41032778\n",
      "Iteration 9, loss = 0.40842923\n",
      "Iteration 10, loss = 0.40695101\n",
      "Iteration 11, loss = 0.40624032\n",
      "Iteration 12, loss = 0.40571976\n",
      "Iteration 13, loss = 0.40243239\n",
      "Iteration 14, loss = 0.40235142\n",
      "Iteration 15, loss = 0.40167869\n",
      "Iteration 16, loss = 0.40015777\n",
      "Iteration 17, loss = 0.39956865\n",
      "Iteration 18, loss = 0.39930186\n",
      "Iteration 19, loss = 0.39778079\n",
      "Iteration 20, loss = 0.39707917\n",
      "Iteration 21, loss = 0.39613673\n",
      "Iteration 22, loss = 0.39544949\n",
      "Iteration 23, loss = 0.39370455\n",
      "Iteration 24, loss = 0.39346113\n",
      "Iteration 25, loss = 0.39183472\n",
      "Iteration 26, loss = 0.39134028\n",
      "Iteration 27, loss = 0.39047403\n",
      "Iteration 28, loss = 0.39073120\n",
      "Iteration 29, loss = 0.38878576\n",
      "Iteration 30, loss = 0.38701421\n",
      "Iteration 31, loss = 0.38772759\n",
      "Iteration 32, loss = 0.38614795\n",
      "Iteration 33, loss = 0.38599554\n",
      "Iteration 34, loss = 0.38384250\n",
      "Iteration 35, loss = 0.38358976\n",
      "Iteration 36, loss = 0.38319719\n",
      "Iteration 37, loss = 0.38135463\n",
      "Iteration 38, loss = 0.38101433\n",
      "Iteration 39, loss = 0.38119580\n",
      "Iteration 40, loss = 0.37986255\n",
      "Iteration 41, loss = 0.37958725\n",
      "Iteration 42, loss = 0.37773147\n",
      "Iteration 43, loss = 0.37722564\n",
      "Iteration 44, loss = 0.37658379\n",
      "Iteration 45, loss = 0.37603735\n",
      "Iteration 46, loss = 0.37484698\n",
      "Iteration 47, loss = 0.37446834\n",
      "Iteration 48, loss = 0.37401202\n",
      "Iteration 49, loss = 0.37223281\n",
      "Iteration 50, loss = 0.37227766\n",
      "Iteration 1, loss = 0.50417593\n",
      "Iteration 2, loss = 0.42785202\n",
      "Iteration 3, loss = 0.42235538\n",
      "Iteration 4, loss = 0.41769051\n",
      "Iteration 5, loss = 0.41556784\n",
      "Iteration 6, loss = 0.41211417\n",
      "Iteration 7, loss = 0.41018933\n",
      "Iteration 8, loss = 0.40761222\n",
      "Iteration 9, loss = 0.40526084\n",
      "Iteration 10, loss = 0.40413902\n",
      "Iteration 11, loss = 0.40223534\n",
      "Iteration 12, loss = 0.40226074\n",
      "Iteration 13, loss = 0.39980844\n",
      "Iteration 14, loss = 0.39956854\n",
      "Iteration 15, loss = 0.39736340\n",
      "Iteration 16, loss = 0.39728198\n",
      "Iteration 17, loss = 0.39718332\n",
      "Iteration 18, loss = 0.39557137\n",
      "Iteration 19, loss = 0.39409273\n",
      "Iteration 20, loss = 0.39353295\n",
      "Iteration 21, loss = 0.39257946\n",
      "Iteration 22, loss = 0.39180226\n",
      "Iteration 23, loss = 0.39016954\n",
      "Iteration 24, loss = 0.39080530\n",
      "Iteration 25, loss = 0.38853691\n",
      "Iteration 26, loss = 0.38740626\n",
      "Iteration 27, loss = 0.38675468\n",
      "Iteration 28, loss = 0.38621194\n",
      "Iteration 29, loss = 0.38585435\n",
      "Iteration 30, loss = 0.38451041\n",
      "Iteration 31, loss = 0.38192769\n",
      "Iteration 32, loss = 0.38283885\n",
      "Iteration 33, loss = 0.38113221\n",
      "Iteration 34, loss = 0.38003918\n",
      "Iteration 35, loss = 0.38107519\n",
      "Iteration 36, loss = 0.37970238\n",
      "Iteration 37, loss = 0.37870071\n",
      "Iteration 38, loss = 0.37776683\n",
      "Iteration 39, loss = 0.37721468\n",
      "Iteration 40, loss = 0.37732929\n",
      "Iteration 41, loss = 0.37587978\n",
      "Iteration 42, loss = 0.37404845\n",
      "Iteration 43, loss = 0.37385360\n",
      "Iteration 44, loss = 0.37284273\n",
      "Iteration 45, loss = 0.37143912\n",
      "Iteration 46, loss = 0.37188429\n",
      "Iteration 47, loss = 0.37095542\n",
      "Iteration 48, loss = 0.37019980\n",
      "Iteration 49, loss = 0.36939050\n",
      "Iteration 50, loss = 0.36939781\n",
      "Iteration 1, loss = 0.50721966\n",
      "Iteration 2, loss = 0.43241635\n",
      "Iteration 3, loss = 0.42423561\n",
      "Iteration 4, loss = 0.42104187\n",
      "Iteration 5, loss = 0.41811986\n",
      "Iteration 6, loss = 0.41416998\n",
      "Iteration 7, loss = 0.41227644\n",
      "Iteration 8, loss = 0.41025052\n",
      "Iteration 9, loss = 0.40883912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.40696233\n",
      "Iteration 11, loss = 0.40634419\n",
      "Iteration 12, loss = 0.40478075\n",
      "Iteration 13, loss = 0.40324372\n",
      "Iteration 14, loss = 0.40341026\n",
      "Iteration 15, loss = 0.40148268\n",
      "Iteration 16, loss = 0.39986041\n",
      "Iteration 17, loss = 0.39839227\n",
      "Iteration 18, loss = 0.39805829\n",
      "Iteration 19, loss = 0.39870187\n",
      "Iteration 20, loss = 0.39587790\n",
      "Iteration 21, loss = 0.39419021\n",
      "Iteration 22, loss = 0.39477129\n",
      "Iteration 23, loss = 0.39461455\n",
      "Iteration 24, loss = 0.39321577\n",
      "Iteration 25, loss = 0.39140897\n",
      "Iteration 26, loss = 0.39156761\n",
      "Iteration 27, loss = 0.38994991\n",
      "Iteration 28, loss = 0.39000821\n",
      "Iteration 29, loss = 0.38810424\n",
      "Iteration 30, loss = 0.38740149\n",
      "Iteration 31, loss = 0.38630184\n",
      "Iteration 32, loss = 0.38723764\n",
      "Iteration 33, loss = 0.38471507\n",
      "Iteration 34, loss = 0.38440048\n",
      "Iteration 35, loss = 0.38512283\n",
      "Iteration 36, loss = 0.38229948\n",
      "Iteration 37, loss = 0.38153343\n",
      "Iteration 38, loss = 0.38133167\n",
      "Iteration 39, loss = 0.37855154\n",
      "Iteration 40, loss = 0.38111629\n",
      "Iteration 41, loss = 0.37807237\n",
      "Iteration 42, loss = 0.37759909\n",
      "Iteration 43, loss = 0.37776163\n",
      "Iteration 44, loss = 0.37637226\n",
      "Iteration 45, loss = 0.37485417\n",
      "Iteration 46, loss = 0.37554283\n",
      "Iteration 47, loss = 0.37418771\n",
      "Iteration 48, loss = 0.37220000\n",
      "Iteration 49, loss = 0.37229895\n",
      "Iteration 50, loss = 0.37323282\n",
      "Iteration 1, loss = 0.50646675\n",
      "Iteration 2, loss = 0.42826216\n",
      "Iteration 3, loss = 0.42557789\n",
      "Iteration 4, loss = 0.41916029\n",
      "Iteration 5, loss = 0.41722580\n",
      "Iteration 6, loss = 0.41275767\n",
      "Iteration 7, loss = 0.41124046\n",
      "Iteration 8, loss = 0.40910675\n",
      "Iteration 9, loss = 0.40743303\n",
      "Iteration 10, loss = 0.40613005\n",
      "Iteration 11, loss = 0.40470710\n",
      "Iteration 12, loss = 0.40310070\n",
      "Iteration 13, loss = 0.40254447\n",
      "Iteration 14, loss = 0.40087655\n",
      "Iteration 15, loss = 0.39984294\n",
      "Iteration 16, loss = 0.39909134\n",
      "Iteration 17, loss = 0.39785212\n",
      "Iteration 18, loss = 0.39821900\n",
      "Iteration 19, loss = 0.39744045\n",
      "Iteration 20, loss = 0.39548392\n",
      "Iteration 21, loss = 0.39427997\n",
      "Iteration 22, loss = 0.39356366\n",
      "Iteration 23, loss = 0.39284589\n",
      "Iteration 24, loss = 0.39182844\n",
      "Iteration 25, loss = 0.39111691\n",
      "Iteration 26, loss = 0.39129131\n",
      "Iteration 27, loss = 0.38944811\n",
      "Iteration 28, loss = 0.38819230\n",
      "Iteration 29, loss = 0.38710790\n",
      "Iteration 30, loss = 0.38780305\n",
      "Iteration 31, loss = 0.38633451\n",
      "Iteration 32, loss = 0.38544946\n",
      "Iteration 33, loss = 0.38484210\n",
      "Iteration 34, loss = 0.38382662\n",
      "Iteration 35, loss = 0.38452107\n",
      "Iteration 36, loss = 0.38277602\n",
      "Iteration 37, loss = 0.38264695\n",
      "Iteration 38, loss = 0.38074060\n",
      "Iteration 39, loss = 0.38022874\n",
      "Iteration 40, loss = 0.37995130\n",
      "Iteration 41, loss = 0.37955092\n",
      "Iteration 42, loss = 0.37709668\n",
      "Iteration 43, loss = 0.37847539\n",
      "Iteration 44, loss = 0.37580406\n",
      "Iteration 45, loss = 0.37635137\n",
      "Iteration 46, loss = 0.37531870\n",
      "Iteration 47, loss = 0.37490041\n",
      "Iteration 48, loss = 0.37445260\n",
      "Iteration 49, loss = 0.37318219\n",
      "Iteration 50, loss = 0.37251888\n",
      "Iteration 1, loss = 0.50601164\n",
      "Iteration 2, loss = 0.42799431\n",
      "Iteration 3, loss = 0.42315804\n",
      "Iteration 4, loss = 0.41841052\n",
      "Iteration 5, loss = 0.41695640\n",
      "Iteration 6, loss = 0.41276533\n",
      "Iteration 7, loss = 0.41056663\n",
      "Iteration 8, loss = 0.40814983\n",
      "Iteration 9, loss = 0.40697586\n",
      "Iteration 10, loss = 0.40633968\n",
      "Iteration 11, loss = 0.40365916\n",
      "Iteration 12, loss = 0.40184549\n",
      "Iteration 13, loss = 0.40079580\n",
      "Iteration 14, loss = 0.40050971\n",
      "Iteration 15, loss = 0.39787808\n",
      "Iteration 16, loss = 0.39762582\n",
      "Iteration 17, loss = 0.39687381\n",
      "Iteration 18, loss = 0.39519522\n",
      "Iteration 19, loss = 0.39531559\n",
      "Iteration 20, loss = 0.39350175\n",
      "Iteration 21, loss = 0.39216741\n",
      "Iteration 22, loss = 0.39273728\n",
      "Iteration 23, loss = 0.38952323\n",
      "Iteration 24, loss = 0.39109740\n",
      "Iteration 25, loss = 0.38898863\n",
      "Iteration 26, loss = 0.38872791\n",
      "Iteration 27, loss = 0.38734941\n",
      "Iteration 28, loss = 0.38634657\n",
      "Iteration 29, loss = 0.38403443\n",
      "Iteration 30, loss = 0.38621896\n",
      "Iteration 31, loss = 0.38224152\n",
      "Iteration 32, loss = 0.38332500\n",
      "Iteration 33, loss = 0.38201488\n",
      "Iteration 34, loss = 0.38196566\n",
      "Iteration 35, loss = 0.38035387\n",
      "Iteration 36, loss = 0.38054488\n",
      "Iteration 37, loss = 0.37876074\n",
      "Iteration 38, loss = 0.37752559\n",
      "Iteration 39, loss = 0.37695294\n",
      "Iteration 40, loss = 0.37775991\n",
      "Iteration 41, loss = 0.37686515\n",
      "Iteration 42, loss = 0.37394115\n",
      "Iteration 43, loss = 0.37330766\n",
      "Iteration 44, loss = 0.37390034\n",
      "Iteration 45, loss = 0.37305675\n",
      "Iteration 46, loss = 0.37187880\n",
      "Iteration 47, loss = 0.37064612\n",
      "Iteration 48, loss = 0.37203855\n",
      "Iteration 49, loss = 0.37095163\n",
      "Iteration 50, loss = 0.36908539\n",
      "Iteration 1, loss = 0.50622895\n",
      "Iteration 2, loss = 0.43009132\n",
      "Iteration 3, loss = 0.42473179\n",
      "Iteration 4, loss = 0.41955253\n",
      "Iteration 5, loss = 0.41574582\n",
      "Iteration 6, loss = 0.41344357\n",
      "Iteration 7, loss = 0.41091094\n",
      "Iteration 8, loss = 0.40941839\n",
      "Iteration 9, loss = 0.40852369\n",
      "Iteration 10, loss = 0.40529628\n",
      "Iteration 11, loss = 0.40486413\n",
      "Iteration 12, loss = 0.40207705\n",
      "Iteration 13, loss = 0.40156866\n",
      "Iteration 14, loss = 0.40076146\n",
      "Iteration 15, loss = 0.39944410\n",
      "Iteration 16, loss = 0.39751584\n",
      "Iteration 17, loss = 0.39715065\n",
      "Iteration 18, loss = 0.39529867\n",
      "Iteration 19, loss = 0.39487261\n",
      "Iteration 20, loss = 0.39495447\n",
      "Iteration 21, loss = 0.39371542\n",
      "Iteration 22, loss = 0.39328665\n",
      "Iteration 23, loss = 0.39200320\n",
      "Iteration 24, loss = 0.39137816\n",
      "Iteration 25, loss = 0.38900046\n",
      "Iteration 26, loss = 0.38919304\n",
      "Iteration 27, loss = 0.38781238\n",
      "Iteration 28, loss = 0.38672102\n",
      "Iteration 29, loss = 0.38662144\n",
      "Iteration 30, loss = 0.38573293\n",
      "Iteration 31, loss = 0.38507764\n",
      "Iteration 32, loss = 0.38358249\n",
      "Iteration 33, loss = 0.38275514\n",
      "Iteration 34, loss = 0.38187373\n",
      "Iteration 35, loss = 0.38053292\n",
      "Iteration 36, loss = 0.38107994\n",
      "Iteration 37, loss = 0.38075865\n",
      "Iteration 38, loss = 0.37894400\n",
      "Iteration 39, loss = 0.37949888\n",
      "Iteration 40, loss = 0.37848028\n",
      "Iteration 41, loss = 0.37611453\n",
      "Iteration 42, loss = 0.37633993\n",
      "Iteration 43, loss = 0.37603173\n",
      "Iteration 44, loss = 0.37463348\n",
      "Iteration 45, loss = 0.37439634\n",
      "Iteration 46, loss = 0.37285867\n",
      "Iteration 47, loss = 0.37324292\n",
      "Iteration 48, loss = 0.37065261\n",
      "Iteration 49, loss = 0.37068867\n",
      "Iteration 50, loss = 0.37100761\n",
      "Iteration 1, loss = 0.50660792\n",
      "Iteration 2, loss = 0.42861892\n",
      "Iteration 3, loss = 0.42315678\n",
      "Iteration 4, loss = 0.41819144\n",
      "Iteration 5, loss = 0.41637086\n",
      "Iteration 6, loss = 0.41304114\n",
      "Iteration 7, loss = 0.41074519\n",
      "Iteration 8, loss = 0.40845372\n",
      "Iteration 9, loss = 0.40762100\n",
      "Iteration 10, loss = 0.40467839\n",
      "Iteration 11, loss = 0.40460680\n",
      "Iteration 12, loss = 0.40251252\n",
      "Iteration 13, loss = 0.40151688\n",
      "Iteration 14, loss = 0.40064903\n",
      "Iteration 15, loss = 0.39904154\n",
      "Iteration 16, loss = 0.39792598\n",
      "Iteration 17, loss = 0.39754022\n",
      "Iteration 18, loss = 0.39656080\n",
      "Iteration 19, loss = 0.39540884\n",
      "Iteration 20, loss = 0.39565627\n",
      "Iteration 21, loss = 0.39259043\n",
      "Iteration 22, loss = 0.39287865\n",
      "Iteration 23, loss = 0.39230376\n",
      "Iteration 24, loss = 0.39028068\n",
      "Iteration 25, loss = 0.39023244\n",
      "Iteration 26, loss = 0.38966390\n",
      "Iteration 27, loss = 0.38867819\n",
      "Iteration 28, loss = 0.38789565\n",
      "Iteration 29, loss = 0.38673159\n",
      "Iteration 30, loss = 0.38684033\n",
      "Iteration 31, loss = 0.38561912\n",
      "Iteration 32, loss = 0.38472280\n",
      "Iteration 33, loss = 0.38293696\n",
      "Iteration 34, loss = 0.38235027\n",
      "Iteration 35, loss = 0.38195826\n",
      "Iteration 36, loss = 0.38108128\n",
      "Iteration 37, loss = 0.38081243\n",
      "Iteration 38, loss = 0.38093773\n",
      "Iteration 39, loss = 0.37812196\n",
      "Iteration 40, loss = 0.37792862\n",
      "Iteration 41, loss = 0.37705442\n",
      "Iteration 42, loss = 0.37672822\n",
      "Iteration 43, loss = 0.37431202\n",
      "Iteration 44, loss = 0.37461799\n",
      "Iteration 45, loss = 0.37444425\n",
      "Iteration 46, loss = 0.37237902\n",
      "Iteration 47, loss = 0.37261737\n",
      "Iteration 48, loss = 0.37162107\n",
      "Iteration 49, loss = 0.37145316\n",
      "Iteration 50, loss = 0.37014705\n",
      "Iteration 1, loss = 0.50830397\n",
      "Iteration 2, loss = 0.43080379\n",
      "Iteration 3, loss = 0.42459537\n",
      "Iteration 4, loss = 0.41982200\n",
      "Iteration 5, loss = 0.41711125\n",
      "Iteration 6, loss = 0.41570853\n",
      "Iteration 7, loss = 0.41416690\n",
      "Iteration 8, loss = 0.40967413\n",
      "Iteration 9, loss = 0.40861449\n",
      "Iteration 10, loss = 0.40729943\n",
      "Iteration 11, loss = 0.40663861\n",
      "Iteration 12, loss = 0.40395143\n",
      "Iteration 13, loss = 0.40218416\n",
      "Iteration 14, loss = 0.40090218\n",
      "Iteration 15, loss = 0.40027661\n",
      "Iteration 16, loss = 0.39958892\n",
      "Iteration 17, loss = 0.39898017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.39830823\n",
      "Iteration 19, loss = 0.39621524\n",
      "Iteration 20, loss = 0.39626407\n",
      "Iteration 21, loss = 0.39481375\n",
      "Iteration 22, loss = 0.39473749\n",
      "Iteration 23, loss = 0.39232622\n",
      "Iteration 24, loss = 0.39337734\n",
      "Iteration 25, loss = 0.39200539\n",
      "Iteration 26, loss = 0.39110168\n",
      "Iteration 27, loss = 0.38969516\n",
      "Iteration 28, loss = 0.38939023\n",
      "Iteration 29, loss = 0.38877845\n",
      "Iteration 30, loss = 0.38698672\n",
      "Iteration 31, loss = 0.38721384\n",
      "Iteration 32, loss = 0.38600632\n",
      "Iteration 33, loss = 0.38521344\n",
      "Iteration 34, loss = 0.38495037\n",
      "Iteration 35, loss = 0.38342473\n",
      "Iteration 36, loss = 0.38237908\n",
      "Iteration 37, loss = 0.38114778\n",
      "Iteration 38, loss = 0.38204245\n",
      "Iteration 39, loss = 0.38109579\n",
      "Iteration 40, loss = 0.38017901\n",
      "Iteration 41, loss = 0.37866675\n",
      "Iteration 42, loss = 0.37816707\n",
      "Iteration 43, loss = 0.37658008\n",
      "Iteration 44, loss = 0.37661783\n",
      "Iteration 45, loss = 0.37542493\n",
      "Iteration 46, loss = 0.37457197\n",
      "Iteration 47, loss = 0.37353486\n",
      "Iteration 48, loss = 0.37221318\n",
      "Iteration 49, loss = 0.37126770\n",
      "Iteration 50, loss = 0.37157463\n",
      "Train Accuracy: 0.8251\n",
      "Running Memory: 0.04 MB\n",
      "Running Time: 70.95 Seconds\n",
      "Test  Accuracy: 0.8175\n",
      "Test  Presion:  0.7822\n",
      "Test  Recall:   0.8687\n",
      "Test  F1-score: 0.8232\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS2_tfidf_2, memory_MLP_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS2_tfidf_2, acc_MLP_DS2_tfidf_2, pre_MLP_DS2_tfidf_2, rec_MLP_DS2_tfidf_2, f1_MLP_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS2_tfidf_2, model_3, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.49226190\n",
      "Iteration 2, loss = 0.44615816\n",
      "Iteration 3, loss = 0.44542581\n",
      "Iteration 4, loss = 0.44441921\n",
      "Iteration 5, loss = 0.44356958\n",
      "Iteration 6, loss = 0.44165200\n",
      "Iteration 7, loss = 0.44264173\n",
      "Iteration 8, loss = 0.44039824\n",
      "Iteration 9, loss = 0.44008410\n",
      "Iteration 10, loss = 0.44004595\n",
      "Iteration 11, loss = 0.43933849\n",
      "Iteration 12, loss = 0.43955125\n",
      "Iteration 13, loss = 0.43850304\n",
      "Iteration 14, loss = 0.43898696\n",
      "Iteration 15, loss = 0.43799107\n",
      "Iteration 16, loss = 0.43748140\n",
      "Iteration 17, loss = 0.43763828\n",
      "Iteration 18, loss = 0.43705861\n",
      "Iteration 19, loss = 0.43642993\n",
      "Iteration 20, loss = 0.43660815\n",
      "Iteration 21, loss = 0.43523586\n",
      "Iteration 22, loss = 0.43572669\n",
      "Iteration 23, loss = 0.43590995\n",
      "Iteration 24, loss = 0.43578817\n",
      "Iteration 25, loss = 0.43558283\n",
      "Iteration 26, loss = 0.43411803\n",
      "Iteration 27, loss = 0.43576971\n",
      "Iteration 28, loss = 0.43391522\n",
      "Iteration 29, loss = 0.43467482\n",
      "Iteration 30, loss = 0.43310205\n",
      "Iteration 31, loss = 0.43511148\n",
      "Iteration 32, loss = 0.43338113\n",
      "Iteration 33, loss = 0.43337824\n",
      "Iteration 34, loss = 0.43308017\n",
      "Iteration 35, loss = 0.43297854\n",
      "Iteration 36, loss = 0.43276406\n",
      "Iteration 37, loss = 0.43247700\n",
      "Iteration 38, loss = 0.43206158\n",
      "Iteration 39, loss = 0.43211629\n",
      "Iteration 40, loss = 0.43167875\n",
      "Iteration 41, loss = 0.43201923\n",
      "Iteration 42, loss = 0.43163876\n",
      "Iteration 43, loss = 0.43128654\n",
      "Iteration 44, loss = 0.43068148\n",
      "Iteration 45, loss = 0.43050858\n",
      "Iteration 46, loss = 0.43069019\n",
      "Iteration 47, loss = 0.43017049\n",
      "Iteration 48, loss = 0.43054015\n",
      "Iteration 49, loss = 0.42958665\n",
      "Iteration 50, loss = 0.42957898\n",
      "Iteration 1, loss = 0.49695200\n",
      "Iteration 2, loss = 0.44711358\n",
      "Iteration 3, loss = 0.44762647\n",
      "Iteration 4, loss = 0.44406995\n",
      "Iteration 5, loss = 0.44351874\n",
      "Iteration 6, loss = 0.44266989\n",
      "Iteration 7, loss = 0.44144492\n",
      "Iteration 8, loss = 0.44209381\n",
      "Iteration 9, loss = 0.44001832\n",
      "Iteration 10, loss = 0.44069524\n",
      "Iteration 11, loss = 0.43974039\n",
      "Iteration 12, loss = 0.43902557\n",
      "Iteration 13, loss = 0.43796987\n",
      "Iteration 14, loss = 0.43796241\n",
      "Iteration 15, loss = 0.43863780\n",
      "Iteration 16, loss = 0.43801948\n",
      "Iteration 17, loss = 0.43705797\n",
      "Iteration 18, loss = 0.43682417\n",
      "Iteration 19, loss = 0.43623534\n",
      "Iteration 20, loss = 0.43618581\n",
      "Iteration 21, loss = 0.43600132\n",
      "Iteration 22, loss = 0.43603543\n",
      "Iteration 23, loss = 0.43517546\n",
      "Iteration 24, loss = 0.43559312\n",
      "Iteration 25, loss = 0.43574195\n",
      "Iteration 26, loss = 0.43554726\n",
      "Iteration 27, loss = 0.43488769\n",
      "Iteration 28, loss = 0.43411310\n",
      "Iteration 29, loss = 0.43432789\n",
      "Iteration 30, loss = 0.43425122\n",
      "Iteration 31, loss = 0.43339969\n",
      "Iteration 32, loss = 0.43318268\n",
      "Iteration 33, loss = 0.43277426\n",
      "Iteration 34, loss = 0.43279982\n",
      "Iteration 35, loss = 0.43274427\n",
      "Iteration 36, loss = 0.43240007\n",
      "Iteration 37, loss = 0.43244078\n",
      "Iteration 38, loss = 0.43277519\n",
      "Iteration 39, loss = 0.43173601\n",
      "Iteration 40, loss = 0.43104928\n",
      "Iteration 41, loss = 0.43187966\n",
      "Iteration 42, loss = 0.43036768\n",
      "Iteration 43, loss = 0.43042464\n",
      "Iteration 44, loss = 0.43021338\n",
      "Iteration 45, loss = 0.43006790\n",
      "Iteration 46, loss = 0.43063758\n",
      "Iteration 47, loss = 0.42936039\n",
      "Iteration 48, loss = 0.42966553\n",
      "Iteration 49, loss = 0.43062784\n",
      "Iteration 50, loss = 0.42961688\n",
      "Iteration 1, loss = 0.49620734\n",
      "Iteration 2, loss = 0.44704111\n",
      "Iteration 3, loss = 0.44413777\n",
      "Iteration 4, loss = 0.44236248\n",
      "Iteration 5, loss = 0.44269873\n",
      "Iteration 6, loss = 0.44152067\n",
      "Iteration 7, loss = 0.43981945\n",
      "Iteration 8, loss = 0.44111984\n",
      "Iteration 9, loss = 0.43952133\n",
      "Iteration 10, loss = 0.43961064\n",
      "Iteration 11, loss = 0.43786769\n",
      "Iteration 12, loss = 0.43825935\n",
      "Iteration 13, loss = 0.43777000\n",
      "Iteration 14, loss = 0.43781966\n",
      "Iteration 15, loss = 0.43683175\n",
      "Iteration 16, loss = 0.43738617\n",
      "Iteration 17, loss = 0.43624304\n",
      "Iteration 18, loss = 0.43643129\n",
      "Iteration 19, loss = 0.43571241\n",
      "Iteration 20, loss = 0.43573059\n",
      "Iteration 21, loss = 0.43553667\n",
      "Iteration 22, loss = 0.43507476\n",
      "Iteration 23, loss = 0.43461221\n",
      "Iteration 24, loss = 0.43496352\n",
      "Iteration 25, loss = 0.43385570\n",
      "Iteration 26, loss = 0.43429521\n",
      "Iteration 27, loss = 0.43341563\n",
      "Iteration 28, loss = 0.43409998\n",
      "Iteration 29, loss = 0.43313658\n",
      "Iteration 30, loss = 0.43244188\n",
      "Iteration 31, loss = 0.43210641\n",
      "Iteration 32, loss = 0.43266287\n",
      "Iteration 33, loss = 0.43264722\n",
      "Iteration 34, loss = 0.43243573\n",
      "Iteration 35, loss = 0.43304500\n",
      "Iteration 36, loss = 0.43128587\n",
      "Iteration 37, loss = 0.43252821\n",
      "Iteration 38, loss = 0.43088931\n",
      "Iteration 39, loss = 0.43226940\n",
      "Iteration 40, loss = 0.43126113\n",
      "Iteration 41, loss = 0.43049858\n",
      "Iteration 42, loss = 0.43074030\n",
      "Iteration 43, loss = 0.43010768\n",
      "Iteration 44, loss = 0.43015552\n",
      "Iteration 45, loss = 0.42901400\n",
      "Iteration 46, loss = 0.42853478\n",
      "Iteration 47, loss = 0.42918419\n",
      "Iteration 48, loss = 0.42952951\n",
      "Iteration 49, loss = 0.42908482\n",
      "Iteration 50, loss = 0.42880317\n",
      "Iteration 1, loss = 0.49759240\n",
      "Iteration 2, loss = 0.44676916\n",
      "Iteration 3, loss = 0.44457201\n",
      "Iteration 4, loss = 0.44311543\n",
      "Iteration 5, loss = 0.44132004\n",
      "Iteration 6, loss = 0.44179349\n",
      "Iteration 7, loss = 0.44125408\n",
      "Iteration 8, loss = 0.44078492\n",
      "Iteration 9, loss = 0.43940369\n",
      "Iteration 10, loss = 0.43888963\n",
      "Iteration 11, loss = 0.43814463\n",
      "Iteration 12, loss = 0.43838144\n",
      "Iteration 13, loss = 0.43762521\n",
      "Iteration 14, loss = 0.43680817\n",
      "Iteration 15, loss = 0.43709231\n",
      "Iteration 16, loss = 0.43724461\n",
      "Iteration 17, loss = 0.43612140\n",
      "Iteration 18, loss = 0.43665904\n",
      "Iteration 19, loss = 0.43629971\n",
      "Iteration 20, loss = 0.43493655\n",
      "Iteration 21, loss = 0.43471763\n",
      "Iteration 22, loss = 0.43524534\n",
      "Iteration 23, loss = 0.43486530\n",
      "Iteration 24, loss = 0.43441630\n",
      "Iteration 25, loss = 0.43462099\n",
      "Iteration 26, loss = 0.43411207\n",
      "Iteration 27, loss = 0.43349304\n",
      "Iteration 28, loss = 0.43371823\n",
      "Iteration 29, loss = 0.43380448\n",
      "Iteration 30, loss = 0.43339375\n",
      "Iteration 31, loss = 0.43299523\n",
      "Iteration 32, loss = 0.43298606\n",
      "Iteration 33, loss = 0.43255278\n",
      "Iteration 34, loss = 0.43277965\n",
      "Iteration 35, loss = 0.43213414\n",
      "Iteration 36, loss = 0.43158125\n",
      "Iteration 37, loss = 0.43105717\n",
      "Iteration 38, loss = 0.43175699\n",
      "Iteration 39, loss = 0.43166207\n",
      "Iteration 40, loss = 0.43137600\n",
      "Iteration 41, loss = 0.43063440\n",
      "Iteration 42, loss = 0.43081387\n",
      "Iteration 43, loss = 0.43008231\n",
      "Iteration 44, loss = 0.43056496\n",
      "Iteration 45, loss = 0.42973008\n",
      "Iteration 46, loss = 0.42970964\n",
      "Iteration 47, loss = 0.43008606\n",
      "Iteration 48, loss = 0.42963863\n",
      "Iteration 49, loss = 0.42956797\n",
      "Iteration 50, loss = 0.42958989\n",
      "Iteration 1, loss = 0.49516400\n",
      "Iteration 2, loss = 0.44570949\n",
      "Iteration 3, loss = 0.44380859\n",
      "Iteration 4, loss = 0.44285966\n",
      "Iteration 5, loss = 0.44214602\n",
      "Iteration 6, loss = 0.44049236\n",
      "Iteration 7, loss = 0.44028169\n",
      "Iteration 8, loss = 0.43982361\n",
      "Iteration 9, loss = 0.43871303\n",
      "Iteration 10, loss = 0.43864313\n",
      "Iteration 11, loss = 0.43812373\n",
      "Iteration 12, loss = 0.43844101\n",
      "Iteration 13, loss = 0.43734193\n",
      "Iteration 14, loss = 0.43714022\n",
      "Iteration 15, loss = 0.43620026\n",
      "Iteration 16, loss = 0.43784259\n",
      "Iteration 17, loss = 0.43663849\n",
      "Iteration 18, loss = 0.43603377\n",
      "Iteration 19, loss = 0.43528567\n",
      "Iteration 20, loss = 0.43492095\n",
      "Iteration 21, loss = 0.43470092\n",
      "Iteration 22, loss = 0.43445887\n",
      "Iteration 23, loss = 0.43357887\n",
      "Iteration 24, loss = 0.43446651\n",
      "Iteration 25, loss = 0.43355496\n",
      "Iteration 26, loss = 0.43304382\n",
      "Iteration 27, loss = 0.43360043\n",
      "Iteration 28, loss = 0.43285310\n",
      "Iteration 29, loss = 0.43294034\n",
      "Iteration 30, loss = 0.43272247\n",
      "Iteration 31, loss = 0.43185223\n",
      "Iteration 32, loss = 0.43206836\n",
      "Iteration 33, loss = 0.43171107\n",
      "Iteration 34, loss = 0.43127546\n",
      "Iteration 35, loss = 0.43186264\n",
      "Iteration 36, loss = 0.43095828\n",
      "Iteration 37, loss = 0.43137517\n",
      "Iteration 38, loss = 0.43058202\n",
      "Iteration 39, loss = 0.43032751\n",
      "Iteration 40, loss = 0.43067056\n",
      "Iteration 41, loss = 0.43072859\n",
      "Iteration 42, loss = 0.42986465\n",
      "Iteration 43, loss = 0.43022049\n",
      "Iteration 44, loss = 0.42974439\n",
      "Iteration 45, loss = 0.42864347\n",
      "Iteration 46, loss = 0.42973694\n",
      "Iteration 47, loss = 0.42833100\n",
      "Iteration 48, loss = 0.42924288\n",
      "Iteration 49, loss = 0.42810102\n",
      "Iteration 50, loss = 0.42898207\n",
      "Iteration 1, loss = 0.49726428\n",
      "Iteration 2, loss = 0.45115725\n",
      "Iteration 3, loss = 0.44664578\n",
      "Iteration 4, loss = 0.44614312\n",
      "Iteration 5, loss = 0.44542148\n",
      "Iteration 6, loss = 0.44335699\n",
      "Iteration 7, loss = 0.44383097\n",
      "Iteration 8, loss = 0.44265331\n",
      "Iteration 9, loss = 0.44239690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.44122292\n",
      "Iteration 11, loss = 0.44082125\n",
      "Iteration 12, loss = 0.44110812\n",
      "Iteration 13, loss = 0.44086249\n",
      "Iteration 14, loss = 0.44134714\n",
      "Iteration 15, loss = 0.43993859\n",
      "Iteration 16, loss = 0.44033096\n",
      "Iteration 17, loss = 0.43910167\n",
      "Iteration 18, loss = 0.43881803\n",
      "Iteration 19, loss = 0.43869944\n",
      "Iteration 20, loss = 0.43816253\n",
      "Iteration 21, loss = 0.43799275\n",
      "Iteration 22, loss = 0.43833780\n",
      "Iteration 23, loss = 0.43712271\n",
      "Iteration 24, loss = 0.43787028\n",
      "Iteration 25, loss = 0.43779120\n",
      "Iteration 26, loss = 0.43728343\n",
      "Iteration 27, loss = 0.43710842\n",
      "Iteration 28, loss = 0.43727701\n",
      "Iteration 29, loss = 0.43621989\n",
      "Iteration 30, loss = 0.43588112\n",
      "Iteration 31, loss = 0.43605992\n",
      "Iteration 32, loss = 0.43637043\n",
      "Iteration 33, loss = 0.43526269\n",
      "Iteration 34, loss = 0.43509756\n",
      "Iteration 35, loss = 0.43568532\n",
      "Iteration 36, loss = 0.43513845\n",
      "Iteration 37, loss = 0.43434490\n",
      "Iteration 38, loss = 0.43449925\n",
      "Iteration 39, loss = 0.43393917\n",
      "Iteration 40, loss = 0.43553882\n",
      "Iteration 41, loss = 0.43330277\n",
      "Iteration 42, loss = 0.43383973\n",
      "Iteration 43, loss = 0.43385184\n",
      "Iteration 44, loss = 0.43293511\n",
      "Iteration 45, loss = 0.43314039\n",
      "Iteration 46, loss = 0.43234147\n",
      "Iteration 47, loss = 0.43306715\n",
      "Iteration 48, loss = 0.43185861\n",
      "Iteration 49, loss = 0.43162149\n",
      "Iteration 50, loss = 0.43238666\n",
      "Iteration 1, loss = 0.49858606\n",
      "Iteration 2, loss = 0.44707526\n",
      "Iteration 3, loss = 0.44598480\n",
      "Iteration 4, loss = 0.44452896\n",
      "Iteration 5, loss = 0.44433554\n",
      "Iteration 6, loss = 0.44250537\n",
      "Iteration 7, loss = 0.44190913\n",
      "Iteration 8, loss = 0.44120591\n",
      "Iteration 9, loss = 0.44092187\n",
      "Iteration 10, loss = 0.43975409\n",
      "Iteration 11, loss = 0.44060403\n",
      "Iteration 12, loss = 0.43933511\n",
      "Iteration 13, loss = 0.43894430\n",
      "Iteration 14, loss = 0.43829269\n",
      "Iteration 15, loss = 0.43774236\n",
      "Iteration 16, loss = 0.43812023\n",
      "Iteration 17, loss = 0.43771789\n",
      "Iteration 18, loss = 0.43746144\n",
      "Iteration 19, loss = 0.43730435\n",
      "Iteration 20, loss = 0.43783949\n",
      "Iteration 21, loss = 0.43670470\n",
      "Iteration 22, loss = 0.43592979\n",
      "Iteration 23, loss = 0.43615671\n",
      "Iteration 24, loss = 0.43549426\n",
      "Iteration 25, loss = 0.43550663\n",
      "Iteration 26, loss = 0.43532555\n",
      "Iteration 27, loss = 0.43484195\n",
      "Iteration 28, loss = 0.43386666\n",
      "Iteration 29, loss = 0.43447245\n",
      "Iteration 30, loss = 0.43465237\n",
      "Iteration 31, loss = 0.43479237\n",
      "Iteration 32, loss = 0.43434272\n",
      "Iteration 33, loss = 0.43407981\n",
      "Iteration 34, loss = 0.43324326\n",
      "Iteration 35, loss = 0.43386416\n",
      "Iteration 36, loss = 0.43439182\n",
      "Iteration 37, loss = 0.43316617\n",
      "Iteration 38, loss = 0.43229352\n",
      "Iteration 39, loss = 0.43229858\n",
      "Iteration 40, loss = 0.43200411\n",
      "Iteration 41, loss = 0.43194093\n",
      "Iteration 42, loss = 0.43145852\n",
      "Iteration 43, loss = 0.43217671\n",
      "Iteration 44, loss = 0.43189944\n",
      "Iteration 45, loss = 0.43099766\n",
      "Iteration 46, loss = 0.43085648\n",
      "Iteration 47, loss = 0.43199012\n",
      "Iteration 48, loss = 0.43001379\n",
      "Iteration 49, loss = 0.43038402\n",
      "Iteration 50, loss = 0.42940892\n",
      "Iteration 1, loss = 0.49976939\n",
      "Iteration 2, loss = 0.44722764\n",
      "Iteration 3, loss = 0.44481492\n",
      "Iteration 4, loss = 0.44334895\n",
      "Iteration 5, loss = 0.44286035\n",
      "Iteration 6, loss = 0.44244944\n",
      "Iteration 7, loss = 0.44146677\n",
      "Iteration 8, loss = 0.43999062\n",
      "Iteration 9, loss = 0.43987443\n",
      "Iteration 10, loss = 0.43962597\n",
      "Iteration 11, loss = 0.44000850\n",
      "Iteration 12, loss = 0.43814348\n",
      "Iteration 13, loss = 0.43798114\n",
      "Iteration 14, loss = 0.43794629\n",
      "Iteration 15, loss = 0.43780144\n",
      "Iteration 16, loss = 0.43771004\n",
      "Iteration 17, loss = 0.43704908\n",
      "Iteration 18, loss = 0.43662131\n",
      "Iteration 19, loss = 0.43683526\n",
      "Iteration 20, loss = 0.43627055\n",
      "Iteration 21, loss = 0.43560715\n",
      "Iteration 22, loss = 0.43571503\n",
      "Iteration 23, loss = 0.43495762\n",
      "Iteration 24, loss = 0.43534246\n",
      "Iteration 25, loss = 0.43529439\n",
      "Iteration 26, loss = 0.43414480\n",
      "Iteration 27, loss = 0.43507294\n",
      "Iteration 28, loss = 0.43405261\n",
      "Iteration 29, loss = 0.43317371\n",
      "Iteration 30, loss = 0.43360673\n",
      "Iteration 31, loss = 0.43259456\n",
      "Iteration 32, loss = 0.43371329\n",
      "Iteration 33, loss = 0.43268953\n",
      "Iteration 34, loss = 0.43287215\n",
      "Iteration 35, loss = 0.43271430\n",
      "Iteration 36, loss = 0.43267625\n",
      "Iteration 37, loss = 0.43187777\n",
      "Iteration 38, loss = 0.43102126\n",
      "Iteration 39, loss = 0.43171112\n",
      "Iteration 40, loss = 0.43143141\n",
      "Iteration 41, loss = 0.43111015\n",
      "Iteration 42, loss = 0.43061210\n",
      "Iteration 43, loss = 0.42959570\n",
      "Iteration 44, loss = 0.43031573\n",
      "Iteration 45, loss = 0.43023209\n",
      "Iteration 46, loss = 0.42954040\n",
      "Iteration 47, loss = 0.42985436\n",
      "Iteration 48, loss = 0.42934787\n",
      "Iteration 49, loss = 0.42937610\n",
      "Iteration 50, loss = 0.42865010\n",
      "Iteration 1, loss = 0.49687507\n",
      "Iteration 2, loss = 0.44716399\n",
      "Iteration 3, loss = 0.44600606\n",
      "Iteration 4, loss = 0.44390921\n",
      "Iteration 5, loss = 0.44297386\n",
      "Iteration 6, loss = 0.44204302\n",
      "Iteration 7, loss = 0.44136770\n",
      "Iteration 8, loss = 0.44128500\n",
      "Iteration 9, loss = 0.44079339\n",
      "Iteration 10, loss = 0.43883043\n",
      "Iteration 11, loss = 0.44007709\n",
      "Iteration 12, loss = 0.43878924\n",
      "Iteration 13, loss = 0.43866309\n",
      "Iteration 14, loss = 0.43818701\n",
      "Iteration 15, loss = 0.43834153\n",
      "Iteration 16, loss = 0.43691093\n",
      "Iteration 17, loss = 0.43685854\n",
      "Iteration 18, loss = 0.43687844\n",
      "Iteration 19, loss = 0.43682204\n",
      "Iteration 20, loss = 0.43625437\n",
      "Iteration 21, loss = 0.43585076\n",
      "Iteration 22, loss = 0.43636578\n",
      "Iteration 23, loss = 0.43552784\n",
      "Iteration 24, loss = 0.43522648\n",
      "Iteration 25, loss = 0.43387988\n",
      "Iteration 26, loss = 0.43460970\n",
      "Iteration 27, loss = 0.43465344\n",
      "Iteration 28, loss = 0.43379192\n",
      "Iteration 29, loss = 0.43441861\n",
      "Iteration 30, loss = 0.43317022\n",
      "Iteration 31, loss = 0.43339583\n",
      "Iteration 32, loss = 0.43353111\n",
      "Iteration 33, loss = 0.43213742\n",
      "Iteration 34, loss = 0.43220922\n",
      "Iteration 35, loss = 0.43179902\n",
      "Iteration 36, loss = 0.43252738\n",
      "Iteration 37, loss = 0.43210668\n",
      "Iteration 38, loss = 0.43194210\n",
      "Iteration 39, loss = 0.43147947\n",
      "Iteration 40, loss = 0.43174853\n",
      "Iteration 41, loss = 0.43069510\n",
      "Iteration 42, loss = 0.43047380\n",
      "Iteration 43, loss = 0.43033472\n",
      "Iteration 44, loss = 0.42961024\n",
      "Iteration 45, loss = 0.42994637\n",
      "Iteration 46, loss = 0.42934047\n",
      "Iteration 47, loss = 0.42907890\n",
      "Iteration 48, loss = 0.42934261\n",
      "Iteration 49, loss = 0.42882660\n",
      "Iteration 50, loss = 0.42860558\n",
      "Iteration 1, loss = 0.49551301\n",
      "Iteration 2, loss = 0.44510589\n",
      "Iteration 3, loss = 0.44375679\n",
      "Iteration 4, loss = 0.44210843\n",
      "Iteration 5, loss = 0.44126897\n",
      "Iteration 6, loss = 0.44117817\n",
      "Iteration 7, loss = 0.44097680\n",
      "Iteration 8, loss = 0.43913627\n",
      "Iteration 9, loss = 0.43933975\n",
      "Iteration 10, loss = 0.43892860\n",
      "Iteration 11, loss = 0.43762296\n",
      "Iteration 12, loss = 0.43791933\n",
      "Iteration 13, loss = 0.43686669\n",
      "Iteration 14, loss = 0.43719109\n",
      "Iteration 15, loss = 0.43669006\n",
      "Iteration 16, loss = 0.43649751\n",
      "Iteration 17, loss = 0.43507857\n",
      "Iteration 18, loss = 0.43576201\n",
      "Iteration 19, loss = 0.43592038\n",
      "Iteration 20, loss = 0.43478809\n",
      "Iteration 21, loss = 0.43469069\n",
      "Iteration 22, loss = 0.43501536\n",
      "Iteration 23, loss = 0.43435241\n",
      "Iteration 24, loss = 0.43348647\n",
      "Iteration 25, loss = 0.43432775\n",
      "Iteration 26, loss = 0.43366008\n",
      "Iteration 27, loss = 0.43315274\n",
      "Iteration 28, loss = 0.43285114\n",
      "Iteration 29, loss = 0.43279328\n",
      "Iteration 30, loss = 0.43355276\n",
      "Iteration 31, loss = 0.43301363\n",
      "Iteration 32, loss = 0.43200136\n",
      "Iteration 33, loss = 0.43225065\n",
      "Iteration 34, loss = 0.43213816\n",
      "Iteration 35, loss = 0.43194602\n",
      "Iteration 36, loss = 0.43171289\n",
      "Iteration 37, loss = 0.43202185\n",
      "Iteration 38, loss = 0.43140955\n",
      "Iteration 39, loss = 0.43233475\n",
      "Iteration 40, loss = 0.43001358\n",
      "Iteration 41, loss = 0.43043949\n",
      "Iteration 42, loss = 0.43012644\n",
      "Iteration 43, loss = 0.43046588\n",
      "Iteration 44, loss = 0.42989112\n",
      "Iteration 45, loss = 0.43213258\n",
      "Iteration 46, loss = 0.42909745\n",
      "Iteration 47, loss = 0.42959147\n",
      "Iteration 48, loss = 0.42907996\n",
      "Iteration 49, loss = 0.42966278\n",
      "Iteration 50, loss = 0.42916750\n",
      "Iteration 1, loss = 0.50048882\n",
      "Iteration 2, loss = 0.44886953\n",
      "Iteration 3, loss = 0.44813032\n",
      "Iteration 4, loss = 0.44540149\n",
      "Iteration 5, loss = 0.44523622\n",
      "Iteration 6, loss = 0.44528105\n",
      "Iteration 7, loss = 0.44532119\n",
      "Iteration 8, loss = 0.44273993\n",
      "Iteration 9, loss = 0.44308937\n",
      "Iteration 10, loss = 0.44299918\n",
      "Iteration 11, loss = 0.44227457\n",
      "Iteration 12, loss = 0.44137980\n",
      "Iteration 13, loss = 0.44070738\n",
      "Iteration 14, loss = 0.44028237\n",
      "Iteration 15, loss = 0.44032404\n",
      "Iteration 16, loss = 0.43995274\n",
      "Iteration 17, loss = 0.43971622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.43992196\n",
      "Iteration 19, loss = 0.43829650\n",
      "Iteration 20, loss = 0.43895821\n",
      "Iteration 21, loss = 0.43877361\n",
      "Iteration 22, loss = 0.43867064\n",
      "Iteration 23, loss = 0.43827671\n",
      "Iteration 24, loss = 0.43794424\n",
      "Iteration 25, loss = 0.43758364\n",
      "Iteration 26, loss = 0.43730557\n",
      "Iteration 27, loss = 0.43714960\n",
      "Iteration 28, loss = 0.43622740\n",
      "Iteration 29, loss = 0.43666130\n",
      "Iteration 30, loss = 0.43518409\n",
      "Iteration 31, loss = 0.43566912\n",
      "Iteration 32, loss = 0.43519630\n",
      "Iteration 33, loss = 0.43529160\n",
      "Iteration 34, loss = 0.43558096\n",
      "Iteration 35, loss = 0.43518911\n",
      "Iteration 36, loss = 0.43528817\n",
      "Iteration 37, loss = 0.43383092\n",
      "Iteration 38, loss = 0.43474842\n",
      "Iteration 39, loss = 0.43390644\n",
      "Iteration 40, loss = 0.43408570\n",
      "Iteration 41, loss = 0.43403171\n",
      "Iteration 42, loss = 0.43447915\n",
      "Iteration 43, loss = 0.43294673\n",
      "Iteration 44, loss = 0.43323470\n",
      "Iteration 45, loss = 0.43282730\n",
      "Iteration 46, loss = 0.43239222\n",
      "Iteration 47, loss = 0.43218783\n",
      "Iteration 48, loss = 0.43188640\n",
      "Iteration 49, loss = 0.43163671\n",
      "Iteration 50, loss = 0.43200182\n",
      "Train Accuracy: 0.8174\n",
      "Running Memory: 0.02 MB\n",
      "Running Time: 63.83 Seconds\n",
      "Test  Accuracy: 0.8184\n",
      "Test  Presion:  0.7609\n",
      "Test  Recall:   0.9167\n",
      "Test  F1-score: 0.8316\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS2_tfidf_3, memory_MLP_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS2_tfidf_3, acc_MLP_DS2_tfidf_3, pre_MLP_DS2_tfidf_3, rec_MLP_DS2_tfidf_3, f1_MLP_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS2_tfidf_3, model_3, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7699\n",
      "Running Memory: 5.61 MB\n",
      "Running Time: 7.57 Seconds\n",
      "Test  Accuracy: 0.7696\n",
      "Test  Presion:  0.7416\n",
      "Test  Recall:   0.8118\n",
      "Test  F1-score: 0.7751\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS2_bow_1, memory_KNN_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS2_bow_1, acc_KNN_DS2_bow_1, pre_KNN_DS2_bow_1, rec_KNN_DS2_bow_1, f1_KNN_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS2_bow_1, model_4, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7909\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 6.52 Seconds\n",
      "Test  Accuracy: 0.7970\n",
      "Test  Presion:  0.7625\n",
      "Test  Recall:   0.8495\n",
      "Test  F1-score: 0.8037\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS2_bow_2, memory_KNN_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS2_bow_2, acc_KNN_DS2_bow_2, pre_KNN_DS2_bow_2, rec_KNN_DS2_bow_2, f1_KNN_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS2_bow_2, model_4, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7678\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 6.27 Seconds\n",
      "Test  Accuracy: 0.6066\n",
      "Test  Presion:  0.8054\n",
      "Test  Recall:   0.2578\n",
      "Test  F1-score: 0.3906\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS2_bow_3, memory_KNN_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS2_bow_3, acc_KNN_DS2_bow_3, pre_KNN_DS2_bow_3, rec_KNN_DS2_bow_3, f1_KNN_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS2_bow_3, model_4, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7555\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 7.58 Seconds\n",
      "Test  Accuracy: 0.7552\n",
      "Test  Presion:  0.7693\n",
      "Test  Recall:   0.7133\n",
      "Test  F1-score: 0.7403\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS2_tfidf_1, memory_KNN_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS2_tfidf_1, acc_KNN_DS2_tfidf_1, pre_KNN_DS2_tfidf_1, rec_KNN_DS2_tfidf_1, f1_KNN_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS2_tfidf_1, model_4, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7850\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 7.10 Seconds\n",
      "Test  Accuracy: 0.7901\n",
      "Test  Presion:  0.7608\n",
      "Test  Recall:   0.8323\n",
      "Test  F1-score: 0.7950\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS2_tfidf_2, memory_KNN_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS2_tfidf_2, acc_KNN_DS2_tfidf_2, pre_KNN_DS2_tfidf_2, rec_KNN_DS2_tfidf_2, f1_KNN_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS2_tfidf_2, model_4, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7617\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 6.55 Seconds\n",
      "Test  Accuracy: 0.6044\n",
      "Test  Presion:  0.7921\n",
      "Test  Recall:   0.2589\n",
      "Test  F1-score: 0.3902\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS2_tfidf_3, memory_KNN_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS2_tfidf_3, acc_KNN_DS2_tfidf_3, pre_KNN_DS2_tfidf_3, rec_KNN_DS2_tfidf_3, f1_KNN_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS2_tfidf_3, model_4, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8048\n",
      "Running Memory: 45.06 MB\n",
      "Running Time: 31.20 Seconds\n",
      "Test  Accuracy: 0.8043\n",
      "Test  Presion:  0.7720\n",
      "Test  Recall:   0.8511\n",
      "Test  F1-score: 0.8096\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS2_bow_1, memory_RF_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS2_bow_1, acc_RF_DS2_bow_1, pre_RF_DS2_bow_1, rec_RF_DS2_bow_1, f1_RF_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS2_bow_1, model_5, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8256\n",
      "Running Memory: 3.35 MB\n",
      "Running Time: 22.36 Seconds\n",
      "Test  Accuracy: 0.8114\n",
      "Test  Presion:  0.7786\n",
      "Test  Recall:   0.8584\n",
      "Test  F1-score: 0.8166\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS2_bow_2, memory_RF_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS2_bow_2, acc_RF_DS2_bow_2, pre_RF_DS2_bow_2, rec_RF_DS2_bow_2, f1_RF_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS2_bow_2, model_5, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8159\n",
      "Running Memory: 132.60 MB\n",
      "Running Time: 21.80 Seconds\n",
      "Test  Accuracy: 0.8203\n",
      "Test  Presion:  0.7663\n",
      "Test  Recall:   0.9099\n",
      "Test  F1-score: 0.8320\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS2_bow_3, memory_RF_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS2_bow_3, acc_RF_DS2_bow_3, pre_RF_DS2_bow_3, rec_RF_DS2_bow_3, f1_RF_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS2_bow_3, model_5, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8059\n",
      "Running Memory: 37.09 MB\n",
      "Running Time: 65.30 Seconds\n",
      "Test  Accuracy: 0.8034\n",
      "Test  Presion:  0.7710\n",
      "Test  Recall:   0.8506\n",
      "Test  F1-score: 0.8088\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS2_tfidf_1, memory_RF_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS2_tfidf_1, acc_RF_DS2_tfidf_1, pre_RF_DS2_tfidf_1, rec_RF_DS2_tfidf_1, f1_RF_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS2_tfidf_1, model_5, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8250\n",
      "Running Memory: 13.29 MB\n",
      "Running Time: 35.41 Seconds\n",
      "Test  Accuracy: 0.8195\n",
      "Test  Presion:  0.7826\n",
      "Test  Recall:   0.8735\n",
      "Test  F1-score: 0.8255\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS2_tfidf_2, memory_RF_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS2_tfidf_2, acc_RF_DS2_tfidf_2, pre_RF_DS2_tfidf_2, rec_RF_DS2_tfidf_2, f1_RF_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS2_tfidf_2, model_5, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8171\n",
      "Running Memory: 35.17 MB\n",
      "Running Time: 24.69 Seconds\n",
      "Test  Accuracy: 0.8196\n",
      "Test  Presion:  0.7659\n",
      "Test  Recall:   0.9088\n",
      "Test  F1-score: 0.8313\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS2_tfidf_3, memory_RF_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS2_tfidf_3, acc_RF_DS2_tfidf_3, pre_RF_DS2_tfidf_3, rec_RF_DS2_tfidf_3, f1_RF_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS2_tfidf_3, model_5, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier (RF,LR,KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7971\n",
      "Running Memory: 73.59 MB\n",
      "Running Time: 40.74 Seconds\n",
      "Test  Accuracy: 0.7977\n",
      "Test  Presion:  0.7516\n",
      "Test  Recall:   0.8757\n",
      "Test  F1-score: 0.8089\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS2_bow_1, memory_VOT_1_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS2_bow_1, acc_VOT_1_DS2_bow_1, pre_VOT_1_DS2_bow_1, rec_VOT_1_DS2_bow_1, f1_VOT_1_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS2_bow_1, model_6, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8261\n",
      "Running Memory: 11.11 MB\n",
      "Running Time: 28.88 Seconds\n",
      "Test  Accuracy: 0.8166\n",
      "Test  Presion:  0.7759\n",
      "Test  Recall:   0.8786\n",
      "Test  F1-score: 0.8241\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS2_bow_2, memory_VOT_1_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS2_bow_2, acc_VOT_1_DS2_bow_2, pre_VOT_1_DS2_bow_2, rec_VOT_1_DS2_bow_2, f1_VOT_1_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS2_bow_2, model_6, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8180\n",
      "Running Memory: 49.10 MB\n",
      "Running Time: 30.28 Seconds\n",
      "Test  Accuracy: 0.8203\n",
      "Test  Presion:  0.7664\n",
      "Test  Recall:   0.9097\n",
      "Test  F1-score: 0.8319\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS2_bow_3, memory_VOT_1_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS2_bow_3, acc_VOT_1_DS2_bow_3, pre_VOT_1_DS2_bow_3, rec_VOT_1_DS2_bow_3, f1_VOT_1_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS2_bow_3, model_6, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7952\n",
      "Running Memory: 21.43 MB\n",
      "Running Time: 62.76 Seconds\n",
      "Test  Accuracy: 0.7968\n",
      "Test  Presion:  0.7744\n",
      "Test  Recall:   0.8247\n",
      "Test  F1-score: 0.7987\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS2_tfidf_1, memory_VOT_1_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS2_tfidf_1, acc_VOT_1_DS2_tfidf_1, pre_VOT_1_DS2_tfidf_1, rec_VOT_1_DS2_tfidf_1, f1_VOT_1_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS2_tfidf_1, model_6, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8230\n",
      "Running Memory: 1.52 MB\n",
      "Running Time: 38.00 Seconds\n",
      "Test  Accuracy: 0.8155\n",
      "Test  Presion:  0.7836\n",
      "Test  Recall:   0.8603\n",
      "Test  F1-score: 0.8202\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS2_tfidf_2, memory_VOT_1_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS2_tfidf_2, acc_VOT_1_DS2_tfidf_2, pre_VOT_1_DS2_tfidf_2, rec_VOT_1_DS2_tfidf_2, f1_VOT_1_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS2_tfidf_2, model_6, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8163\n",
      "Running Memory: 47.18 MB\n",
      "Running Time: 33.67 Seconds\n",
      "Test  Accuracy: 0.8163\n",
      "Test  Presion:  0.7669\n",
      "Test  Recall:   0.8970\n",
      "Test  F1-score: 0.8268\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS2_tfidf_3, memory_VOT_1_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS2_tfidf_3, acc_VOT_1_DS2_tfidf_3, pre_VOT_1_DS2_tfidf_3, rec_VOT_1_DS2_tfidf_3, f1_VOT_1_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS2_tfidf_3, model_6, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier (LR, LSVM, CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7720\n",
      "Running Memory: 20.78 MB\n",
      "Running Time: 24.85 Seconds\n",
      "Test  Accuracy: 0.7746\n",
      "Test  Presion:  0.7267\n",
      "Test  Recall:   0.8641\n",
      "Test  F1-score: 0.7895\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS2_bow_1, memory_VOT_2_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS2_bow_1, acc_VOT_2_DS2_bow_1, pre_VOT_2_DS2_bow_1, rec_VOT_2_DS2_bow_1, f1_VOT_2_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS2_bow_1, model_7, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8207\n",
      "Running Memory: 23.79 MB\n",
      "Running Time: 11.26 Seconds\n",
      "Test  Accuracy: 0.8117\n",
      "Test  Presion:  0.7596\n",
      "Test  Recall:   0.8997\n",
      "Test  F1-score: 0.8237\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS2_bow_2, memory_VOT_2_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS2_bow_2, acc_VOT_2_DS2_bow_2, pre_VOT_2_DS2_bow_2, rec_VOT_2_DS2_bow_2, f1_VOT_2_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS2_bow_2, model_7, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8187\n",
      "Running Memory: 25.80 MB\n",
      "Running Time: 8.97 Seconds\n",
      "Test  Accuracy: 0.8174\n",
      "Test  Presion:  0.7624\n",
      "Test  Recall:   0.9102\n",
      "Test  F1-score: 0.8297\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS2_bow_3, memory_VOT_2_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS2_bow_3, acc_VOT_2_DS2_bow_3, pre_VOT_2_DS2_bow_3, rec_VOT_2_DS2_bow_3, f1_VOT_2_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS2_bow_3, model_7, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7760\n",
      "Running Memory: 24.24 MB\n",
      "Running Time: 8.88 Seconds\n",
      "Test  Accuracy: 0.7794\n",
      "Test  Presion:  0.7558\n",
      "Test  Recall:   0.8107\n",
      "Test  F1-score: 0.7823\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS2_tfidf_1, memory_VOT_2_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS2_tfidf_1, acc_VOT_2_DS2_tfidf_1, pre_VOT_2_DS2_tfidf_1, rec_VOT_2_DS2_tfidf_1, f1_VOT_2_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS2_tfidf_1, model_7, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8134\n",
      "Running Memory: 24.30 MB\n",
      "Running Time: 5.19 Seconds\n",
      "Test  Accuracy: 0.8054\n",
      "Test  Presion:  0.7822\n",
      "Test  Recall:   0.8341\n",
      "Test  F1-score: 0.8074\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS2_tfidf_2, memory_VOT_2_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS2_tfidf_2, acc_VOT_2_DS2_tfidf_2, pre_VOT_2_DS2_tfidf_2, rec_VOT_2_DS2_tfidf_2, f1_VOT_2_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS2_tfidf_2, model_7, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8112\n",
      "Running Memory: 24.22 MB\n",
      "Running Time: 4.02 Seconds\n",
      "Test  Accuracy: 0.8098\n",
      "Test  Presion:  0.7652\n",
      "Test  Recall:   0.8816\n",
      "Test  F1-score: 0.8193\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS2_tfidf_3, memory_VOT_2_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS2_tfidf_3, acc_VOT_2_DS2_tfidf_3, pre_VOT_2_DS2_tfidf_3, rec_VOT_2_DS2_tfidf_3, f1_VOT_2_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS2_tfidf_3, model_7, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging classifier (decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8052\n",
      "Running Memory: 24.34 MB\n",
      "Running Time: 146.37 Seconds\n",
      "Test  Accuracy: 0.8085\n",
      "Test  Presion:  0.7806\n",
      "Test  Recall:   0.8463\n",
      "Test  F1-score: 0.8121\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS2_bow_1, memory_BAGG_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS2_bow_1, acc_BAGG_DS2_bow_1, pre_BAGG_DS2_bow_1, rec_BAGG_DS2_bow_1, f1_BAGG_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS2_bow_1, model_8, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8181\n",
      "Running Memory: 1.17 MB\n",
      "Running Time: 77.40 Seconds\n",
      "Test  Accuracy: 0.8050\n",
      "Test  Presion:  0.7727\n",
      "Test  Recall:   0.8517\n",
      "Test  F1-score: 0.8103\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS2_bow_2, memory_BAGG_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS2_bow_2, acc_BAGG_DS2_bow_2, pre_BAGG_DS2_bow_2, rec_BAGG_DS2_bow_2, f1_BAGG_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS2_bow_2, model_8, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8137\n",
      "Running Memory: 32.38 MB\n",
      "Running Time: 62.50 Seconds\n",
      "Test  Accuracy: 0.8178\n",
      "Test  Presion:  0.7638\n",
      "Test  Recall:   0.9080\n",
      "Test  F1-score: 0.8297\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS2_bow_3, memory_BAGG_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS2_bow_3, acc_BAGG_DS2_bow_3, pre_BAGG_DS2_bow_3, rec_BAGG_DS2_bow_3, f1_BAGG_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS2_bow_3, model_8, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8027\n",
      "Running Memory: 22.63 MB\n",
      "Running Time: 425.30 Seconds\n",
      "Test  Accuracy: 0.8055\n",
      "Test  Presion:  0.7715\n",
      "Test  Recall:   0.8557\n",
      "Test  F1-score: 0.8114\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS2_tfidf_1, memory_BAGG_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS2_tfidf_1, acc_BAGG_DS2_tfidf_1, pre_BAGG_DS2_tfidf_1, rec_BAGG_DS2_tfidf_1, f1_BAGG_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS2_tfidf_1, model_8, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8217\n",
      "Running Memory: 2.11 MB\n",
      "Running Time: 190.56 Seconds\n",
      "Test  Accuracy: 0.8174\n",
      "Test  Presion:  0.7801\n",
      "Test  Recall:   0.8724\n",
      "Test  F1-score: 0.8237\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS2_tfidf_2, memory_BAGG_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS2_tfidf_2, acc_BAGG_DS2_tfidf_2, pre_BAGG_DS2_tfidf_2, rec_BAGG_DS2_tfidf_2, f1_BAGG_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS2_tfidf_2, model_8, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8161\n",
      "Running Memory: 3.77 MB\n",
      "Running Time: 81.18 Seconds\n",
      "Test  Accuracy: 0.8197\n",
      "Test  Presion:  0.7656\n",
      "Test  Recall:   0.9099\n",
      "Test  F1-score: 0.8315\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS2_tfidf_3, memory_BAGG_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS2_tfidf_3, acc_BAGG_DS2_tfidf_3, pre_BAGG_DS2_tfidf_3, rec_BAGG_DS2_tfidf_3, f1_BAGG_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS2_tfidf_3, model_8, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost (Boosting classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7829\n",
      "Running Memory: 0.36 MB\n",
      "Running Time: 13.68 Seconds\n",
      "Test  Accuracy: 0.7820\n",
      "Test  Presion:  0.7502\n",
      "Test  Recall:   0.8309\n",
      "Test  F1-score: 0.7885\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS2_bow_1, memory_ADA_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS2_bow_1, acc_ADA_DS2_bow_1, pre_ADA_DS2_bow_1, rec_ADA_DS2_bow_1, f1_ADA_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS2_bow_1, model_9, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8194\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 9.02 Seconds\n",
      "Test  Accuracy: 0.8141\n",
      "Test  Presion:  0.7656\n",
      "Test  Recall:   0.8932\n",
      "Test  F1-score: 0.8245\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS2_bow_2, memory_ADA_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS2_bow_2, acc_ADA_DS2_bow_2, pre_ADA_DS2_bow_2, rec_ADA_DS2_bow_2, f1_ADA_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS2_bow_2, model_9, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8170\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 7.99 Seconds\n",
      "Test  Accuracy: 0.8155\n",
      "Test  Presion:  0.7619\n",
      "Test  Recall:   0.9059\n",
      "Test  F1-score: 0.8276\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS2_bow_3, memory_ADA_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS2_bow_3, acc_ADA_DS2_bow_3, pre_ADA_DS2_bow_3, rec_ADA_DS2_bow_3, f1_ADA_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS2_bow_3, model_9, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7795\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 27.34 Seconds\n",
      "Test  Accuracy: 0.7774\n",
      "Test  Presion:  0.7558\n",
      "Test  Recall:   0.8047\n",
      "Test  F1-score: 0.7795\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS2_tfidf_1, memory_ADA_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS2_tfidf_1, acc_ADA_DS2_tfidf_1, pre_ADA_DS2_tfidf_1, rec_ADA_DS2_tfidf_1, f1_ADA_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS2_tfidf_1, model_9, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8200\n",
      "Running Memory: 20.59 MB\n",
      "Running Time: 10.91 Seconds\n",
      "Test  Accuracy: 0.8122\n",
      "Test  Presion:  0.7749\n",
      "Test  Recall:   0.8681\n",
      "Test  F1-score: 0.8189\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS2_tfidf_2, memory_ADA_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS2_tfidf_2, acc_ADA_DS2_tfidf_2, pre_ADA_DS2_tfidf_2, rec_ADA_DS2_tfidf_2, f1_ADA_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS2_tfidf_2, model_9, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8183\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 7.68 Seconds\n",
      "Test  Accuracy: 0.8183\n",
      "Test  Presion:  0.7651\n",
      "Test  Recall:   0.9067\n",
      "Test  F1-score: 0.8299\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS2_tfidf_3, memory_ADA_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS2_tfidf_3, acc_ADA_DS2_tfidf_3, pre_ADA_DS2_tfidf_3, rec_ADA_DS2_tfidf_3, f1_ADA_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS2_tfidf_3, model_9, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Boosting classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8054\n",
      "Running Memory: 16.74 MB\n",
      "Running Time: 15.79 Seconds\n",
      "Test  Accuracy: 0.8039\n",
      "Test  Presion:  0.7774\n",
      "Test  Recall:   0.8393\n",
      "Test  F1-score: 0.8072\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS2_bow_1, memory_XG_DS2_bow_1 = train(DS2_bow_1_x_train, DS2_bow_1_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS2_bow_1, acc_XG_DS2_bow_1, pre_XG_DS2_bow_1, rec_XG_DS2_bow_1, f1_XG_DS2_bow_1 = predict(DS2_bow_1_x_test, DS2_bow_1_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS2_bow_1, model_10, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8313\n",
      "Running Memory: 0.89 MB\n",
      "Running Time: 14.91 Seconds\n",
      "Test  Accuracy: 0.8209\n",
      "Test  Presion:  0.7820\n",
      "Test  Recall:   0.8786\n",
      "Test  F1-score: 0.8275\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS2_bow_2, memory_XG_DS2_bow_2 = train(DS2_bow_2_x_train, DS2_bow_2_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS2_bow_2, acc_XG_DS2_bow_2, pre_XG_DS2_bow_2, rec_XG_DS2_bow_2, f1_XG_DS2_bow_2 = predict(DS2_bow_2_x_test, DS2_bow_2_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS2_bow_2, model_10, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8193\n",
      "Running Memory: 0.40 MB\n",
      "Running Time: 13.55 Seconds\n",
      "Test  Accuracy: 0.8192\n",
      "Test  Presion:  0.7662\n",
      "Test  Recall:   0.9070\n",
      "Test  F1-score: 0.8307\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS2_bow_3, memory_XG_DS2_bow_3 = train(DS2_bow_3_x_train, DS2_bow_3_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS2_bow_3, acc_XG_DS2_bow_3, pre_XG_DS2_bow_3, rec_XG_DS2_bow_3, f1_XG_DS2_bow_3 = predict(DS2_bow_3_x_test, DS2_bow_3_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS2_bow_3, model_10, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7979\n",
      "Running Memory: 0.34 MB\n",
      "Running Time: 43.45 Seconds\n",
      "Test  Accuracy: 0.8021\n",
      "Test  Presion:  0.7785\n",
      "Test  Recall:   0.8320\n",
      "Test  F1-score: 0.8043\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS2_tfidf_1, memory_XG_DS2_tfidf_1 = train(DS2_tfidf_1_x_train, DS2_tfidf_1_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS2_tfidf_1, acc_XG_DS2_tfidf_1, pre_XG_DS2_tfidf_1, rec_XG_DS2_tfidf_1, f1_XG_DS2_tfidf_1 = predict(DS2_tfidf_1_x_test, DS2_tfidf_1_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS2_tfidf_1, model_10, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8286\n",
      "Running Memory: 0.22 MB\n",
      "Running Time: 21.19 Seconds\n",
      "Test  Accuracy: 0.8212\n",
      "Test  Presion:  0.7815\n",
      "Test  Recall:   0.8805\n",
      "Test  F1-score: 0.8280\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS2_tfidf_2, memory_XG_DS2_tfidf_2 = train(DS2_tfidf_2_x_train, DS2_tfidf_2_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS2_tfidf_2, acc_XG_DS2_tfidf_2, pre_XG_DS2_tfidf_2, rec_XG_DS2_tfidf_2, f1_XG_DS2_tfidf_2 = predict(DS2_tfidf_2_x_test, DS2_tfidf_2_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS2_tfidf_2, model_10, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8185\n",
      "Running Memory: 0.32 MB\n",
      "Running Time: 14.56 Seconds\n",
      "Test  Accuracy: 0.8209\n",
      "Test  Presion:  0.7669\n",
      "Test  Recall:   0.9105\n",
      "Test  F1-score: 0.8326\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS2_tfidf_3, memory_XG_DS2_tfidf_3 = train(DS2_tfidf_3_x_train, DS2_tfidf_3_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS2_tfidf_3, acc_XG_DS2_tfidf_3, pre_XG_DS2_tfidf_3, rec_XG_DS2_tfidf_3, f1_XG_DS2_tfidf_3 = predict(DS2_tfidf_3_x_test, DS2_tfidf_3_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS2_tfidf_3, model_10, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8649\n",
      "Running Memory: 7.69 MB\n",
      "Running Time: 0.73 Seconds\n",
      "Test  Accuracy: 0.8588\n",
      "Test  Presion:  0.8305\n",
      "Test  Recall:   0.9251\n",
      "Test  F1-score: 0.8753\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS3_bow_1, memory_LR_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS3_bow_1, acc_LR_DS3_bow_1, pre_LR_DS3_bow_1, rec_LR_DS3_bow_1, f1_LR_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS3_bow_1, model_1, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8201\n",
      "Running Memory: 3.84 MB\n",
      "Running Time: 1.00 Seconds\n",
      "Test  Accuracy: 0.8154\n",
      "Test  Presion:  0.7800\n",
      "Test  Recall:   0.9126\n",
      "Test  F1-score: 0.8411\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS3_bow_2, memory_LR_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS3_bow_2, acc_LR_DS3_bow_2, pre_LR_DS3_bow_2, rec_LR_DS3_bow_2, f1_LR_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS3_bow_2, model_1, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 2.30 MB\n",
      "Running Time: 0.42 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS3_bow_3, memory_LR_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS3_bow_3, acc_LR_DS3_bow_3, pre_LR_DS3_bow_3, rec_LR_DS3_bow_3, f1_LR_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS3_bow_3, model_1, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8402\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.35 Seconds\n",
      "Test  Accuracy: 0.8538\n",
      "Test  Presion:  0.8687\n",
      "Test  Recall:   0.8565\n",
      "Test  F1-score: 0.8625\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS3_tfidf_1, memory_LR_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS3_tfidf_1, acc_LR_DS3_tfidf_1, pre_LR_DS3_tfidf_1, rec_LR_DS3_tfidf_1, f1_LR_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS3_tfidf_1, model_1, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8155\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.55 Seconds\n",
      "Test  Accuracy: 0.8062\n",
      "Test  Presion:  0.7752\n",
      "Test  Recall:   0.8986\n",
      "Test  F1-score: 0.8324\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS3_tfidf_2, memory_LR_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS3_tfidf_2, acc_LR_DS3_tfidf_2, pre_LR_DS3_tfidf_2, rec_LR_DS3_tfidf_2, f1_LR_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS3_tfidf_2, model_1, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.44 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_LR_DS3_tfidf_3, memory_LR_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_LR, SKF)\n",
    "cm_LR_DS3_tfidf_3, acc_LR_DS3_tfidf_3, pre_LR_DS3_tfidf_3, rec_LR_DS3_tfidf_3, f1_LR_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_LR)\n",
    "#show_confusion_maxtrix(cm_LR_DS3_tfidf_3, model_1, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8606\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 2.51 Seconds\n",
      "Test  Accuracy: 0.8613\n",
      "Test  Presion:  0.8276\n",
      "Test  Recall:   0.9360\n",
      "Test  F1-score: 0.8785\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS3_bow_1, memory_LSVM_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS3_bow_1, acc_LSVM_DS3_bow_1, pre_LSVM_DS3_bow_1, rec_LSVM_DS3_bow_1, f1_LSVM_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS3_bow_1, model_2, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8241\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 1.17 Seconds\n",
      "Test  Accuracy: 0.8212\n",
      "Test  Presion:  0.7806\n",
      "Test  Recall:   0.9267\n",
      "Test  F1-score: 0.8474\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS3_bow_2, memory_LSVM_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS3_bow_2, acc_LSVM_DS3_bow_2, pre_LSVM_DS3_bow_2, rec_LSVM_DS3_bow_2, f1_LSVM_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS3_bow_2, model_2, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 1.36 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS3_bow_3, memory_LSVM_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS3_bow_3, acc_LSVM_DS3_bow_3, pre_LSVM_DS3_bow_3, rec_LSVM_DS3_bow_3, f1_LSVM_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS3_bow_3, model_2, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8456\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.28 Seconds\n",
      "Test  Accuracy: 0.8580\n",
      "Test  Presion:  0.8768\n",
      "Test  Recall:   0.8549\n",
      "Test  F1-score: 0.8657\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS3_tfidf_1, memory_LSVM_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS3_tfidf_1, acc_LSVM_DS3_tfidf_1, pre_LSVM_DS3_tfidf_1, rec_LSVM_DS3_tfidf_1, f1_LSVM_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS3_tfidf_1, model_2, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8158\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.16 Seconds\n",
      "Test  Accuracy: 0.8087\n",
      "Test  Presion:  0.7791\n",
      "Test  Recall:   0.8970\n",
      "Test  F1-score: 0.8339\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS3_tfidf_2, memory_LSVM_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS3_tfidf_2, acc_LSVM_DS3_tfidf_2, pre_LSVM_DS3_tfidf_2, rec_LSVM_DS3_tfidf_2, f1_LSVM_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS3_tfidf_2, model_2, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.16 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_LSVM_DS3_tfidf_3, memory_LSVM_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_LSVM, SKF)\n",
    "cm_LSVM_DS3_tfidf_3, acc_LSVM_DS3_tfidf_3, pre_LSVM_DS3_tfidf_3, rec_LSVM_DS3_tfidf_3, f1_LSVM_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_LSVM)\n",
    "#show_confusion_maxtrix(cm_LSVM_DS3_tfidf_3, model_2, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.48054249\n",
      "Iteration 2, loss = 0.33315784\n",
      "Iteration 3, loss = 0.27561366\n",
      "Iteration 4, loss = 0.23453244\n",
      "Iteration 5, loss = 0.20428828\n",
      "Iteration 6, loss = 0.18288713\n",
      "Iteration 7, loss = 0.16727051\n",
      "Iteration 8, loss = 0.14614698\n",
      "Iteration 9, loss = 0.13230219\n",
      "Iteration 10, loss = 0.13891616\n",
      "Iteration 11, loss = 0.11623386\n",
      "Iteration 12, loss = 0.10176613\n",
      "Iteration 13, loss = 0.09459562\n",
      "Iteration 14, loss = 0.08572210\n",
      "Iteration 15, loss = 0.09380997\n",
      "Iteration 16, loss = 0.08155503\n",
      "Iteration 17, loss = 0.07391177\n",
      "Iteration 18, loss = 0.06313237\n",
      "Iteration 19, loss = 0.06092612\n",
      "Iteration 20, loss = 0.05608222\n",
      "Iteration 21, loss = 0.05227576\n",
      "Iteration 22, loss = 0.05289074\n",
      "Iteration 23, loss = 0.04960738\n",
      "Iteration 24, loss = 0.06279272\n",
      "Iteration 25, loss = 0.11537401\n",
      "Iteration 26, loss = 0.15206615\n",
      "Iteration 27, loss = 0.11216121\n",
      "Iteration 28, loss = 0.07476260\n",
      "Iteration 29, loss = 0.05674577\n",
      "Iteration 30, loss = 0.04679569\n",
      "Iteration 31, loss = 0.04159652\n",
      "Iteration 32, loss = 0.03969998\n",
      "Iteration 33, loss = 0.03570643\n",
      "Iteration 34, loss = 0.03633623\n",
      "Iteration 35, loss = 0.03364400\n",
      "Iteration 36, loss = 0.03153690\n",
      "Iteration 37, loss = 0.03154600\n",
      "Iteration 38, loss = 0.02939958\n",
      "Iteration 39, loss = 0.03140763\n",
      "Iteration 40, loss = 0.03098242\n",
      "Iteration 41, loss = 0.03064859\n",
      "Iteration 42, loss = 0.02914214\n",
      "Iteration 43, loss = 0.02868106\n",
      "Iteration 44, loss = 0.02843994\n",
      "Iteration 45, loss = 0.02782224\n",
      "Iteration 46, loss = 0.02696794\n",
      "Iteration 47, loss = 0.02825046\n",
      "Iteration 48, loss = 0.02638064\n",
      "Iteration 49, loss = 0.02675744\n",
      "Iteration 50, loss = 0.02616693\n",
      "Iteration 1, loss = 0.45506563\n",
      "Iteration 2, loss = 0.32967693\n",
      "Iteration 3, loss = 0.28647372\n",
      "Iteration 4, loss = 0.24029253\n",
      "Iteration 5, loss = 0.21728644\n",
      "Iteration 6, loss = 0.18499217\n",
      "Iteration 7, loss = 0.16638590\n",
      "Iteration 8, loss = 0.15321668\n",
      "Iteration 9, loss = 0.14646719\n",
      "Iteration 10, loss = 0.12271226\n",
      "Iteration 11, loss = 0.11238613\n",
      "Iteration 12, loss = 0.10881335\n",
      "Iteration 13, loss = 0.09566733\n",
      "Iteration 14, loss = 0.09846352\n",
      "Iteration 15, loss = 0.22773393\n",
      "Iteration 16, loss = 0.10885284\n",
      "Iteration 17, loss = 0.09761536\n",
      "Iteration 18, loss = 0.08078557\n",
      "Iteration 19, loss = 0.07040378\n",
      "Iteration 20, loss = 0.06199337\n",
      "Iteration 21, loss = 0.05820754\n",
      "Iteration 22, loss = 0.05422306\n",
      "Iteration 23, loss = 0.05083692\n",
      "Iteration 24, loss = 0.04878086\n",
      "Iteration 25, loss = 0.04628416\n",
      "Iteration 26, loss = 0.04474185\n",
      "Iteration 27, loss = 0.05965099\n",
      "Iteration 28, loss = 0.05308077\n",
      "Iteration 29, loss = 0.04496359\n",
      "Iteration 30, loss = 0.04156659\n",
      "Iteration 31, loss = 0.03997743\n",
      "Iteration 32, loss = 0.03756917\n",
      "Iteration 33, loss = 0.03669003\n",
      "Iteration 34, loss = 0.03506250\n",
      "Iteration 35, loss = 0.03379216\n",
      "Iteration 36, loss = 0.03406248\n",
      "Iteration 37, loss = 0.03368191\n",
      "Iteration 38, loss = 0.03193263\n",
      "Iteration 39, loss = 0.03148199\n",
      "Iteration 40, loss = 0.03075235\n",
      "Iteration 41, loss = 0.03095863\n",
      "Iteration 42, loss = 0.02997514\n",
      "Iteration 43, loss = 0.02997394\n",
      "Iteration 44, loss = 0.03011897\n",
      "Iteration 45, loss = 0.02940712\n",
      "Iteration 46, loss = 0.02841601\n",
      "Iteration 47, loss = 0.02777822\n",
      "Iteration 48, loss = 0.02905572\n",
      "Iteration 49, loss = 0.02701250\n",
      "Iteration 50, loss = 0.02738672\n",
      "Iteration 1, loss = 0.45019620\n",
      "Iteration 2, loss = 0.33476220\n",
      "Iteration 3, loss = 0.28611579\n",
      "Iteration 4, loss = 0.24358456\n",
      "Iteration 5, loss = 0.20625626\n",
      "Iteration 6, loss = 0.18714952\n",
      "Iteration 7, loss = 0.16713745\n",
      "Iteration 8, loss = 0.15162059\n",
      "Iteration 9, loss = 0.14028556\n",
      "Iteration 10, loss = 0.12766784\n",
      "Iteration 11, loss = 0.11732834\n",
      "Iteration 12, loss = 0.10351400\n",
      "Iteration 13, loss = 0.10405704\n",
      "Iteration 14, loss = 0.09258755\n",
      "Iteration 15, loss = 0.08057905\n",
      "Iteration 16, loss = 0.07281073\n",
      "Iteration 17, loss = 0.06980431\n",
      "Iteration 18, loss = 0.06978304\n",
      "Iteration 19, loss = 0.10471993\n",
      "Iteration 20, loss = 0.12146640\n",
      "Iteration 21, loss = 0.09528550\n",
      "Iteration 22, loss = 0.08062112\n",
      "Iteration 23, loss = 0.06497454\n",
      "Iteration 24, loss = 0.06380918\n",
      "Iteration 25, loss = 0.05215010\n",
      "Iteration 26, loss = 0.04682275\n",
      "Iteration 27, loss = 0.04326764\n",
      "Iteration 28, loss = 0.03997773\n",
      "Iteration 29, loss = 0.03855974\n",
      "Iteration 30, loss = 0.03699742\n",
      "Iteration 31, loss = 0.03504720\n",
      "Iteration 32, loss = 0.03398154\n",
      "Iteration 33, loss = 0.03254468\n",
      "Iteration 34, loss = 0.03221623\n",
      "Iteration 35, loss = 0.03040998\n",
      "Iteration 36, loss = 0.03062298\n",
      "Iteration 37, loss = 0.02995060\n",
      "Iteration 38, loss = 0.02914840\n",
      "Iteration 39, loss = 0.03033082\n",
      "Iteration 40, loss = 0.02864276\n",
      "Iteration 41, loss = 0.02773424\n",
      "Iteration 42, loss = 0.02789195\n",
      "Iteration 43, loss = 0.02658052\n",
      "Iteration 44, loss = 0.02759458\n",
      "Iteration 45, loss = 0.02603503\n",
      "Iteration 46, loss = 0.02774070\n",
      "Iteration 47, loss = 0.02586297\n",
      "Iteration 48, loss = 0.02959248\n",
      "Iteration 49, loss = 0.02762623\n",
      "Iteration 50, loss = 0.02611034\n",
      "Iteration 1, loss = 0.46405927\n",
      "Iteration 2, loss = 0.32878875\n",
      "Iteration 3, loss = 0.27249114\n",
      "Iteration 4, loss = 0.24178431\n",
      "Iteration 5, loss = 0.21021362\n",
      "Iteration 6, loss = 0.20454070\n",
      "Iteration 7, loss = 0.17346302\n",
      "Iteration 8, loss = 0.15519821\n",
      "Iteration 9, loss = 0.13951500\n",
      "Iteration 10, loss = 0.12860625\n",
      "Iteration 11, loss = 0.13216285\n",
      "Iteration 12, loss = 0.11569024\n",
      "Iteration 13, loss = 0.10860886\n",
      "Iteration 14, loss = 0.10272625\n",
      "Iteration 15, loss = 0.09186193\n",
      "Iteration 16, loss = 0.08282650\n",
      "Iteration 17, loss = 0.07840739\n",
      "Iteration 18, loss = 0.07829038\n",
      "Iteration 19, loss = 0.07502548\n",
      "Iteration 20, loss = 0.07198925\n",
      "Iteration 21, loss = 0.06466073\n",
      "Iteration 22, loss = 0.05848695\n",
      "Iteration 23, loss = 0.08049154\n",
      "Iteration 24, loss = 0.07691822\n",
      "Iteration 25, loss = 0.07799447\n",
      "Iteration 26, loss = 0.05950410\n",
      "Iteration 27, loss = 0.05643550\n",
      "Iteration 28, loss = 0.05327292\n",
      "Iteration 29, loss = 0.05310710\n",
      "Iteration 30, loss = 0.04451761\n",
      "Iteration 31, loss = 0.04161877\n",
      "Iteration 32, loss = 0.03710638\n",
      "Iteration 33, loss = 0.03688593\n",
      "Iteration 34, loss = 0.03354291\n",
      "Iteration 35, loss = 0.03378350\n",
      "Iteration 36, loss = 0.03157761\n",
      "Iteration 37, loss = 0.03217332\n",
      "Iteration 38, loss = 0.03040451\n",
      "Iteration 39, loss = 0.02961005\n",
      "Iteration 40, loss = 0.02982172\n",
      "Iteration 41, loss = 0.02895277\n",
      "Iteration 42, loss = 0.02875967\n",
      "Iteration 43, loss = 0.02868811\n",
      "Iteration 44, loss = 0.02854729\n",
      "Iteration 45, loss = 0.02691750\n",
      "Iteration 46, loss = 0.02610578\n",
      "Iteration 47, loss = 0.02642686\n",
      "Iteration 48, loss = 0.02690786\n",
      "Iteration 49, loss = 0.02604574\n",
      "Iteration 50, loss = 0.03041581\n",
      "Iteration 1, loss = 0.47098300\n",
      "Iteration 2, loss = 0.32757331\n",
      "Iteration 3, loss = 0.27917997\n",
      "Iteration 4, loss = 0.23795558\n",
      "Iteration 5, loss = 0.20986246\n",
      "Iteration 6, loss = 0.18989291\n",
      "Iteration 7, loss = 0.16938067\n",
      "Iteration 8, loss = 0.15141254\n",
      "Iteration 9, loss = 0.13929678\n",
      "Iteration 10, loss = 0.13904320\n",
      "Iteration 11, loss = 0.12637856\n",
      "Iteration 12, loss = 0.10852258\n",
      "Iteration 13, loss = 0.09722911\n",
      "Iteration 14, loss = 0.09196194\n",
      "Iteration 15, loss = 0.08372882\n",
      "Iteration 16, loss = 0.09133258\n",
      "Iteration 17, loss = 0.21271442\n",
      "Iteration 18, loss = 0.13555402\n",
      "Iteration 19, loss = 0.09757483\n",
      "Iteration 20, loss = 0.08335268\n",
      "Iteration 21, loss = 0.07289881\n",
      "Iteration 22, loss = 0.06767356\n",
      "Iteration 23, loss = 0.06055612\n",
      "Iteration 24, loss = 0.05531760\n",
      "Iteration 25, loss = 0.05314228\n",
      "Iteration 26, loss = 0.05243667\n",
      "Iteration 27, loss = 0.04846999\n",
      "Iteration 28, loss = 0.04523542\n",
      "Iteration 29, loss = 0.04201851\n",
      "Iteration 30, loss = 0.03985809\n",
      "Iteration 31, loss = 0.03942935\n",
      "Iteration 32, loss = 0.03865656\n",
      "Iteration 33, loss = 0.03663686\n",
      "Iteration 34, loss = 0.03552933\n",
      "Iteration 35, loss = 0.03454170\n",
      "Iteration 36, loss = 0.03522641\n",
      "Iteration 37, loss = 0.03324810\n",
      "Iteration 38, loss = 0.03345898\n",
      "Iteration 39, loss = 0.03150290\n",
      "Iteration 40, loss = 0.03029533\n",
      "Iteration 41, loss = 0.03025216\n",
      "Iteration 42, loss = 0.02976340\n",
      "Iteration 43, loss = 0.02918565\n",
      "Iteration 44, loss = 0.02884825\n",
      "Iteration 45, loss = 0.02867615\n",
      "Iteration 46, loss = 0.02821654\n",
      "Iteration 47, loss = 0.02810888\n",
      "Iteration 48, loss = 0.02871256\n",
      "Iteration 49, loss = 0.02711702\n",
      "Iteration 50, loss = 0.02762530\n",
      "Iteration 1, loss = 0.46829060\n",
      "Iteration 2, loss = 0.33778848\n",
      "Iteration 3, loss = 0.27392925\n",
      "Iteration 4, loss = 0.22832180\n",
      "Iteration 5, loss = 0.20662949\n",
      "Iteration 6, loss = 0.17698281\n",
      "Iteration 7, loss = 0.16674091\n",
      "Iteration 8, loss = 0.16176388\n",
      "Iteration 9, loss = 0.14029617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, loss = 0.12161112\n",
      "Iteration 11, loss = 0.11133450\n",
      "Iteration 12, loss = 0.09562855\n",
      "Iteration 13, loss = 0.08914542\n",
      "Iteration 14, loss = 0.08526060\n",
      "Iteration 15, loss = 0.07885618\n",
      "Iteration 16, loss = 0.07771122\n",
      "Iteration 17, loss = 0.07028222\n",
      "Iteration 18, loss = 0.06669246\n",
      "Iteration 19, loss = 0.06053760\n",
      "Iteration 20, loss = 0.05466597\n",
      "Iteration 21, loss = 0.05060091\n",
      "Iteration 22, loss = 0.04751742\n",
      "Iteration 23, loss = 0.04632085\n",
      "Iteration 24, loss = 0.04070997\n",
      "Iteration 25, loss = 0.04056862\n",
      "Iteration 26, loss = 0.04218945\n",
      "Iteration 27, loss = 0.03988484\n",
      "Iteration 28, loss = 0.03975111\n",
      "Iteration 29, loss = 0.11364907\n",
      "Iteration 30, loss = 0.11229295\n",
      "Iteration 31, loss = 0.07300537\n",
      "Iteration 32, loss = 0.07919860\n",
      "Iteration 33, loss = 0.07942276\n",
      "Iteration 34, loss = 0.22730464\n",
      "Iteration 35, loss = 0.10159538\n",
      "Iteration 36, loss = 0.06901443\n",
      "Iteration 37, loss = 0.05287453\n",
      "Iteration 38, loss = 0.04245886\n",
      "Iteration 39, loss = 0.03927708\n",
      "Iteration 40, loss = 0.03548528\n",
      "Iteration 41, loss = 0.03240869\n",
      "Iteration 42, loss = 0.03246663\n",
      "Iteration 43, loss = 0.02975776\n",
      "Iteration 44, loss = 0.03083750\n",
      "Iteration 45, loss = 0.02839407\n",
      "Iteration 46, loss = 0.02802012\n",
      "Iteration 47, loss = 0.02671566\n",
      "Iteration 48, loss = 0.02617958\n",
      "Iteration 49, loss = 0.02661153\n",
      "Iteration 50, loss = 0.02631490\n",
      "Iteration 1, loss = 0.47397577\n",
      "Iteration 2, loss = 0.32345215\n",
      "Iteration 3, loss = 0.27025962\n",
      "Iteration 4, loss = 0.23271272\n",
      "Iteration 5, loss = 0.20456343\n",
      "Iteration 6, loss = 0.18318509\n",
      "Iteration 7, loss = 0.16277771\n",
      "Iteration 8, loss = 0.14555902\n",
      "Iteration 9, loss = 0.13695682\n",
      "Iteration 10, loss = 0.12914816\n",
      "Iteration 11, loss = 0.11580008\n",
      "Iteration 12, loss = 0.10569533\n",
      "Iteration 13, loss = 0.11838051\n",
      "Iteration 14, loss = 0.13357504\n",
      "Iteration 15, loss = 0.11654610\n",
      "Iteration 16, loss = 0.09860740\n",
      "Iteration 17, loss = 0.08683315\n",
      "Iteration 18, loss = 0.07418937\n",
      "Iteration 19, loss = 0.07152457\n",
      "Iteration 20, loss = 0.06644876\n",
      "Iteration 21, loss = 0.06747081\n",
      "Iteration 22, loss = 0.06397065\n",
      "Iteration 23, loss = 0.07253596\n",
      "Iteration 24, loss = 0.06474538\n",
      "Iteration 25, loss = 0.05347946\n",
      "Iteration 26, loss = 0.05347099\n",
      "Iteration 27, loss = 0.04654818\n",
      "Iteration 28, loss = 0.04555328\n",
      "Iteration 29, loss = 0.05219427\n",
      "Iteration 30, loss = 0.05145003\n",
      "Iteration 31, loss = 0.04494014\n",
      "Iteration 32, loss = 0.04132764\n",
      "Iteration 33, loss = 0.03707349\n",
      "Iteration 34, loss = 0.03491193\n",
      "Iteration 35, loss = 0.03422801\n",
      "Iteration 36, loss = 0.03234327\n",
      "Iteration 37, loss = 0.03355716\n",
      "Iteration 38, loss = 0.02992866\n",
      "Iteration 39, loss = 0.03007932\n",
      "Iteration 40, loss = 0.02859295\n",
      "Iteration 41, loss = 0.02818650\n",
      "Iteration 42, loss = 0.02810583\n",
      "Iteration 43, loss = 0.02737789\n",
      "Iteration 44, loss = 0.02851148\n",
      "Iteration 45, loss = 0.02655210\n",
      "Iteration 46, loss = 0.02666898\n",
      "Iteration 47, loss = 0.02641284\n",
      "Iteration 48, loss = 0.02637383\n",
      "Iteration 49, loss = 0.02617598\n",
      "Iteration 50, loss = 0.02592679\n",
      "Iteration 1, loss = 0.46714793\n",
      "Iteration 2, loss = 0.32266356\n",
      "Iteration 3, loss = 0.27793976\n",
      "Iteration 4, loss = 0.23133974\n",
      "Iteration 5, loss = 0.21474308\n",
      "Iteration 6, loss = 0.18822919\n",
      "Iteration 7, loss = 0.16312268\n",
      "Iteration 8, loss = 0.15139584\n",
      "Iteration 9, loss = 0.13181535\n",
      "Iteration 10, loss = 0.11663724\n",
      "Iteration 11, loss = 0.10359407\n",
      "Iteration 12, loss = 0.10721799\n",
      "Iteration 13, loss = 0.10249947\n",
      "Iteration 14, loss = 0.10540033\n",
      "Iteration 15, loss = 0.09399871\n",
      "Iteration 16, loss = 0.08395723\n",
      "Iteration 17, loss = 0.08279542\n",
      "Iteration 18, loss = 0.10726672\n",
      "Iteration 19, loss = 0.08057033\n",
      "Iteration 20, loss = 0.06579525\n",
      "Iteration 21, loss = 0.05686976\n",
      "Iteration 22, loss = 0.05275694\n",
      "Iteration 23, loss = 0.04983143\n",
      "Iteration 24, loss = 0.04658875\n",
      "Iteration 25, loss = 0.04401898\n",
      "Iteration 26, loss = 0.04346315\n",
      "Iteration 27, loss = 0.04790348\n",
      "Iteration 28, loss = 0.04800376\n",
      "Iteration 29, loss = 0.04420733\n",
      "Iteration 30, loss = 0.04402374\n",
      "Iteration 31, loss = 0.03999810\n",
      "Iteration 32, loss = 0.04068651\n",
      "Iteration 33, loss = 0.03656188\n",
      "Iteration 34, loss = 0.03313712\n",
      "Iteration 35, loss = 0.03283565\n",
      "Iteration 36, loss = 0.03136623\n",
      "Iteration 37, loss = 0.03098635\n",
      "Iteration 38, loss = 0.03037759\n",
      "Iteration 39, loss = 0.02975550\n",
      "Iteration 40, loss = 0.02986946\n",
      "Iteration 41, loss = 0.02769715\n",
      "Iteration 42, loss = 0.03076164\n",
      "Iteration 43, loss = 0.02818804\n",
      "Iteration 44, loss = 0.02812787\n",
      "Iteration 45, loss = 0.02829123\n",
      "Iteration 46, loss = 0.02727114\n",
      "Iteration 47, loss = 0.02614610\n",
      "Iteration 48, loss = 0.02610776\n",
      "Iteration 49, loss = 0.02607419\n",
      "Iteration 50, loss = 0.02539011\n",
      "Iteration 1, loss = 0.47401408\n",
      "Iteration 2, loss = 0.34418422\n",
      "Iteration 3, loss = 0.28598267\n",
      "Iteration 4, loss = 0.24649903\n",
      "Iteration 5, loss = 0.22493319\n",
      "Iteration 6, loss = 0.19010637\n",
      "Iteration 7, loss = 0.18083532\n",
      "Iteration 8, loss = 0.17082145\n",
      "Iteration 9, loss = 0.14625281\n",
      "Iteration 10, loss = 0.13253446\n",
      "Iteration 11, loss = 0.12075496\n",
      "Iteration 12, loss = 0.11420266\n",
      "Iteration 13, loss = 0.10791554\n",
      "Iteration 14, loss = 0.12990347\n",
      "Iteration 15, loss = 0.14312854\n",
      "Iteration 16, loss = 0.10866480\n",
      "Iteration 17, loss = 0.09618316\n",
      "Iteration 18, loss = 0.08024576\n",
      "Iteration 19, loss = 0.06959935\n",
      "Iteration 20, loss = 0.06450461\n",
      "Iteration 21, loss = 0.05917640\n",
      "Iteration 22, loss = 0.05617494\n",
      "Iteration 23, loss = 0.05513220\n",
      "Iteration 24, loss = 0.05442000\n",
      "Iteration 25, loss = 0.05669886\n",
      "Iteration 26, loss = 0.05056749\n",
      "Iteration 27, loss = 0.04643376\n",
      "Iteration 28, loss = 0.04200643\n",
      "Iteration 29, loss = 0.04348304\n",
      "Iteration 30, loss = 0.04067222\n",
      "Iteration 31, loss = 0.03796734\n",
      "Iteration 32, loss = 0.03791612\n",
      "Iteration 33, loss = 0.03493428\n",
      "Iteration 34, loss = 0.03846660\n",
      "Iteration 35, loss = 0.03493270\n",
      "Iteration 36, loss = 0.03877496\n",
      "Iteration 37, loss = 0.03435744\n",
      "Iteration 38, loss = 0.03337800\n",
      "Iteration 39, loss = 0.03019252\n",
      "Iteration 40, loss = 0.03436642\n",
      "Iteration 41, loss = 0.03395958\n",
      "Iteration 42, loss = 0.03198901\n",
      "Iteration 43, loss = 0.03202866\n",
      "Iteration 44, loss = 0.03559397\n",
      "Iteration 45, loss = 0.68311527\n",
      "Iteration 46, loss = 0.23966192\n",
      "Iteration 47, loss = 0.17841073\n",
      "Iteration 48, loss = 0.14715783\n",
      "Iteration 49, loss = 0.12511968\n",
      "Iteration 50, loss = 0.10857780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.47413844\n",
      "Iteration 2, loss = 0.32783336\n",
      "Iteration 3, loss = 0.28187816\n",
      "Iteration 4, loss = 0.24943920\n",
      "Iteration 5, loss = 0.21480409\n",
      "Iteration 6, loss = 0.18738927\n",
      "Iteration 7, loss = 0.16591423\n",
      "Iteration 8, loss = 0.14958576\n",
      "Iteration 9, loss = 0.13954489\n",
      "Iteration 10, loss = 0.12537427\n",
      "Iteration 11, loss = 0.12400780\n",
      "Iteration 12, loss = 0.11082195\n",
      "Iteration 13, loss = 0.11422338\n",
      "Iteration 14, loss = 0.09896845\n",
      "Iteration 15, loss = 0.08949035\n",
      "Iteration 16, loss = 0.08255515\n",
      "Iteration 17, loss = 0.07326230\n",
      "Iteration 18, loss = 0.07109261\n",
      "Iteration 19, loss = 0.06965738\n",
      "Iteration 20, loss = 0.08061214\n",
      "Iteration 21, loss = 0.07505355\n",
      "Iteration 22, loss = 0.06135868\n",
      "Iteration 23, loss = 0.07682693\n",
      "Iteration 24, loss = 0.06940792\n",
      "Iteration 25, loss = 0.08756465\n",
      "Iteration 26, loss = 0.06479829\n",
      "Iteration 27, loss = 0.05465809\n",
      "Iteration 28, loss = 0.05563662\n",
      "Iteration 29, loss = 0.04525268\n",
      "Iteration 30, loss = 0.04365721\n",
      "Iteration 31, loss = 0.03926527\n",
      "Iteration 32, loss = 0.03652357\n",
      "Iteration 33, loss = 0.03404945\n",
      "Iteration 34, loss = 0.03289908\n",
      "Iteration 35, loss = 0.03155996\n",
      "Iteration 36, loss = 0.03218997\n",
      "Iteration 37, loss = 0.03049902\n",
      "Iteration 38, loss = 0.02957672\n",
      "Iteration 39, loss = 0.03019885\n",
      "Iteration 40, loss = 0.02922448\n",
      "Iteration 41, loss = 0.02779589\n",
      "Iteration 42, loss = 0.02846325\n",
      "Iteration 43, loss = 0.02706135\n",
      "Iteration 44, loss = 0.02746144\n",
      "Iteration 45, loss = 0.05400186\n",
      "Iteration 46, loss = 0.10710751\n",
      "Iteration 47, loss = 0.33037133\n",
      "Iteration 48, loss = 0.12044093\n",
      "Iteration 49, loss = 0.07610165\n",
      "Iteration 50, loss = 0.05622324\n",
      "Iteration 1, loss = 0.45892301\n",
      "Iteration 2, loss = 0.33199351\n",
      "Iteration 3, loss = 0.27841258\n",
      "Iteration 4, loss = 0.24348583\n",
      "Iteration 5, loss = 0.21512840\n",
      "Iteration 6, loss = 0.19280307\n",
      "Iteration 7, loss = 0.18234775\n",
      "Iteration 8, loss = 0.15513303\n",
      "Iteration 9, loss = 0.14241485\n",
      "Iteration 10, loss = 0.13115099\n",
      "Iteration 11, loss = 0.12627895\n",
      "Iteration 12, loss = 0.12238291\n",
      "Iteration 13, loss = 0.10946801\n",
      "Iteration 14, loss = 0.09912228\n",
      "Iteration 15, loss = 0.09636218\n",
      "Iteration 16, loss = 0.10507142\n",
      "Iteration 17, loss = 0.09727192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.07763382\n",
      "Iteration 19, loss = 0.07163932\n",
      "Iteration 20, loss = 0.06489977\n",
      "Iteration 21, loss = 0.05803021\n",
      "Iteration 22, loss = 0.05429309\n",
      "Iteration 23, loss = 0.04861165\n",
      "Iteration 24, loss = 0.04766416\n",
      "Iteration 25, loss = 0.04388626\n",
      "Iteration 26, loss = 0.04375777\n",
      "Iteration 27, loss = 0.04110633\n",
      "Iteration 28, loss = 0.03897007\n",
      "Iteration 29, loss = 0.03776802\n",
      "Iteration 30, loss = 0.03969934\n",
      "Iteration 31, loss = 0.26309573\n",
      "Iteration 32, loss = 0.53135749\n",
      "Iteration 33, loss = 0.21724410\n",
      "Iteration 34, loss = 0.17138036\n",
      "Iteration 35, loss = 0.13985927\n",
      "Iteration 36, loss = 0.11597129\n",
      "Iteration 37, loss = 0.09775574\n",
      "Iteration 38, loss = 0.08319754\n",
      "Iteration 39, loss = 0.07692338\n",
      "Iteration 40, loss = 0.08191127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Train Accuracy: 0.9219\n",
      "Running Memory: 0.54 MB\n",
      "Running Time: 22.10 Seconds\n",
      "Test  Accuracy: 0.9190\n",
      "Test  Presion:  0.9048\n",
      "Test  Recall:   0.9485\n",
      "Test  F1-score: 0.9261\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS3_bow_1, memory_MLP_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS3_bow_1, acc_MLP_DS3_bow_1, pre_MLP_DS3_bow_1, rec_MLP_DS3_bow_1, f1_MLP_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS3_bow_1, model_3, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55751947\n",
      "Iteration 2, loss = 0.36670952\n",
      "Iteration 3, loss = 0.33356255\n",
      "Iteration 4, loss = 0.32610865\n",
      "Iteration 5, loss = 0.31855848\n",
      "Iteration 6, loss = 0.31370918\n",
      "Iteration 7, loss = 0.30550541\n",
      "Iteration 8, loss = 0.30591050\n",
      "Iteration 9, loss = 0.30579914\n",
      "Iteration 10, loss = 0.30216289\n",
      "Iteration 11, loss = 0.30035579\n",
      "Iteration 12, loss = 0.29843654\n",
      "Iteration 13, loss = 0.29491216\n",
      "Iteration 14, loss = 0.29584113\n",
      "Iteration 15, loss = 0.29186067\n",
      "Iteration 16, loss = 0.29215005\n",
      "Iteration 17, loss = 0.29012951\n",
      "Iteration 18, loss = 0.28937619\n",
      "Iteration 19, loss = 0.28952692\n",
      "Iteration 20, loss = 0.28873833\n",
      "Iteration 21, loss = 0.29233196\n",
      "Iteration 22, loss = 0.29045509\n",
      "Iteration 23, loss = 0.28972321\n",
      "Iteration 24, loss = 0.28425651\n",
      "Iteration 25, loss = 0.28319925\n",
      "Iteration 26, loss = 0.28561057\n",
      "Iteration 27, loss = 0.28581939\n",
      "Iteration 28, loss = 0.28367764\n",
      "Iteration 29, loss = 0.28347608\n",
      "Iteration 30, loss = 0.27780931\n",
      "Iteration 31, loss = 0.28339947\n",
      "Iteration 32, loss = 0.28431836\n",
      "Iteration 33, loss = 0.28417908\n",
      "Iteration 34, loss = 0.28227332\n",
      "Iteration 35, loss = 0.27915253\n",
      "Iteration 36, loss = 0.27561112\n",
      "Iteration 37, loss = 0.28268391\n",
      "Iteration 38, loss = 0.27733450\n",
      "Iteration 39, loss = 0.27852490\n",
      "Iteration 40, loss = 0.27854400\n",
      "Iteration 41, loss = 0.27347586\n",
      "Iteration 42, loss = 0.27113982\n",
      "Iteration 43, loss = 0.28148978\n",
      "Iteration 44, loss = 0.27838721\n",
      "Iteration 45, loss = 0.27653344\n",
      "Iteration 46, loss = 0.27315334\n",
      "Iteration 47, loss = 0.27251127\n",
      "Iteration 48, loss = 0.27620364\n",
      "Iteration 49, loss = 0.27402434\n",
      "Iteration 50, loss = 0.27998068\n",
      "Iteration 1, loss = 0.56761420\n",
      "Iteration 2, loss = 0.37020307\n",
      "Iteration 3, loss = 0.34624115\n",
      "Iteration 4, loss = 0.32788087\n",
      "Iteration 5, loss = 0.31762347\n",
      "Iteration 6, loss = 0.31443365\n",
      "Iteration 7, loss = 0.31551506\n",
      "Iteration 8, loss = 0.30727734\n",
      "Iteration 9, loss = 0.31042670\n",
      "Iteration 10, loss = 0.30364952\n",
      "Iteration 11, loss = 0.30060285\n",
      "Iteration 12, loss = 0.29912726\n",
      "Iteration 13, loss = 0.29660720\n",
      "Iteration 14, loss = 0.29781396\n",
      "Iteration 15, loss = 0.29731171\n",
      "Iteration 16, loss = 0.29140875\n",
      "Iteration 17, loss = 0.29157972\n",
      "Iteration 18, loss = 0.29100415\n",
      "Iteration 19, loss = 0.28983787\n",
      "Iteration 20, loss = 0.29170797\n",
      "Iteration 21, loss = 0.29652378\n",
      "Iteration 22, loss = 0.28720072\n",
      "Iteration 23, loss = 0.28407454\n",
      "Iteration 24, loss = 0.28892857\n",
      "Iteration 25, loss = 0.29288854\n",
      "Iteration 26, loss = 0.28822644\n",
      "Iteration 27, loss = 0.28434298\n",
      "Iteration 28, loss = 0.28372465\n",
      "Iteration 29, loss = 0.28528582\n",
      "Iteration 30, loss = 0.28355145\n",
      "Iteration 31, loss = 0.28217969\n",
      "Iteration 32, loss = 0.28651470\n",
      "Iteration 33, loss = 0.27848742\n",
      "Iteration 34, loss = 0.27949125\n",
      "Iteration 35, loss = 0.28206832\n",
      "Iteration 36, loss = 0.27894974\n",
      "Iteration 37, loss = 0.27588373\n",
      "Iteration 38, loss = 0.27899298\n",
      "Iteration 39, loss = 0.28009968\n",
      "Iteration 40, loss = 0.28005692\n",
      "Iteration 41, loss = 0.28091429\n",
      "Iteration 42, loss = 0.28132759\n",
      "Iteration 43, loss = 0.27678332\n",
      "Iteration 44, loss = 0.27120364\n",
      "Iteration 45, loss = 0.27202571\n",
      "Iteration 46, loss = 0.27939244\n",
      "Iteration 47, loss = 0.27144110\n",
      "Iteration 48, loss = 0.27843037\n",
      "Iteration 49, loss = 0.27345937\n",
      "Iteration 50, loss = 0.27593885\n",
      "Iteration 1, loss = 0.56632247\n",
      "Iteration 2, loss = 0.36758045\n",
      "Iteration 3, loss = 0.33431980\n",
      "Iteration 4, loss = 0.31814499\n",
      "Iteration 5, loss = 0.31715133\n",
      "Iteration 6, loss = 0.31160904\n",
      "Iteration 7, loss = 0.30807353\n",
      "Iteration 8, loss = 0.30326580\n",
      "Iteration 9, loss = 0.29875851\n",
      "Iteration 10, loss = 0.29895023\n",
      "Iteration 11, loss = 0.30096436\n",
      "Iteration 12, loss = 0.30145919\n",
      "Iteration 13, loss = 0.29633246\n",
      "Iteration 14, loss = 0.29823937\n",
      "Iteration 15, loss = 0.29178415\n",
      "Iteration 16, loss = 0.29056266\n",
      "Iteration 17, loss = 0.29145914\n",
      "Iteration 18, loss = 0.29403076\n",
      "Iteration 19, loss = 0.28973302\n",
      "Iteration 20, loss = 0.29207727\n",
      "Iteration 21, loss = 0.28632340\n",
      "Iteration 22, loss = 0.28525573\n",
      "Iteration 23, loss = 0.28708064\n",
      "Iteration 24, loss = 0.28443020\n",
      "Iteration 25, loss = 0.28561850\n",
      "Iteration 26, loss = 0.28793462\n",
      "Iteration 27, loss = 0.28713401\n",
      "Iteration 28, loss = 0.28288482\n",
      "Iteration 29, loss = 0.27978993\n",
      "Iteration 30, loss = 0.27859795\n",
      "Iteration 31, loss = 0.27774716\n",
      "Iteration 32, loss = 0.28528868\n",
      "Iteration 33, loss = 0.27773398\n",
      "Iteration 34, loss = 0.28049077\n",
      "Iteration 35, loss = 0.27667909\n",
      "Iteration 36, loss = 0.28095766\n",
      "Iteration 37, loss = 0.28448921\n",
      "Iteration 38, loss = 0.27665203\n",
      "Iteration 39, loss = 0.27187016\n",
      "Iteration 40, loss = 0.27799212\n",
      "Iteration 41, loss = 0.27695164\n",
      "Iteration 42, loss = 0.27624198\n",
      "Iteration 43, loss = 0.27670491\n",
      "Iteration 44, loss = 0.26890429\n",
      "Iteration 45, loss = 0.27672965\n",
      "Iteration 46, loss = 0.27896703\n",
      "Iteration 47, loss = 0.27084299\n",
      "Iteration 48, loss = 0.27557427\n",
      "Iteration 49, loss = 0.27166871\n",
      "Iteration 50, loss = 0.27168421\n",
      "Iteration 1, loss = 0.56565219\n",
      "Iteration 2, loss = 0.37235816\n",
      "Iteration 3, loss = 0.33533566\n",
      "Iteration 4, loss = 0.32116955\n",
      "Iteration 5, loss = 0.31953941\n",
      "Iteration 6, loss = 0.31798480\n",
      "Iteration 7, loss = 0.30891709\n",
      "Iteration 8, loss = 0.30584704\n",
      "Iteration 9, loss = 0.30100604\n",
      "Iteration 10, loss = 0.30321119\n",
      "Iteration 11, loss = 0.30192448\n",
      "Iteration 12, loss = 0.30116298\n",
      "Iteration 13, loss = 0.29813167\n",
      "Iteration 14, loss = 0.29668406\n",
      "Iteration 15, loss = 0.29641130\n",
      "Iteration 16, loss = 0.29263247\n",
      "Iteration 17, loss = 0.29691429\n",
      "Iteration 18, loss = 0.29077112\n",
      "Iteration 19, loss = 0.29545262\n",
      "Iteration 20, loss = 0.29396693\n",
      "Iteration 21, loss = 0.29008705\n",
      "Iteration 22, loss = 0.28769817\n",
      "Iteration 23, loss = 0.29646753\n",
      "Iteration 24, loss = 0.28743340\n",
      "Iteration 25, loss = 0.28676711\n",
      "Iteration 26, loss = 0.28438352\n",
      "Iteration 27, loss = 0.28585926\n",
      "Iteration 28, loss = 0.28859315\n",
      "Iteration 29, loss = 0.28498755\n",
      "Iteration 30, loss = 0.28686505\n",
      "Iteration 31, loss = 0.28675688\n",
      "Iteration 32, loss = 0.28717984\n",
      "Iteration 33, loss = 0.28764678\n",
      "Iteration 34, loss = 0.28367433\n",
      "Iteration 35, loss = 0.28272013\n",
      "Iteration 36, loss = 0.28292568\n",
      "Iteration 37, loss = 0.28255983\n",
      "Iteration 38, loss = 0.27903305\n",
      "Iteration 39, loss = 0.27975343\n",
      "Iteration 40, loss = 0.27937552\n",
      "Iteration 41, loss = 0.27738929\n",
      "Iteration 42, loss = 0.27892511\n",
      "Iteration 43, loss = 0.28272989\n",
      "Iteration 44, loss = 0.27818076\n",
      "Iteration 45, loss = 0.28568934\n",
      "Iteration 46, loss = 0.27731683\n",
      "Iteration 47, loss = 0.27671985\n",
      "Iteration 48, loss = 0.27557183\n",
      "Iteration 49, loss = 0.27312169\n",
      "Iteration 50, loss = 0.27768327\n",
      "Iteration 1, loss = 0.56799304\n",
      "Iteration 2, loss = 0.36568554\n",
      "Iteration 3, loss = 0.33124662\n",
      "Iteration 4, loss = 0.31976127\n",
      "Iteration 5, loss = 0.32113224\n",
      "Iteration 6, loss = 0.31165445\n",
      "Iteration 7, loss = 0.30660537\n",
      "Iteration 8, loss = 0.30441284\n",
      "Iteration 9, loss = 0.30109072\n",
      "Iteration 10, loss = 0.30164651\n",
      "Iteration 11, loss = 0.29601916\n",
      "Iteration 12, loss = 0.29644241\n",
      "Iteration 13, loss = 0.29737079\n",
      "Iteration 14, loss = 0.29346431\n",
      "Iteration 15, loss = 0.29362955\n",
      "Iteration 16, loss = 0.29405538\n",
      "Iteration 17, loss = 0.29679369\n",
      "Iteration 18, loss = 0.29731772\n",
      "Iteration 19, loss = 0.28877498\n",
      "Iteration 20, loss = 0.29075862\n",
      "Iteration 21, loss = 0.29446827\n",
      "Iteration 22, loss = 0.28960215\n",
      "Iteration 23, loss = 0.28423766\n",
      "Iteration 24, loss = 0.28660450\n",
      "Iteration 25, loss = 0.28324943\n",
      "Iteration 26, loss = 0.28151431\n",
      "Iteration 27, loss = 0.28079201\n",
      "Iteration 28, loss = 0.28290594\n",
      "Iteration 29, loss = 0.28327229\n",
      "Iteration 30, loss = 0.27940738\n",
      "Iteration 31, loss = 0.28027226\n",
      "Iteration 32, loss = 0.28366922\n",
      "Iteration 33, loss = 0.28105941\n",
      "Iteration 34, loss = 0.28004620\n",
      "Iteration 35, loss = 0.27761188\n",
      "Iteration 36, loss = 0.27888181\n",
      "Iteration 37, loss = 0.27525892\n",
      "Iteration 38, loss = 0.27505097\n",
      "Iteration 39, loss = 0.27230305\n",
      "Iteration 40, loss = 0.28277251\n",
      "Iteration 41, loss = 0.27459980\n",
      "Iteration 42, loss = 0.27574189\n",
      "Iteration 43, loss = 0.27982087\n",
      "Iteration 44, loss = 0.27318749\n",
      "Iteration 45, loss = 0.27112902\n",
      "Iteration 46, loss = 0.27361355\n",
      "Iteration 47, loss = 0.27519105\n",
      "Iteration 48, loss = 0.27879954\n",
      "Iteration 49, loss = 0.29264197\n",
      "Iteration 50, loss = 0.28784103\n",
      "Iteration 1, loss = 0.57005761\n",
      "Iteration 2, loss = 0.36956354\n",
      "Iteration 3, loss = 0.33807653\n",
      "Iteration 4, loss = 0.32906138\n",
      "Iteration 5, loss = 0.32131876\n",
      "Iteration 6, loss = 0.31895360\n",
      "Iteration 7, loss = 0.31371858\n",
      "Iteration 8, loss = 0.30572222\n",
      "Iteration 9, loss = 0.31138889\n",
      "Iteration 10, loss = 0.30549294\n",
      "Iteration 11, loss = 0.30451701\n",
      "Iteration 12, loss = 0.29954570\n",
      "Iteration 13, loss = 0.30387738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, loss = 0.29641883\n",
      "Iteration 15, loss = 0.29077766\n",
      "Iteration 16, loss = 0.29194801\n",
      "Iteration 17, loss = 0.29823697\n",
      "Iteration 18, loss = 0.28870458\n",
      "Iteration 19, loss = 0.29019364\n",
      "Iteration 20, loss = 0.28831929\n",
      "Iteration 21, loss = 0.29778801\n",
      "Iteration 22, loss = 0.29472573\n",
      "Iteration 23, loss = 0.28809169\n",
      "Iteration 24, loss = 0.28465556\n",
      "Iteration 25, loss = 0.28406557\n",
      "Iteration 26, loss = 0.29203624\n",
      "Iteration 27, loss = 0.28215220\n",
      "Iteration 28, loss = 0.28430287\n",
      "Iteration 29, loss = 0.27911141\n",
      "Iteration 30, loss = 0.27603972\n",
      "Iteration 31, loss = 0.27673885\n",
      "Iteration 32, loss = 0.28891769\n",
      "Iteration 33, loss = 0.28832931\n",
      "Iteration 34, loss = 0.28628437\n",
      "Iteration 35, loss = 0.27472325\n",
      "Iteration 36, loss = 0.28002417\n",
      "Iteration 37, loss = 0.27553336\n",
      "Iteration 38, loss = 0.27541201\n",
      "Iteration 39, loss = 0.27973035\n",
      "Iteration 40, loss = 0.27805692\n",
      "Iteration 41, loss = 0.27550705\n",
      "Iteration 42, loss = 0.28205046\n",
      "Iteration 43, loss = 0.27637319\n",
      "Iteration 44, loss = 0.27493881\n",
      "Iteration 45, loss = 0.27904567\n",
      "Iteration 46, loss = 0.28466757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56755238\n",
      "Iteration 2, loss = 0.37090983\n",
      "Iteration 3, loss = 0.33810265\n",
      "Iteration 4, loss = 0.32654788\n",
      "Iteration 5, loss = 0.31759795\n",
      "Iteration 6, loss = 0.31152534\n",
      "Iteration 7, loss = 0.30716398\n",
      "Iteration 8, loss = 0.30191219\n",
      "Iteration 9, loss = 0.30193850\n",
      "Iteration 10, loss = 0.30313519\n",
      "Iteration 11, loss = 0.29636095\n",
      "Iteration 12, loss = 0.29398499\n",
      "Iteration 13, loss = 0.29548470\n",
      "Iteration 14, loss = 0.29352183\n",
      "Iteration 15, loss = 0.29125191\n",
      "Iteration 16, loss = 0.28927475\n",
      "Iteration 17, loss = 0.28773023\n",
      "Iteration 18, loss = 0.28801057\n",
      "Iteration 19, loss = 0.29006450\n",
      "Iteration 20, loss = 0.28732797\n",
      "Iteration 21, loss = 0.28983329\n",
      "Iteration 22, loss = 0.28503143\n",
      "Iteration 23, loss = 0.28216964\n",
      "Iteration 24, loss = 0.28504058\n",
      "Iteration 25, loss = 0.27965462\n",
      "Iteration 26, loss = 0.27795483\n",
      "Iteration 27, loss = 0.27951606\n",
      "Iteration 28, loss = 0.27929025\n",
      "Iteration 29, loss = 0.28720563\n",
      "Iteration 30, loss = 0.27926223\n",
      "Iteration 31, loss = 0.28365961\n",
      "Iteration 32, loss = 0.27792874\n",
      "Iteration 33, loss = 0.27965687\n",
      "Iteration 34, loss = 0.28173323\n",
      "Iteration 35, loss = 0.28561352\n",
      "Iteration 36, loss = 0.27970520\n",
      "Iteration 37, loss = 0.27505458\n",
      "Iteration 38, loss = 0.27451429\n",
      "Iteration 39, loss = 0.27859455\n",
      "Iteration 40, loss = 0.27411621\n",
      "Iteration 41, loss = 0.27660431\n",
      "Iteration 42, loss = 0.27579987\n",
      "Iteration 43, loss = 0.26765418\n",
      "Iteration 44, loss = 0.27392906\n",
      "Iteration 45, loss = 0.27443458\n",
      "Iteration 46, loss = 0.26799617\n",
      "Iteration 47, loss = 0.27213205\n",
      "Iteration 48, loss = 0.27503202\n",
      "Iteration 49, loss = 0.27642996\n",
      "Iteration 50, loss = 0.27156793\n",
      "Iteration 1, loss = 0.56639612\n",
      "Iteration 2, loss = 0.36649843\n",
      "Iteration 3, loss = 0.33178425\n",
      "Iteration 4, loss = 0.32092106\n",
      "Iteration 5, loss = 0.31235949\n",
      "Iteration 6, loss = 0.30457421\n",
      "Iteration 7, loss = 0.30212418\n",
      "Iteration 8, loss = 0.30139132\n",
      "Iteration 9, loss = 0.29390816\n",
      "Iteration 10, loss = 0.29867329\n",
      "Iteration 11, loss = 0.29356536\n",
      "Iteration 12, loss = 0.29099950\n",
      "Iteration 13, loss = 0.28972233\n",
      "Iteration 14, loss = 0.29046528\n",
      "Iteration 15, loss = 0.28646538\n",
      "Iteration 16, loss = 0.28658001\n",
      "Iteration 17, loss = 0.28556300\n",
      "Iteration 18, loss = 0.28755988\n",
      "Iteration 19, loss = 0.28503813\n",
      "Iteration 20, loss = 0.27990499\n",
      "Iteration 21, loss = 0.28660387\n",
      "Iteration 22, loss = 0.28155604\n",
      "Iteration 23, loss = 0.27929838\n",
      "Iteration 24, loss = 0.27857553\n",
      "Iteration 25, loss = 0.27648388\n",
      "Iteration 26, loss = 0.28037975\n",
      "Iteration 27, loss = 0.27555332\n",
      "Iteration 28, loss = 0.27498080\n",
      "Iteration 29, loss = 0.27861050\n",
      "Iteration 30, loss = 0.27358934\n",
      "Iteration 31, loss = 0.27022990\n",
      "Iteration 32, loss = 0.27323456\n",
      "Iteration 33, loss = 0.27849942\n",
      "Iteration 34, loss = 0.27770248\n",
      "Iteration 35, loss = 0.27713626\n",
      "Iteration 36, loss = 0.26850999\n",
      "Iteration 37, loss = 0.27481510\n",
      "Iteration 38, loss = 0.27016151\n",
      "Iteration 39, loss = 0.26964968\n",
      "Iteration 40, loss = 0.27071066\n",
      "Iteration 41, loss = 0.26969108\n",
      "Iteration 42, loss = 0.26602288\n",
      "Iteration 43, loss = 0.27643540\n",
      "Iteration 44, loss = 0.26802954\n",
      "Iteration 45, loss = 0.27166656\n",
      "Iteration 46, loss = 0.27133924\n",
      "Iteration 47, loss = 0.26708531\n",
      "Iteration 48, loss = 0.26417828\n",
      "Iteration 49, loss = 0.26966184\n",
      "Iteration 50, loss = 0.26778144\n",
      "Iteration 1, loss = 0.57079939\n",
      "Iteration 2, loss = 0.37538791\n",
      "Iteration 3, loss = 0.33564630\n",
      "Iteration 4, loss = 0.32404704\n",
      "Iteration 5, loss = 0.32221246\n",
      "Iteration 6, loss = 0.31885551\n",
      "Iteration 7, loss = 0.31059515\n",
      "Iteration 8, loss = 0.30945782\n",
      "Iteration 9, loss = 0.30726242\n",
      "Iteration 10, loss = 0.30165199\n",
      "Iteration 11, loss = 0.30365489\n",
      "Iteration 12, loss = 0.30240130\n",
      "Iteration 13, loss = 0.30048029\n",
      "Iteration 14, loss = 0.29655696\n",
      "Iteration 15, loss = 0.29399261\n",
      "Iteration 16, loss = 0.29918286\n",
      "Iteration 17, loss = 0.29239318\n",
      "Iteration 18, loss = 0.29085396\n",
      "Iteration 19, loss = 0.29308821\n",
      "Iteration 20, loss = 0.28791639\n",
      "Iteration 21, loss = 0.29400194\n",
      "Iteration 22, loss = 0.28936692\n",
      "Iteration 23, loss = 0.28842834\n",
      "Iteration 24, loss = 0.28955802\n",
      "Iteration 25, loss = 0.28545867\n",
      "Iteration 26, loss = 0.28263851\n",
      "Iteration 27, loss = 0.28577824\n",
      "Iteration 28, loss = 0.28359379\n",
      "Iteration 29, loss = 0.28114224\n",
      "Iteration 30, loss = 0.28453656\n",
      "Iteration 31, loss = 0.28808504\n",
      "Iteration 32, loss = 0.28965356\n",
      "Iteration 33, loss = 0.28470380\n",
      "Iteration 34, loss = 0.28356237\n",
      "Iteration 35, loss = 0.27800521\n",
      "Iteration 36, loss = 0.27995754\n",
      "Iteration 37, loss = 0.28003621\n",
      "Iteration 38, loss = 0.27829639\n",
      "Iteration 39, loss = 0.27937389\n",
      "Iteration 40, loss = 0.27915146\n",
      "Iteration 41, loss = 0.27746123\n",
      "Iteration 42, loss = 0.28306094\n",
      "Iteration 43, loss = 0.28001572\n",
      "Iteration 44, loss = 0.27669121\n",
      "Iteration 45, loss = 0.27790110\n",
      "Iteration 46, loss = 0.27624813\n",
      "Iteration 47, loss = 0.28239364\n",
      "Iteration 48, loss = 0.27717851\n",
      "Iteration 49, loss = 0.27427078\n",
      "Iteration 50, loss = 0.27304024\n",
      "Iteration 1, loss = 0.56497479\n",
      "Iteration 2, loss = 0.36836163\n",
      "Iteration 3, loss = 0.33333230\n",
      "Iteration 4, loss = 0.31895773\n",
      "Iteration 5, loss = 0.31150572\n",
      "Iteration 6, loss = 0.31353089\n",
      "Iteration 7, loss = 0.30456735\n",
      "Iteration 8, loss = 0.31174207\n",
      "Iteration 9, loss = 0.30918475\n",
      "Iteration 10, loss = 0.29930458\n",
      "Iteration 11, loss = 0.29818309\n",
      "Iteration 12, loss = 0.29276153\n",
      "Iteration 13, loss = 0.29092662\n",
      "Iteration 14, loss = 0.28709898\n",
      "Iteration 15, loss = 0.29602103\n",
      "Iteration 16, loss = 0.28645598\n",
      "Iteration 17, loss = 0.28664848\n",
      "Iteration 18, loss = 0.28787112\n",
      "Iteration 19, loss = 0.28439185\n",
      "Iteration 20, loss = 0.28340234\n",
      "Iteration 21, loss = 0.28490437\n",
      "Iteration 22, loss = 0.28294327\n",
      "Iteration 23, loss = 0.27963999\n",
      "Iteration 24, loss = 0.28134525\n",
      "Iteration 25, loss = 0.27857394\n",
      "Iteration 26, loss = 0.28401350\n",
      "Iteration 27, loss = 0.27859177\n",
      "Iteration 28, loss = 0.27821985\n",
      "Iteration 29, loss = 0.27756366\n",
      "Iteration 30, loss = 0.28143264\n",
      "Iteration 31, loss = 0.27997361\n",
      "Iteration 32, loss = 0.27260680\n",
      "Iteration 33, loss = 0.27389029\n",
      "Iteration 34, loss = 0.27241853\n",
      "Iteration 35, loss = 0.27099831\n",
      "Iteration 36, loss = 0.27862274\n",
      "Iteration 37, loss = 0.28677153\n",
      "Iteration 38, loss = 0.27498038\n",
      "Iteration 39, loss = 0.27895713\n",
      "Iteration 40, loss = 0.27434994\n",
      "Iteration 41, loss = 0.27265033\n",
      "Iteration 42, loss = 0.27152337\n",
      "Iteration 43, loss = 0.26900219\n",
      "Iteration 44, loss = 0.27036621\n",
      "Iteration 45, loss = 0.27265170\n",
      "Iteration 46, loss = 0.26593045\n",
      "Iteration 47, loss = 0.26763946\n",
      "Iteration 48, loss = 0.26517313\n",
      "Iteration 49, loss = 0.27181902\n",
      "Iteration 50, loss = 0.26552306\n",
      "Iteration 1, loss = 0.56529052\n",
      "Iteration 2, loss = 0.37448489\n",
      "Iteration 3, loss = 0.33619055\n",
      "Iteration 4, loss = 0.32296261\n",
      "Iteration 5, loss = 0.31633042\n",
      "Iteration 6, loss = 0.31434709\n",
      "Iteration 7, loss = 0.31436247\n",
      "Iteration 8, loss = 0.30678082\n",
      "Iteration 9, loss = 0.30403739\n",
      "Iteration 10, loss = 0.30601530\n",
      "Iteration 11, loss = 0.30638381\n",
      "Iteration 12, loss = 0.31056438\n",
      "Iteration 13, loss = 0.30186551\n",
      "Iteration 14, loss = 0.29856658\n",
      "Iteration 15, loss = 0.30118589\n",
      "Iteration 16, loss = 0.30520656\n",
      "Iteration 17, loss = 0.29690216\n",
      "Iteration 18, loss = 0.29335807\n",
      "Iteration 19, loss = 0.28918316\n",
      "Iteration 20, loss = 0.29432407\n",
      "Iteration 21, loss = 0.28871412\n",
      "Iteration 22, loss = 0.28921247\n",
      "Iteration 23, loss = 0.28967480\n",
      "Iteration 24, loss = 0.28610843\n",
      "Iteration 25, loss = 0.28761931\n",
      "Iteration 26, loss = 0.28726058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 27, loss = 0.28347535\n",
      "Iteration 28, loss = 0.28612009\n",
      "Iteration 29, loss = 0.28490117\n",
      "Iteration 30, loss = 0.28769500\n",
      "Iteration 31, loss = 0.28410355\n",
      "Iteration 32, loss = 0.27907691\n",
      "Iteration 33, loss = 0.28168144\n",
      "Iteration 34, loss = 0.28114058\n",
      "Iteration 35, loss = 0.28162388\n",
      "Iteration 36, loss = 0.28027557\n",
      "Iteration 37, loss = 0.27748266\n",
      "Iteration 38, loss = 0.27704422\n",
      "Iteration 39, loss = 0.28214809\n",
      "Iteration 40, loss = 0.27650414\n",
      "Iteration 41, loss = 0.27820147\n",
      "Iteration 42, loss = 0.27568908\n",
      "Iteration 43, loss = 0.27237492\n",
      "Iteration 44, loss = 0.28246353\n",
      "Iteration 45, loss = 0.28320818\n",
      "Iteration 46, loss = 0.28593353\n",
      "Iteration 47, loss = 0.28232163\n",
      "Iteration 48, loss = 0.27902109\n",
      "Iteration 49, loss = 0.28654960\n",
      "Iteration 50, loss = 0.27868739\n",
      "Train Accuracy: 0.8176\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 20.74 Seconds\n",
      "Test  Accuracy: 0.8246\n",
      "Test  Presion:  0.7745\n",
      "Test  Recall:   0.9485\n",
      "Test  F1-score: 0.8527\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS3_bow_2, memory_MLP_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS3_bow_2, acc_MLP_DS3_bow_2, pre_MLP_DS3_bow_2, rec_MLP_DS3_bow_2, f1_MLP_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS3_bow_2, model_3, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60911441\n",
      "Iteration 2, loss = 0.49190131\n",
      "Iteration 3, loss = 0.46404252\n",
      "Iteration 4, loss = 0.45978099\n",
      "Iteration 5, loss = 0.45735347\n",
      "Iteration 6, loss = 0.45682392\n",
      "Iteration 7, loss = 0.45397266\n",
      "Iteration 8, loss = 0.45437400\n",
      "Iteration 9, loss = 0.45708284\n",
      "Iteration 10, loss = 0.45562759\n",
      "Iteration 11, loss = 0.45463484\n",
      "Iteration 12, loss = 0.45698614\n",
      "Iteration 13, loss = 0.45244148\n",
      "Iteration 14, loss = 0.45593870\n",
      "Iteration 15, loss = 0.45322280\n",
      "Iteration 16, loss = 0.45476338\n",
      "Iteration 17, loss = 0.45301735\n",
      "Iteration 18, loss = 0.45193634\n",
      "Iteration 19, loss = 0.45217317\n",
      "Iteration 20, loss = 0.45123956\n",
      "Iteration 21, loss = 0.45241210\n",
      "Iteration 22, loss = 0.45159791\n",
      "Iteration 23, loss = 0.45500201\n",
      "Iteration 24, loss = 0.45164555\n",
      "Iteration 25, loss = 0.44993251\n",
      "Iteration 26, loss = 0.45165441\n",
      "Iteration 27, loss = 0.45469128\n",
      "Iteration 28, loss = 0.45338683\n",
      "Iteration 29, loss = 0.45123096\n",
      "Iteration 30, loss = 0.45255581\n",
      "Iteration 31, loss = 0.45171567\n",
      "Iteration 32, loss = 0.45309643\n",
      "Iteration 33, loss = 0.45153114\n",
      "Iteration 34, loss = 0.45248922\n",
      "Iteration 35, loss = 0.45297006\n",
      "Iteration 36, loss = 0.44986036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61661960\n",
      "Iteration 2, loss = 0.49959323\n",
      "Iteration 3, loss = 0.46847165\n",
      "Iteration 4, loss = 0.46088865\n",
      "Iteration 5, loss = 0.45891780\n",
      "Iteration 6, loss = 0.46092384\n",
      "Iteration 7, loss = 0.45951552\n",
      "Iteration 8, loss = 0.45675559\n",
      "Iteration 9, loss = 0.45648277\n",
      "Iteration 10, loss = 0.45773958\n",
      "Iteration 11, loss = 0.45562500\n",
      "Iteration 12, loss = 0.45517853\n",
      "Iteration 13, loss = 0.45738417\n",
      "Iteration 14, loss = 0.45596655\n",
      "Iteration 15, loss = 0.45665224\n",
      "Iteration 16, loss = 0.45493339\n",
      "Iteration 17, loss = 0.45434162\n",
      "Iteration 18, loss = 0.45378970\n",
      "Iteration 19, loss = 0.45376196\n",
      "Iteration 20, loss = 0.45555268\n",
      "Iteration 21, loss = 0.45675217\n",
      "Iteration 22, loss = 0.45246384\n",
      "Iteration 23, loss = 0.45403387\n",
      "Iteration 24, loss = 0.45478843\n",
      "Iteration 25, loss = 0.45758525\n",
      "Iteration 26, loss = 0.45318156\n",
      "Iteration 27, loss = 0.45359934\n",
      "Iteration 28, loss = 0.45583160\n",
      "Iteration 29, loss = 0.45468935\n",
      "Iteration 30, loss = 0.45500300\n",
      "Iteration 31, loss = 0.45280160\n",
      "Iteration 32, loss = 0.45509506\n",
      "Iteration 33, loss = 0.45305526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61503519\n",
      "Iteration 2, loss = 0.49770229\n",
      "Iteration 3, loss = 0.46674204\n",
      "Iteration 4, loss = 0.45952220\n",
      "Iteration 5, loss = 0.45978201\n",
      "Iteration 6, loss = 0.45662732\n",
      "Iteration 7, loss = 0.45705025\n",
      "Iteration 8, loss = 0.45731316\n",
      "Iteration 9, loss = 0.45483512\n",
      "Iteration 10, loss = 0.45910864\n",
      "Iteration 11, loss = 0.45475596\n",
      "Iteration 12, loss = 0.45793703\n",
      "Iteration 13, loss = 0.45433244\n",
      "Iteration 14, loss = 0.45541192\n",
      "Iteration 15, loss = 0.45394593\n",
      "Iteration 16, loss = 0.45442043\n",
      "Iteration 17, loss = 0.45400113\n",
      "Iteration 18, loss = 0.45485270\n",
      "Iteration 19, loss = 0.45329098\n",
      "Iteration 20, loss = 0.45520172\n",
      "Iteration 21, loss = 0.45420792\n",
      "Iteration 22, loss = 0.45381341\n",
      "Iteration 23, loss = 0.45758247\n",
      "Iteration 24, loss = 0.45208755\n",
      "Iteration 25, loss = 0.45294884\n",
      "Iteration 26, loss = 0.45340436\n",
      "Iteration 27, loss = 0.45519352\n",
      "Iteration 28, loss = 0.45100802\n",
      "Iteration 29, loss = 0.45288003\n",
      "Iteration 30, loss = 0.45192644\n",
      "Iteration 31, loss = 0.45232528\n",
      "Iteration 32, loss = 0.45588262\n",
      "Iteration 33, loss = 0.45285787\n",
      "Iteration 34, loss = 0.45197512\n",
      "Iteration 35, loss = 0.45273298\n",
      "Iteration 36, loss = 0.45392695\n",
      "Iteration 37, loss = 0.45213942\n",
      "Iteration 38, loss = 0.45212684\n",
      "Iteration 39, loss = 0.45227845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61602698\n",
      "Iteration 2, loss = 0.49977101\n",
      "Iteration 3, loss = 0.46540434\n",
      "Iteration 4, loss = 0.45953420\n",
      "Iteration 5, loss = 0.45594374\n",
      "Iteration 6, loss = 0.45996263\n",
      "Iteration 7, loss = 0.45429021\n",
      "Iteration 8, loss = 0.45373696\n",
      "Iteration 9, loss = 0.45428132\n",
      "Iteration 10, loss = 0.45579016\n",
      "Iteration 11, loss = 0.45683196\n",
      "Iteration 12, loss = 0.45753364\n",
      "Iteration 13, loss = 0.45510128\n",
      "Iteration 14, loss = 0.45581119\n",
      "Iteration 15, loss = 0.45349964\n",
      "Iteration 16, loss = 0.45300364\n",
      "Iteration 17, loss = 0.45454624\n",
      "Iteration 18, loss = 0.45264863\n",
      "Iteration 19, loss = 0.45692537\n",
      "Iteration 20, loss = 0.45737925\n",
      "Iteration 21, loss = 0.45179936\n",
      "Iteration 22, loss = 0.45323381\n",
      "Iteration 23, loss = 0.45480243\n",
      "Iteration 24, loss = 0.45185183\n",
      "Iteration 25, loss = 0.45217987\n",
      "Iteration 26, loss = 0.45082491\n",
      "Iteration 27, loss = 0.45260926\n",
      "Iteration 28, loss = 0.45374385\n",
      "Iteration 29, loss = 0.45149818\n",
      "Iteration 30, loss = 0.45400562\n",
      "Iteration 31, loss = 0.45142323\n",
      "Iteration 32, loss = 0.45525328\n",
      "Iteration 33, loss = 0.45154956\n",
      "Iteration 34, loss = 0.45280881\n",
      "Iteration 35, loss = 0.45225837\n",
      "Iteration 36, loss = 0.45121222\n",
      "Iteration 37, loss = 0.45119862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61908912\n",
      "Iteration 2, loss = 0.50040804\n",
      "Iteration 3, loss = 0.46959579\n",
      "Iteration 4, loss = 0.46104352\n",
      "Iteration 5, loss = 0.46368910\n",
      "Iteration 6, loss = 0.45896271\n",
      "Iteration 7, loss = 0.45753830\n",
      "Iteration 8, loss = 0.45743514\n",
      "Iteration 9, loss = 0.45761888\n",
      "Iteration 10, loss = 0.45917027\n",
      "Iteration 11, loss = 0.45499857\n",
      "Iteration 12, loss = 0.45524248\n",
      "Iteration 13, loss = 0.45628607\n",
      "Iteration 14, loss = 0.45724733\n",
      "Iteration 15, loss = 0.45554759\n",
      "Iteration 16, loss = 0.45739243\n",
      "Iteration 17, loss = 0.45741214\n",
      "Iteration 18, loss = 0.45681378\n",
      "Iteration 19, loss = 0.45291407\n",
      "Iteration 20, loss = 0.45394186\n",
      "Iteration 21, loss = 0.46105607\n",
      "Iteration 22, loss = 0.45564941\n",
      "Iteration 23, loss = 0.45459469\n",
      "Iteration 24, loss = 0.45727198\n",
      "Iteration 25, loss = 0.45368829\n",
      "Iteration 26, loss = 0.45597180\n",
      "Iteration 27, loss = 0.45365312\n",
      "Iteration 28, loss = 0.45530163\n",
      "Iteration 29, loss = 0.45426704\n",
      "Iteration 30, loss = 0.45332140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61738019\n",
      "Iteration 2, loss = 0.49823516\n",
      "Iteration 3, loss = 0.46770181\n",
      "Iteration 4, loss = 0.46230865\n",
      "Iteration 5, loss = 0.45933020\n",
      "Iteration 6, loss = 0.45703862\n",
      "Iteration 7, loss = 0.45531786\n",
      "Iteration 8, loss = 0.45608221\n",
      "Iteration 9, loss = 0.45552374\n",
      "Iteration 10, loss = 0.45436393\n",
      "Iteration 11, loss = 0.45730528\n",
      "Iteration 12, loss = 0.45484281\n",
      "Iteration 13, loss = 0.45674342\n",
      "Iteration 14, loss = 0.45390851\n",
      "Iteration 15, loss = 0.45300758\n",
      "Iteration 16, loss = 0.45484154\n",
      "Iteration 17, loss = 0.45555711\n",
      "Iteration 18, loss = 0.45422469\n",
      "Iteration 19, loss = 0.45447543\n",
      "Iteration 20, loss = 0.45115968\n",
      "Iteration 21, loss = 0.45653437\n",
      "Iteration 22, loss = 0.45537827\n",
      "Iteration 23, loss = 0.45178281\n",
      "Iteration 24, loss = 0.45117635\n",
      "Iteration 25, loss = 0.45134015\n",
      "Iteration 26, loss = 0.45516171\n",
      "Iteration 27, loss = 0.45035701\n",
      "Iteration 28, loss = 0.45351876\n",
      "Iteration 29, loss = 0.45275807\n",
      "Iteration 30, loss = 0.45094359\n",
      "Iteration 31, loss = 0.45310402\n",
      "Iteration 32, loss = 0.45075353\n",
      "Iteration 33, loss = 0.45452209\n",
      "Iteration 34, loss = 0.45273260\n",
      "Iteration 35, loss = 0.45259004\n",
      "Iteration 36, loss = 0.45402432\n",
      "Iteration 37, loss = 0.45095929\n",
      "Iteration 38, loss = 0.45177209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61510028\n",
      "Iteration 2, loss = 0.49779826\n",
      "Iteration 3, loss = 0.46610701\n",
      "Iteration 4, loss = 0.45981564\n",
      "Iteration 5, loss = 0.45633688\n",
      "Iteration 6, loss = 0.45759870\n",
      "Iteration 7, loss = 0.45298270\n",
      "Iteration 8, loss = 0.45471557\n",
      "Iteration 9, loss = 0.45499749\n",
      "Iteration 10, loss = 0.45303195\n",
      "Iteration 11, loss = 0.45277007\n",
      "Iteration 12, loss = 0.45208135\n",
      "Iteration 13, loss = 0.45226733\n",
      "Iteration 14, loss = 0.45275659\n",
      "Iteration 15, loss = 0.45469016\n",
      "Iteration 16, loss = 0.44990348\n",
      "Iteration 17, loss = 0.45198705\n",
      "Iteration 18, loss = 0.45307316\n",
      "Iteration 19, loss = 0.45011495\n",
      "Iteration 20, loss = 0.45087202\n",
      "Iteration 21, loss = 0.45176953\n",
      "Iteration 22, loss = 0.45189513\n",
      "Iteration 23, loss = 0.45142854\n",
      "Iteration 24, loss = 0.45492263\n",
      "Iteration 25, loss = 0.44977110\n",
      "Iteration 26, loss = 0.45280475\n",
      "Iteration 27, loss = 0.44979644\n",
      "Iteration 28, loss = 0.44786670\n",
      "Iteration 29, loss = 0.45446593\n",
      "Iteration 30, loss = 0.45318261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, loss = 0.45188523\n",
      "Iteration 32, loss = 0.45002995\n",
      "Iteration 33, loss = 0.45173929\n",
      "Iteration 34, loss = 0.44978716\n",
      "Iteration 35, loss = 0.45363528\n",
      "Iteration 36, loss = 0.44916543\n",
      "Iteration 37, loss = 0.44934523\n",
      "Iteration 38, loss = 0.44766742\n",
      "Iteration 39, loss = 0.45072464\n",
      "Iteration 40, loss = 0.44902248\n",
      "Iteration 41, loss = 0.44816015\n",
      "Iteration 42, loss = 0.45267771\n",
      "Iteration 43, loss = 0.44751620\n",
      "Iteration 44, loss = 0.44795224\n",
      "Iteration 45, loss = 0.45195409\n",
      "Iteration 46, loss = 0.44899171\n",
      "Iteration 47, loss = 0.44826358\n",
      "Iteration 48, loss = 0.44813055\n",
      "Iteration 49, loss = 0.44859854\n",
      "Iteration 50, loss = 0.45129604\n",
      "Iteration 1, loss = 0.61414032\n",
      "Iteration 2, loss = 0.49269456\n",
      "Iteration 3, loss = 0.46244643\n",
      "Iteration 4, loss = 0.45761599\n",
      "Iteration 5, loss = 0.45421097\n",
      "Iteration 6, loss = 0.45507078\n",
      "Iteration 7, loss = 0.45070075\n",
      "Iteration 8, loss = 0.45332464\n",
      "Iteration 9, loss = 0.44958220\n",
      "Iteration 10, loss = 0.45381209\n",
      "Iteration 11, loss = 0.44909838\n",
      "Iteration 12, loss = 0.44810148\n",
      "Iteration 13, loss = 0.44994853\n",
      "Iteration 14, loss = 0.44853067\n",
      "Iteration 15, loss = 0.44762521\n",
      "Iteration 16, loss = 0.44764979\n",
      "Iteration 17, loss = 0.44795757\n",
      "Iteration 18, loss = 0.44760411\n",
      "Iteration 19, loss = 0.44924660\n",
      "Iteration 20, loss = 0.44791177\n",
      "Iteration 21, loss = 0.44945295\n",
      "Iteration 22, loss = 0.45074924\n",
      "Iteration 23, loss = 0.44598146\n",
      "Iteration 24, loss = 0.44782646\n",
      "Iteration 25, loss = 0.44479937\n",
      "Iteration 26, loss = 0.44685380\n",
      "Iteration 27, loss = 0.44547537\n",
      "Iteration 28, loss = 0.44608691\n",
      "Iteration 29, loss = 0.45023065\n",
      "Iteration 30, loss = 0.44720001\n",
      "Iteration 31, loss = 0.44670851\n",
      "Iteration 32, loss = 0.44540329\n",
      "Iteration 33, loss = 0.44816567\n",
      "Iteration 34, loss = 0.44786854\n",
      "Iteration 35, loss = 0.44755491\n",
      "Iteration 36, loss = 0.44584785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61528956\n",
      "Iteration 2, loss = 0.49827884\n",
      "Iteration 3, loss = 0.46641528\n",
      "Iteration 4, loss = 0.46048471\n",
      "Iteration 5, loss = 0.46172753\n",
      "Iteration 6, loss = 0.45719376\n",
      "Iteration 7, loss = 0.45738667\n",
      "Iteration 8, loss = 0.45704599\n",
      "Iteration 9, loss = 0.45582605\n",
      "Iteration 10, loss = 0.45416662\n",
      "Iteration 11, loss = 0.45464573\n",
      "Iteration 12, loss = 0.45448563\n",
      "Iteration 13, loss = 0.45534376\n",
      "Iteration 14, loss = 0.45260600\n",
      "Iteration 15, loss = 0.45119711\n",
      "Iteration 16, loss = 0.45412103\n",
      "Iteration 17, loss = 0.45323692\n",
      "Iteration 18, loss = 0.45108647\n",
      "Iteration 19, loss = 0.45520156\n",
      "Iteration 20, loss = 0.45193627\n",
      "Iteration 21, loss = 0.45831914\n",
      "Iteration 22, loss = 0.45224403\n",
      "Iteration 23, loss = 0.45392989\n",
      "Iteration 24, loss = 0.45309395\n",
      "Iteration 25, loss = 0.45115057\n",
      "Iteration 26, loss = 0.45278044\n",
      "Iteration 27, loss = 0.45212446\n",
      "Iteration 28, loss = 0.45154821\n",
      "Iteration 29, loss = 0.45083838\n",
      "Iteration 30, loss = 0.45158393\n",
      "Iteration 31, loss = 0.45231070\n",
      "Iteration 32, loss = 0.45356754\n",
      "Iteration 33, loss = 0.45262526\n",
      "Iteration 34, loss = 0.45070277\n",
      "Iteration 35, loss = 0.44989242\n",
      "Iteration 36, loss = 0.45226761\n",
      "Iteration 37, loss = 0.45332558\n",
      "Iteration 38, loss = 0.45234800\n",
      "Iteration 39, loss = 0.45075369\n",
      "Iteration 40, loss = 0.45211033\n",
      "Iteration 41, loss = 0.45099477\n",
      "Iteration 42, loss = 0.45262789\n",
      "Iteration 43, loss = 0.45155785\n",
      "Iteration 44, loss = 0.45142238\n",
      "Iteration 45, loss = 0.45129308\n",
      "Iteration 46, loss = 0.45162249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61692212\n",
      "Iteration 2, loss = 0.49445317\n",
      "Iteration 3, loss = 0.46385824\n",
      "Iteration 4, loss = 0.45723953\n",
      "Iteration 5, loss = 0.45573546\n",
      "Iteration 6, loss = 0.45333222\n",
      "Iteration 7, loss = 0.45289103\n",
      "Iteration 8, loss = 0.45820019\n",
      "Iteration 9, loss = 0.45395044\n",
      "Iteration 10, loss = 0.45092141\n",
      "Iteration 11, loss = 0.45269291\n",
      "Iteration 12, loss = 0.45262857\n",
      "Iteration 13, loss = 0.45490949\n",
      "Iteration 14, loss = 0.44926451\n",
      "Iteration 15, loss = 0.45498206\n",
      "Iteration 16, loss = 0.45119523\n",
      "Iteration 17, loss = 0.45355038\n",
      "Iteration 18, loss = 0.45204027\n",
      "Iteration 19, loss = 0.45229218\n",
      "Iteration 20, loss = 0.44982391\n",
      "Iteration 21, loss = 0.45072131\n",
      "Iteration 22, loss = 0.45252377\n",
      "Iteration 23, loss = 0.45110106\n",
      "Iteration 24, loss = 0.44880942\n",
      "Iteration 25, loss = 0.45217935\n",
      "Iteration 26, loss = 0.45089628\n",
      "Iteration 27, loss = 0.44995737\n",
      "Iteration 28, loss = 0.45065535\n",
      "Iteration 29, loss = 0.45109956\n",
      "Iteration 30, loss = 0.45112199\n",
      "Iteration 31, loss = 0.45081733\n",
      "Iteration 32, loss = 0.44816172\n",
      "Iteration 33, loss = 0.45172840\n",
      "Iteration 34, loss = 0.44908283\n",
      "Iteration 35, loss = 0.44754795\n",
      "Iteration 36, loss = 0.45424987\n",
      "Iteration 37, loss = 0.45046240\n",
      "Iteration 38, loss = 0.44898973\n",
      "Iteration 39, loss = 0.45096289\n",
      "Iteration 40, loss = 0.45103655\n",
      "Iteration 41, loss = 0.44936538\n",
      "Iteration 42, loss = 0.44921099\n",
      "Iteration 43, loss = 0.45346938\n",
      "Iteration 44, loss = 0.45160979\n",
      "Iteration 45, loss = 0.44867771\n",
      "Iteration 46, loss = 0.44948465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61362112\n",
      "Iteration 2, loss = 0.49664198\n",
      "Iteration 3, loss = 0.46449661\n",
      "Iteration 4, loss = 0.45993204\n",
      "Iteration 5, loss = 0.45644719\n",
      "Iteration 6, loss = 0.45818087\n",
      "Iteration 7, loss = 0.45928621\n",
      "Iteration 8, loss = 0.45625995\n",
      "Iteration 9, loss = 0.45490204\n",
      "Iteration 10, loss = 0.45762295\n",
      "Iteration 11, loss = 0.45357548\n",
      "Iteration 12, loss = 0.45377089\n",
      "Iteration 13, loss = 0.45441715\n",
      "Iteration 14, loss = 0.45379763\n",
      "Iteration 15, loss = 0.45439003\n",
      "Iteration 16, loss = 0.46120463\n",
      "Iteration 17, loss = 0.45279797\n",
      "Iteration 18, loss = 0.45302370\n",
      "Iteration 19, loss = 0.45202166\n",
      "Iteration 20, loss = 0.45627339\n",
      "Iteration 21, loss = 0.45169254\n",
      "Iteration 22, loss = 0.45307770\n",
      "Iteration 23, loss = 0.45311804\n",
      "Iteration 24, loss = 0.45274020\n",
      "Iteration 25, loss = 0.45285903\n",
      "Iteration 26, loss = 0.45347491\n",
      "Iteration 27, loss = 0.45043820\n",
      "Iteration 28, loss = 0.45215536\n",
      "Iteration 29, loss = 0.45162353\n",
      "Iteration 30, loss = 0.45392658\n",
      "Iteration 31, loss = 0.45198489\n",
      "Iteration 32, loss = 0.45109145\n",
      "Iteration 33, loss = 0.45406472\n",
      "Iteration 34, loss = 0.45143378\n",
      "Iteration 35, loss = 0.45114166\n",
      "Iteration 36, loss = 0.45408680\n",
      "Iteration 37, loss = 0.45213836\n",
      "Iteration 38, loss = 0.45301746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Train Accuracy: 0.6980\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 18.14 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  0.9965\n",
      "Test  Recall:   0.4399\n",
      "Test  F1-score: 0.6104\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS3_bow_3, memory_MLP_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS3_bow_3, acc_MLP_DS3_bow_3, pre_MLP_DS3_bow_3, rec_MLP_DS3_bow_3, f1_MLP_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS3_bow_3, model_3, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63355310\n",
      "Iteration 2, loss = 0.46315709\n",
      "Iteration 3, loss = 0.36939107\n",
      "Iteration 4, loss = 0.33355935\n",
      "Iteration 5, loss = 0.31633001\n",
      "Iteration 6, loss = 0.30295524\n",
      "Iteration 7, loss = 0.28827594\n",
      "Iteration 8, loss = 0.28033224\n",
      "Iteration 9, loss = 0.28080528\n",
      "Iteration 10, loss = 0.26557854\n",
      "Iteration 11, loss = 0.25365548\n",
      "Iteration 12, loss = 0.24240660\n",
      "Iteration 13, loss = 0.23432213\n",
      "Iteration 14, loss = 0.22760009\n",
      "Iteration 15, loss = 0.21668098\n",
      "Iteration 16, loss = 0.22077297\n",
      "Iteration 17, loss = 0.21617019\n",
      "Iteration 18, loss = 0.20703985\n",
      "Iteration 19, loss = 0.20273831\n",
      "Iteration 20, loss = 0.19461704\n",
      "Iteration 21, loss = 0.18652219\n",
      "Iteration 22, loss = 0.18706945\n",
      "Iteration 23, loss = 0.18001211\n",
      "Iteration 24, loss = 0.17461034\n",
      "Iteration 25, loss = 0.16603999\n",
      "Iteration 26, loss = 0.17584446\n",
      "Iteration 27, loss = 0.16201695\n",
      "Iteration 28, loss = 0.15817921\n",
      "Iteration 29, loss = 0.15005064\n",
      "Iteration 30, loss = 0.15793394\n",
      "Iteration 31, loss = 0.13541669\n",
      "Iteration 32, loss = 0.14759205\n",
      "Iteration 33, loss = 0.15842817\n",
      "Iteration 34, loss = 0.13984404\n",
      "Iteration 35, loss = 0.13422719\n",
      "Iteration 36, loss = 0.12140525\n",
      "Iteration 37, loss = 0.12951707\n",
      "Iteration 38, loss = 0.15323360\n",
      "Iteration 39, loss = 0.17355542\n",
      "Iteration 40, loss = 0.12871769\n",
      "Iteration 41, loss = 0.12030136\n",
      "Iteration 42, loss = 0.10981334\n",
      "Iteration 43, loss = 0.12140823\n",
      "Iteration 44, loss = 0.12309644\n",
      "Iteration 45, loss = 0.10254340\n",
      "Iteration 46, loss = 0.10570901\n",
      "Iteration 47, loss = 0.09262376\n",
      "Iteration 48, loss = 0.11957386\n",
      "Iteration 49, loss = 0.13959995\n",
      "Iteration 50, loss = 0.10638554\n",
      "Iteration 1, loss = 0.63732410\n",
      "Iteration 2, loss = 0.47697369\n",
      "Iteration 3, loss = 0.36942283\n",
      "Iteration 4, loss = 0.33206805\n",
      "Iteration 5, loss = 0.31706325\n",
      "Iteration 6, loss = 0.29867887\n",
      "Iteration 7, loss = 0.28777706\n",
      "Iteration 8, loss = 0.27564182\n",
      "Iteration 9, loss = 0.26518914\n",
      "Iteration 10, loss = 0.25867410\n",
      "Iteration 11, loss = 0.25493974\n",
      "Iteration 12, loss = 0.24745098\n",
      "Iteration 13, loss = 0.24033154\n",
      "Iteration 14, loss = 0.22825557\n",
      "Iteration 15, loss = 0.22359558\n",
      "Iteration 16, loss = 0.21576245\n",
      "Iteration 17, loss = 0.21540017\n",
      "Iteration 18, loss = 0.20201113\n",
      "Iteration 19, loss = 0.20693319\n",
      "Iteration 20, loss = 0.20285440\n",
      "Iteration 21, loss = 0.19969016\n",
      "Iteration 22, loss = 0.19214373\n",
      "Iteration 23, loss = 0.18364003\n",
      "Iteration 24, loss = 0.17933519\n",
      "Iteration 25, loss = 0.16897518\n",
      "Iteration 26, loss = 0.16581746\n",
      "Iteration 27, loss = 0.16419138\n",
      "Iteration 28, loss = 0.15188604\n",
      "Iteration 29, loss = 0.14401599\n",
      "Iteration 30, loss = 0.14110840\n",
      "Iteration 31, loss = 0.13801210\n",
      "Iteration 32, loss = 0.15979677\n",
      "Iteration 33, loss = 0.14933417\n",
      "Iteration 34, loss = 0.14146962\n",
      "Iteration 35, loss = 0.12268819\n",
      "Iteration 36, loss = 0.12367427\n",
      "Iteration 37, loss = 0.12817734\n",
      "Iteration 38, loss = 0.11831220\n",
      "Iteration 39, loss = 0.11406834\n",
      "Iteration 40, loss = 0.11170234\n",
      "Iteration 41, loss = 0.11258166\n",
      "Iteration 42, loss = 0.11160371\n",
      "Iteration 43, loss = 0.09777369\n",
      "Iteration 44, loss = 0.09837575\n",
      "Iteration 45, loss = 0.13300394\n",
      "Iteration 46, loss = 0.09286672\n",
      "Iteration 47, loss = 0.11578058\n",
      "Iteration 48, loss = 0.11708165\n",
      "Iteration 49, loss = 0.09420691\n",
      "Iteration 50, loss = 0.08902014\n",
      "Iteration 1, loss = 0.63887818\n",
      "Iteration 2, loss = 0.48551128\n",
      "Iteration 3, loss = 0.37317052\n",
      "Iteration 4, loss = 0.33616417\n",
      "Iteration 5, loss = 0.32210511\n",
      "Iteration 6, loss = 0.30334278\n",
      "Iteration 7, loss = 0.29275762\n",
      "Iteration 8, loss = 0.29038762\n",
      "Iteration 9, loss = 0.26967778\n",
      "Iteration 10, loss = 0.26618061\n",
      "Iteration 11, loss = 0.24924623\n",
      "Iteration 12, loss = 0.24049423\n",
      "Iteration 13, loss = 0.24411265\n",
      "Iteration 14, loss = 0.23220047\n",
      "Iteration 15, loss = 0.22118094\n",
      "Iteration 16, loss = 0.21859918\n",
      "Iteration 17, loss = 0.21262313\n",
      "Iteration 18, loss = 0.21238839\n",
      "Iteration 19, loss = 0.20358411\n",
      "Iteration 20, loss = 0.20304666\n",
      "Iteration 21, loss = 0.19623152\n",
      "Iteration 22, loss = 0.18969025\n",
      "Iteration 23, loss = 0.18509034\n",
      "Iteration 24, loss = 0.17676113\n",
      "Iteration 25, loss = 0.17032171\n",
      "Iteration 26, loss = 0.17695907\n",
      "Iteration 27, loss = 0.17880852\n",
      "Iteration 28, loss = 0.15739988\n",
      "Iteration 29, loss = 0.16334032\n",
      "Iteration 30, loss = 0.18704446\n",
      "Iteration 31, loss = 0.15445251\n",
      "Iteration 32, loss = 0.16434178\n",
      "Iteration 33, loss = 0.14735188\n",
      "Iteration 34, loss = 0.13621975\n",
      "Iteration 35, loss = 0.13099697\n",
      "Iteration 36, loss = 0.14933581\n",
      "Iteration 37, loss = 0.13192684\n",
      "Iteration 38, loss = 0.13814914\n",
      "Iteration 39, loss = 0.13333966\n",
      "Iteration 40, loss = 0.12945202\n",
      "Iteration 41, loss = 0.13404014\n",
      "Iteration 42, loss = 0.11384187\n",
      "Iteration 43, loss = 0.14334193\n",
      "Iteration 44, loss = 0.12059679\n",
      "Iteration 45, loss = 0.10685885\n",
      "Iteration 46, loss = 0.10538042\n",
      "Iteration 47, loss = 0.10284777\n",
      "Iteration 48, loss = 0.09496129\n",
      "Iteration 49, loss = 0.12853525\n",
      "Iteration 50, loss = 0.09973274\n",
      "Iteration 1, loss = 0.63750922\n",
      "Iteration 2, loss = 0.48133452\n",
      "Iteration 3, loss = 0.37023410\n",
      "Iteration 4, loss = 0.33792727\n",
      "Iteration 5, loss = 0.31476354\n",
      "Iteration 6, loss = 0.31524004\n",
      "Iteration 7, loss = 0.29179136\n",
      "Iteration 8, loss = 0.27781888\n",
      "Iteration 9, loss = 0.26983932\n",
      "Iteration 10, loss = 0.26001713\n",
      "Iteration 11, loss = 0.25553511\n",
      "Iteration 12, loss = 0.24883633\n",
      "Iteration 13, loss = 0.23758988\n",
      "Iteration 14, loss = 0.24033640\n",
      "Iteration 15, loss = 0.22142837\n",
      "Iteration 16, loss = 0.22088415\n",
      "Iteration 17, loss = 0.21132292\n",
      "Iteration 18, loss = 0.20780508\n",
      "Iteration 19, loss = 0.21262611\n",
      "Iteration 20, loss = 0.19742969\n",
      "Iteration 21, loss = 0.18462680\n",
      "Iteration 22, loss = 0.18272770\n",
      "Iteration 23, loss = 0.18039143\n",
      "Iteration 24, loss = 0.16833191\n",
      "Iteration 25, loss = 0.20603662\n",
      "Iteration 26, loss = 0.16665569\n",
      "Iteration 27, loss = 0.16409844\n",
      "Iteration 28, loss = 0.16415883\n",
      "Iteration 29, loss = 0.15649277\n",
      "Iteration 30, loss = 0.14630668\n",
      "Iteration 31, loss = 0.15261584\n",
      "Iteration 32, loss = 0.13679979\n",
      "Iteration 33, loss = 0.18712769\n",
      "Iteration 34, loss = 0.13354652\n",
      "Iteration 35, loss = 0.15820455\n",
      "Iteration 36, loss = 0.14025267\n",
      "Iteration 37, loss = 0.14105426\n",
      "Iteration 38, loss = 0.12314911\n",
      "Iteration 39, loss = 0.12272695\n",
      "Iteration 40, loss = 0.12081009\n",
      "Iteration 41, loss = 0.12196466\n",
      "Iteration 42, loss = 0.13994731\n",
      "Iteration 43, loss = 0.11279869\n",
      "Iteration 44, loss = 0.11520296\n",
      "Iteration 45, loss = 0.11103505\n",
      "Iteration 46, loss = 0.09569265\n",
      "Iteration 47, loss = 0.08997197\n",
      "Iteration 48, loss = 0.23275302\n",
      "Iteration 49, loss = 0.10698438\n",
      "Iteration 50, loss = 0.10804742\n",
      "Iteration 1, loss = 0.63920498\n",
      "Iteration 2, loss = 0.48561778\n",
      "Iteration 3, loss = 0.37803448\n",
      "Iteration 4, loss = 0.34042911\n",
      "Iteration 5, loss = 0.33082656\n",
      "Iteration 6, loss = 0.31145143\n",
      "Iteration 7, loss = 0.29090204\n",
      "Iteration 8, loss = 0.27718182\n",
      "Iteration 9, loss = 0.26776680\n",
      "Iteration 10, loss = 0.26040858\n",
      "Iteration 11, loss = 0.25238799\n",
      "Iteration 12, loss = 0.24439150\n",
      "Iteration 13, loss = 0.24368817\n",
      "Iteration 14, loss = 0.23312901\n",
      "Iteration 15, loss = 0.22326125\n",
      "Iteration 16, loss = 0.22094259\n",
      "Iteration 17, loss = 0.21606310\n",
      "Iteration 18, loss = 0.21546657\n",
      "Iteration 19, loss = 0.20113754\n",
      "Iteration 20, loss = 0.19870124\n",
      "Iteration 21, loss = 0.20011445\n",
      "Iteration 22, loss = 0.19951052\n",
      "Iteration 23, loss = 0.18047958\n",
      "Iteration 24, loss = 0.17839363\n",
      "Iteration 25, loss = 0.17162546\n",
      "Iteration 26, loss = 0.19138693\n",
      "Iteration 27, loss = 0.17271878\n",
      "Iteration 28, loss = 0.16620907\n",
      "Iteration 29, loss = 0.16149887\n",
      "Iteration 30, loss = 0.15369213\n",
      "Iteration 31, loss = 0.21855774\n",
      "Iteration 32, loss = 0.15347481\n",
      "Iteration 33, loss = 0.14681057\n",
      "Iteration 34, loss = 0.14494146\n",
      "Iteration 35, loss = 0.14336952\n",
      "Iteration 36, loss = 0.13883543\n",
      "Iteration 37, loss = 0.12016568\n",
      "Iteration 38, loss = 0.13293204\n",
      "Iteration 39, loss = 0.13018032\n",
      "Iteration 40, loss = 0.11589565\n",
      "Iteration 41, loss = 0.13376799\n",
      "Iteration 42, loss = 0.10581919\n",
      "Iteration 43, loss = 0.10477166\n",
      "Iteration 44, loss = 0.09836054\n",
      "Iteration 45, loss = 0.14625337\n",
      "Iteration 46, loss = 0.13975521\n",
      "Iteration 47, loss = 0.10661801\n",
      "Iteration 48, loss = 0.09590321\n",
      "Iteration 49, loss = 0.10064245\n",
      "Iteration 50, loss = 0.09115588\n",
      "Iteration 1, loss = 0.63882318\n",
      "Iteration 2, loss = 0.48479890\n",
      "Iteration 3, loss = 0.37869391\n",
      "Iteration 4, loss = 0.33411458\n",
      "Iteration 5, loss = 0.32547510\n",
      "Iteration 6, loss = 0.30471232\n",
      "Iteration 7, loss = 0.29114685\n",
      "Iteration 8, loss = 0.27890976\n",
      "Iteration 9, loss = 0.27602906\n",
      "Iteration 10, loss = 0.25909308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.25577773\n",
      "Iteration 12, loss = 0.24361924\n",
      "Iteration 13, loss = 0.24146826\n",
      "Iteration 14, loss = 0.23107008\n",
      "Iteration 15, loss = 0.22631743\n",
      "Iteration 16, loss = 0.22245563\n",
      "Iteration 17, loss = 0.21904549\n",
      "Iteration 18, loss = 0.21174904\n",
      "Iteration 19, loss = 0.22050921\n",
      "Iteration 20, loss = 0.20080918\n",
      "Iteration 21, loss = 0.20387662\n",
      "Iteration 22, loss = 0.19223165\n",
      "Iteration 23, loss = 0.18444603\n",
      "Iteration 24, loss = 0.17578069\n",
      "Iteration 25, loss = 0.17723913\n",
      "Iteration 26, loss = 0.18254633\n",
      "Iteration 27, loss = 0.15920135\n",
      "Iteration 28, loss = 0.16765176\n",
      "Iteration 29, loss = 0.15036016\n",
      "Iteration 30, loss = 0.15187948\n",
      "Iteration 31, loss = 0.14371445\n",
      "Iteration 32, loss = 0.14543649\n",
      "Iteration 33, loss = 0.15099904\n",
      "Iteration 34, loss = 0.13969977\n",
      "Iteration 35, loss = 0.12567209\n",
      "Iteration 36, loss = 0.15277701\n",
      "Iteration 37, loss = 0.12067484\n",
      "Iteration 38, loss = 0.14702552\n",
      "Iteration 39, loss = 0.12211453\n",
      "Iteration 40, loss = 0.13171288\n",
      "Iteration 41, loss = 0.10993340\n",
      "Iteration 42, loss = 0.15159820\n",
      "Iteration 43, loss = 0.10692907\n",
      "Iteration 44, loss = 0.10410935\n",
      "Iteration 45, loss = 0.10209421\n",
      "Iteration 46, loss = 0.09699904\n",
      "Iteration 47, loss = 0.09538212\n",
      "Iteration 48, loss = 0.08221198\n",
      "Iteration 49, loss = 0.09709489\n",
      "Iteration 50, loss = 0.11422847\n",
      "Iteration 1, loss = 0.63900337\n",
      "Iteration 2, loss = 0.48141058\n",
      "Iteration 3, loss = 0.37797192\n",
      "Iteration 4, loss = 0.34892179\n",
      "Iteration 5, loss = 0.32414731\n",
      "Iteration 6, loss = 0.30696082\n",
      "Iteration 7, loss = 0.29691263\n",
      "Iteration 8, loss = 0.28088710\n",
      "Iteration 9, loss = 0.27201576\n",
      "Iteration 10, loss = 0.26380043\n",
      "Iteration 11, loss = 0.25405421\n",
      "Iteration 12, loss = 0.24403223\n",
      "Iteration 13, loss = 0.23587581\n",
      "Iteration 14, loss = 0.23109999\n",
      "Iteration 15, loss = 0.22547972\n",
      "Iteration 16, loss = 0.21386292\n",
      "Iteration 17, loss = 0.21681458\n",
      "Iteration 18, loss = 0.20133539\n",
      "Iteration 19, loss = 0.20331985\n",
      "Iteration 20, loss = 0.19449233\n",
      "Iteration 21, loss = 0.18920456\n",
      "Iteration 22, loss = 0.19556376\n",
      "Iteration 23, loss = 0.18589821\n",
      "Iteration 24, loss = 0.17268287\n",
      "Iteration 25, loss = 0.16534741\n",
      "Iteration 26, loss = 0.16472821\n",
      "Iteration 27, loss = 0.16035273\n",
      "Iteration 28, loss = 0.15458053\n",
      "Iteration 29, loss = 0.14677207\n",
      "Iteration 30, loss = 0.16063760\n",
      "Iteration 31, loss = 0.14403491\n",
      "Iteration 32, loss = 0.14096312\n",
      "Iteration 33, loss = 0.13754288\n",
      "Iteration 34, loss = 0.14345378\n",
      "Iteration 35, loss = 0.13042329\n",
      "Iteration 36, loss = 0.12983681\n",
      "Iteration 37, loss = 0.12566856\n",
      "Iteration 38, loss = 0.13108952\n",
      "Iteration 39, loss = 0.12641528\n",
      "Iteration 40, loss = 0.11278982\n",
      "Iteration 41, loss = 0.10863716\n",
      "Iteration 42, loss = 0.10063841\n",
      "Iteration 43, loss = 0.09670858\n",
      "Iteration 44, loss = 0.10997297\n",
      "Iteration 45, loss = 0.09679557\n",
      "Iteration 46, loss = 0.13501949\n",
      "Iteration 47, loss = 0.09878649\n",
      "Iteration 48, loss = 0.09241281\n",
      "Iteration 49, loss = 0.09879113\n",
      "Iteration 50, loss = 0.08872051\n",
      "Iteration 1, loss = 0.63882961\n",
      "Iteration 2, loss = 0.48136367\n",
      "Iteration 3, loss = 0.37349667\n",
      "Iteration 4, loss = 0.33409519\n",
      "Iteration 5, loss = 0.31476138\n",
      "Iteration 6, loss = 0.30682564\n",
      "Iteration 7, loss = 0.28875356\n",
      "Iteration 8, loss = 0.27847690\n",
      "Iteration 9, loss = 0.27085255\n",
      "Iteration 10, loss = 0.25936360\n",
      "Iteration 11, loss = 0.24703379\n",
      "Iteration 12, loss = 0.24491226\n",
      "Iteration 13, loss = 0.23604331\n",
      "Iteration 14, loss = 0.23850147\n",
      "Iteration 15, loss = 0.22965775\n",
      "Iteration 16, loss = 0.22047143\n",
      "Iteration 17, loss = 0.21643680\n",
      "Iteration 18, loss = 0.20775419\n",
      "Iteration 19, loss = 0.20662188\n",
      "Iteration 20, loss = 0.20530999\n",
      "Iteration 21, loss = 0.19480863\n",
      "Iteration 22, loss = 0.19215981\n",
      "Iteration 23, loss = 0.18081552\n",
      "Iteration 24, loss = 0.17478032\n",
      "Iteration 25, loss = 0.17174137\n",
      "Iteration 26, loss = 0.17379255\n",
      "Iteration 27, loss = 0.15977335\n",
      "Iteration 28, loss = 0.17621860\n",
      "Iteration 29, loss = 0.15784753\n",
      "Iteration 30, loss = 0.19165762\n",
      "Iteration 31, loss = 0.14677540\n",
      "Iteration 32, loss = 0.13982360\n",
      "Iteration 33, loss = 0.14970883\n",
      "Iteration 34, loss = 0.13130853\n",
      "Iteration 35, loss = 0.13872793\n",
      "Iteration 36, loss = 0.12729499\n",
      "Iteration 37, loss = 0.11724064\n",
      "Iteration 38, loss = 0.12549351\n",
      "Iteration 39, loss = 0.11393332\n",
      "Iteration 40, loss = 0.15343578\n",
      "Iteration 41, loss = 0.11957984\n",
      "Iteration 42, loss = 0.13939696\n",
      "Iteration 43, loss = 0.11968252\n",
      "Iteration 44, loss = 0.10019662\n",
      "Iteration 45, loss = 0.11209441\n",
      "Iteration 46, loss = 0.13275346\n",
      "Iteration 47, loss = 0.09749050\n",
      "Iteration 48, loss = 0.09411826\n",
      "Iteration 49, loss = 0.09315667\n",
      "Iteration 50, loss = 0.08847806\n",
      "Iteration 1, loss = 0.63989691\n",
      "Iteration 2, loss = 0.48538064\n",
      "Iteration 3, loss = 0.37985888\n",
      "Iteration 4, loss = 0.34347218\n",
      "Iteration 5, loss = 0.33400337\n",
      "Iteration 6, loss = 0.31968246\n",
      "Iteration 7, loss = 0.30745588\n",
      "Iteration 8, loss = 0.29363842\n",
      "Iteration 9, loss = 0.28458980\n",
      "Iteration 10, loss = 0.27247190\n",
      "Iteration 11, loss = 0.26021236\n",
      "Iteration 12, loss = 0.25795758\n",
      "Iteration 13, loss = 0.25477224\n",
      "Iteration 14, loss = 0.24550712\n",
      "Iteration 15, loss = 0.23359217\n",
      "Iteration 16, loss = 0.22849706\n",
      "Iteration 17, loss = 0.22908505\n",
      "Iteration 18, loss = 0.22288537\n",
      "Iteration 19, loss = 0.21182432\n",
      "Iteration 20, loss = 0.20968407\n",
      "Iteration 21, loss = 0.20337941\n",
      "Iteration 22, loss = 0.19458812\n",
      "Iteration 23, loss = 0.19737365\n",
      "Iteration 24, loss = 0.18834065\n",
      "Iteration 25, loss = 0.18811684\n",
      "Iteration 26, loss = 0.17862045\n",
      "Iteration 27, loss = 0.17672185\n",
      "Iteration 28, loss = 0.17243327\n",
      "Iteration 29, loss = 0.15873251\n",
      "Iteration 30, loss = 0.16083739\n",
      "Iteration 31, loss = 0.15810911\n",
      "Iteration 32, loss = 0.15909640\n",
      "Iteration 33, loss = 0.14232785\n",
      "Iteration 34, loss = 0.21896270\n",
      "Iteration 35, loss = 0.14221229\n",
      "Iteration 36, loss = 0.12998826\n",
      "Iteration 37, loss = 0.13137460\n",
      "Iteration 38, loss = 0.18594203\n",
      "Iteration 39, loss = 0.13280319\n",
      "Iteration 40, loss = 0.13382628\n",
      "Iteration 41, loss = 0.11829013\n",
      "Iteration 42, loss = 0.14709585\n",
      "Iteration 43, loss = 0.13634766\n",
      "Iteration 44, loss = 0.12265494\n",
      "Iteration 45, loss = 0.14182765\n",
      "Iteration 46, loss = 0.10848672\n",
      "Iteration 47, loss = 0.11135946\n",
      "Iteration 48, loss = 0.10293261\n",
      "Iteration 49, loss = 0.10014882\n",
      "Iteration 50, loss = 0.09855411\n",
      "Iteration 1, loss = 0.64239258\n",
      "Iteration 2, loss = 0.48584847\n",
      "Iteration 3, loss = 0.37399706\n",
      "Iteration 4, loss = 0.33484852\n",
      "Iteration 5, loss = 0.32343866\n",
      "Iteration 6, loss = 0.30571972\n",
      "Iteration 7, loss = 0.29252693\n",
      "Iteration 8, loss = 0.28671556\n",
      "Iteration 9, loss = 0.28432672\n",
      "Iteration 10, loss = 0.27076599\n",
      "Iteration 11, loss = 0.26845233\n",
      "Iteration 12, loss = 0.25634382\n",
      "Iteration 13, loss = 0.25172582\n",
      "Iteration 14, loss = 0.24069637\n",
      "Iteration 15, loss = 0.24462722\n",
      "Iteration 16, loss = 0.22941821\n",
      "Iteration 17, loss = 0.23430962\n",
      "Iteration 18, loss = 0.21570032\n",
      "Iteration 19, loss = 0.21464586\n",
      "Iteration 20, loss = 0.20641110\n",
      "Iteration 21, loss = 0.21827776\n",
      "Iteration 22, loss = 0.19739638\n",
      "Iteration 23, loss = 0.19329897\n",
      "Iteration 24, loss = 0.18204479\n",
      "Iteration 25, loss = 0.17989875\n",
      "Iteration 26, loss = 0.18978131\n",
      "Iteration 27, loss = 0.16967659\n",
      "Iteration 28, loss = 0.16627872\n",
      "Iteration 29, loss = 0.17192589\n",
      "Iteration 30, loss = 0.16233266\n",
      "Iteration 31, loss = 0.16655703\n",
      "Iteration 32, loss = 0.15064856\n",
      "Iteration 33, loss = 0.14683241\n",
      "Iteration 34, loss = 0.14685617\n",
      "Iteration 35, loss = 0.14011926\n",
      "Iteration 36, loss = 0.15144916\n",
      "Iteration 37, loss = 0.14133996\n",
      "Iteration 38, loss = 0.11920304\n",
      "Iteration 39, loss = 0.12657523\n",
      "Iteration 40, loss = 0.11919978\n",
      "Iteration 41, loss = 0.13649620\n",
      "Iteration 42, loss = 0.12068968\n",
      "Iteration 43, loss = 0.11994171\n",
      "Iteration 44, loss = 0.11872516\n",
      "Iteration 45, loss = 0.12596615\n",
      "Iteration 46, loss = 0.10715597\n",
      "Iteration 47, loss = 0.10163930\n",
      "Iteration 48, loss = 0.09487307\n",
      "Iteration 49, loss = 0.14529889\n",
      "Iteration 50, loss = 0.11246551\n",
      "Iteration 1, loss = 0.63961715\n",
      "Iteration 2, loss = 0.48429053\n",
      "Iteration 3, loss = 0.37871350\n",
      "Iteration 4, loss = 0.33957355\n",
      "Iteration 5, loss = 0.32426911\n",
      "Iteration 6, loss = 0.30827952\n",
      "Iteration 7, loss = 0.30343090\n",
      "Iteration 8, loss = 0.28261519\n",
      "Iteration 9, loss = 0.27364646\n",
      "Iteration 10, loss = 0.26561509\n",
      "Iteration 11, loss = 0.24986062\n",
      "Iteration 12, loss = 0.24345556\n",
      "Iteration 13, loss = 0.23462418\n",
      "Iteration 14, loss = 0.22831193\n",
      "Iteration 15, loss = 0.24018472\n",
      "Iteration 16, loss = 0.21679492\n",
      "Iteration 17, loss = 0.21875828\n",
      "Iteration 18, loss = 0.20542248\n",
      "Iteration 19, loss = 0.20894097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.20006870\n",
      "Iteration 21, loss = 0.18724282\n",
      "Iteration 22, loss = 0.18629583\n",
      "Iteration 23, loss = 0.17995014\n",
      "Iteration 24, loss = 0.17328119\n",
      "Iteration 25, loss = 0.17785238\n",
      "Iteration 26, loss = 0.16972813\n",
      "Iteration 27, loss = 0.16283639\n",
      "Iteration 28, loss = 0.15911901\n",
      "Iteration 29, loss = 0.14901237\n",
      "Iteration 30, loss = 0.14536511\n",
      "Iteration 31, loss = 0.16999019\n",
      "Iteration 32, loss = 0.14068206\n",
      "Iteration 33, loss = 0.13351797\n",
      "Iteration 34, loss = 0.15093786\n",
      "Iteration 35, loss = 0.14309909\n",
      "Iteration 36, loss = 0.12885769\n",
      "Iteration 37, loss = 0.12792019\n",
      "Iteration 38, loss = 0.15386677\n",
      "Iteration 39, loss = 0.12401159\n",
      "Iteration 40, loss = 0.22559556\n",
      "Iteration 41, loss = 0.13022487\n",
      "Iteration 42, loss = 0.12120900\n",
      "Iteration 43, loss = 0.10829818\n",
      "Iteration 44, loss = 0.11140321\n",
      "Iteration 45, loss = 0.10882258\n",
      "Iteration 46, loss = 0.10074641\n",
      "Iteration 47, loss = 0.10332132\n",
      "Iteration 48, loss = 0.11082084\n",
      "Iteration 49, loss = 0.10856173\n",
      "Iteration 50, loss = 0.10009668\n",
      "Train Accuracy: 0.9011\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 23.54 Seconds\n",
      "Test  Accuracy: 0.9148\n",
      "Test  Presion:  0.9140\n",
      "Test  Recall:   0.9282\n",
      "Test  F1-score: 0.9211\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS3_tfidf_1, memory_MLP_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS3_tfidf_1, acc_MLP_DS3_tfidf_1, pre_MLP_DS3_tfidf_1, rec_MLP_DS3_tfidf_1, f1_MLP_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS3_tfidf_1, model_3, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66809203\n",
      "Iteration 2, loss = 0.51240317\n",
      "Iteration 3, loss = 0.36749462\n",
      "Iteration 4, loss = 0.35464758\n",
      "Iteration 5, loss = 0.34270972\n",
      "Iteration 6, loss = 0.34306632\n",
      "Iteration 7, loss = 0.33040054\n",
      "Iteration 8, loss = 0.33015664\n",
      "Iteration 9, loss = 0.34374598\n",
      "Iteration 10, loss = 0.33499034\n",
      "Iteration 11, loss = 0.33336354\n",
      "Iteration 12, loss = 0.33240511\n",
      "Iteration 13, loss = 0.32528875\n",
      "Iteration 14, loss = 0.32882639\n",
      "Iteration 15, loss = 0.32457747\n",
      "Iteration 16, loss = 0.33157642\n",
      "Iteration 17, loss = 0.32580511\n",
      "Iteration 18, loss = 0.32245919\n",
      "Iteration 19, loss = 0.32471826\n",
      "Iteration 20, loss = 0.32443296\n",
      "Iteration 21, loss = 0.32626556\n",
      "Iteration 22, loss = 0.32144686\n",
      "Iteration 23, loss = 0.32360918\n",
      "Iteration 24, loss = 0.31869412\n",
      "Iteration 25, loss = 0.31956895\n",
      "Iteration 26, loss = 0.32285048\n",
      "Iteration 27, loss = 0.32494316\n",
      "Iteration 28, loss = 0.32306434\n",
      "Iteration 29, loss = 0.31987982\n",
      "Iteration 30, loss = 0.31462898\n",
      "Iteration 31, loss = 0.32065097\n",
      "Iteration 32, loss = 0.32182655\n",
      "Iteration 33, loss = 0.32499268\n",
      "Iteration 34, loss = 0.32109123\n",
      "Iteration 35, loss = 0.31700229\n",
      "Iteration 36, loss = 0.31101838\n",
      "Iteration 37, loss = 0.31925363\n",
      "Iteration 38, loss = 0.31091160\n",
      "Iteration 39, loss = 0.31900273\n",
      "Iteration 40, loss = 0.31635048\n",
      "Iteration 41, loss = 0.31117249\n",
      "Iteration 42, loss = 0.31066464\n",
      "Iteration 43, loss = 0.32924903\n",
      "Iteration 44, loss = 0.31300191\n",
      "Iteration 45, loss = 0.30938760\n",
      "Iteration 46, loss = 0.30755920\n",
      "Iteration 47, loss = 0.31096883\n",
      "Iteration 48, loss = 0.31265367\n",
      "Iteration 49, loss = 0.31701846\n",
      "Iteration 50, loss = 0.31811279\n",
      "Iteration 1, loss = 0.67249464\n",
      "Iteration 2, loss = 0.53715403\n",
      "Iteration 3, loss = 0.38560541\n",
      "Iteration 4, loss = 0.34523350\n",
      "Iteration 5, loss = 0.33790040\n",
      "Iteration 6, loss = 0.34112650\n",
      "Iteration 7, loss = 0.34060270\n",
      "Iteration 8, loss = 0.34016467\n",
      "Iteration 9, loss = 0.33723853\n",
      "Iteration 10, loss = 0.33295978\n",
      "Iteration 11, loss = 0.32988571\n",
      "Iteration 12, loss = 0.33042705\n",
      "Iteration 13, loss = 0.33129596\n",
      "Iteration 14, loss = 0.32399315\n",
      "Iteration 15, loss = 0.32798000\n",
      "Iteration 16, loss = 0.32872417\n",
      "Iteration 17, loss = 0.32330595\n",
      "Iteration 18, loss = 0.32608209\n",
      "Iteration 19, loss = 0.32336382\n",
      "Iteration 20, loss = 0.32317821\n",
      "Iteration 21, loss = 0.33527263\n",
      "Iteration 22, loss = 0.32738129\n",
      "Iteration 23, loss = 0.32454153\n",
      "Iteration 24, loss = 0.33565682\n",
      "Iteration 25, loss = 0.34002557\n",
      "Iteration 26, loss = 0.32528988\n",
      "Iteration 27, loss = 0.32593449\n",
      "Iteration 28, loss = 0.31890068\n",
      "Iteration 29, loss = 0.32217820\n",
      "Iteration 30, loss = 0.32239260\n",
      "Iteration 31, loss = 0.31946795\n",
      "Iteration 32, loss = 0.32420672\n",
      "Iteration 33, loss = 0.31742277\n",
      "Iteration 34, loss = 0.31955522\n",
      "Iteration 35, loss = 0.32050848\n",
      "Iteration 36, loss = 0.31742699\n",
      "Iteration 37, loss = 0.31713627\n",
      "Iteration 38, loss = 0.31068832\n",
      "Iteration 39, loss = 0.32131923\n",
      "Iteration 40, loss = 0.32786646\n",
      "Iteration 41, loss = 0.31649911\n",
      "Iteration 42, loss = 0.32142217\n",
      "Iteration 43, loss = 0.31139682\n",
      "Iteration 44, loss = 0.30965978\n",
      "Iteration 45, loss = 0.31284476\n",
      "Iteration 46, loss = 0.31826977\n",
      "Iteration 47, loss = 0.30867376\n",
      "Iteration 48, loss = 0.31226231\n",
      "Iteration 49, loss = 0.31054843\n",
      "Iteration 50, loss = 0.31544567\n",
      "Iteration 1, loss = 0.67249615\n",
      "Iteration 2, loss = 0.53456863\n",
      "Iteration 3, loss = 0.37838579\n",
      "Iteration 4, loss = 0.34506296\n",
      "Iteration 5, loss = 0.34482877\n",
      "Iteration 6, loss = 0.33091828\n",
      "Iteration 7, loss = 0.33037749\n",
      "Iteration 8, loss = 0.33561533\n",
      "Iteration 9, loss = 0.32835335\n",
      "Iteration 10, loss = 0.33760703\n",
      "Iteration 11, loss = 0.32885550\n",
      "Iteration 12, loss = 0.33452119\n",
      "Iteration 13, loss = 0.32927218\n",
      "Iteration 14, loss = 0.33844914\n",
      "Iteration 15, loss = 0.32288752\n",
      "Iteration 16, loss = 0.32172512\n",
      "Iteration 17, loss = 0.32304101\n",
      "Iteration 18, loss = 0.32813435\n",
      "Iteration 19, loss = 0.32325836\n",
      "Iteration 20, loss = 0.32806436\n",
      "Iteration 21, loss = 0.32621251\n",
      "Iteration 22, loss = 0.32040031\n",
      "Iteration 23, loss = 0.32357218\n",
      "Iteration 24, loss = 0.32276583\n",
      "Iteration 25, loss = 0.31674192\n",
      "Iteration 26, loss = 0.32134579\n",
      "Iteration 27, loss = 0.33077693\n",
      "Iteration 28, loss = 0.31856613\n",
      "Iteration 29, loss = 0.31755116\n",
      "Iteration 30, loss = 0.31615435\n",
      "Iteration 31, loss = 0.31590164\n",
      "Iteration 32, loss = 0.31992570\n",
      "Iteration 33, loss = 0.31317335\n",
      "Iteration 34, loss = 0.30988824\n",
      "Iteration 35, loss = 0.31084977\n",
      "Iteration 36, loss = 0.31710531\n",
      "Iteration 37, loss = 0.31630842\n",
      "Iteration 38, loss = 0.31002782\n",
      "Iteration 39, loss = 0.30714620\n",
      "Iteration 40, loss = 0.31925491\n",
      "Iteration 41, loss = 0.31679104\n",
      "Iteration 42, loss = 0.31210015\n",
      "Iteration 43, loss = 0.31258867\n",
      "Iteration 44, loss = 0.30477055\n",
      "Iteration 45, loss = 0.31603128\n",
      "Iteration 46, loss = 0.31357590\n",
      "Iteration 47, loss = 0.30622055\n",
      "Iteration 48, loss = 0.30878388\n",
      "Iteration 49, loss = 0.30718758\n",
      "Iteration 50, loss = 0.30884838\n",
      "Iteration 1, loss = 0.67234820\n",
      "Iteration 2, loss = 0.53753205\n",
      "Iteration 3, loss = 0.38214562\n",
      "Iteration 4, loss = 0.34302170\n",
      "Iteration 5, loss = 0.34239298\n",
      "Iteration 6, loss = 0.35154403\n",
      "Iteration 7, loss = 0.33227126\n",
      "Iteration 8, loss = 0.32776163\n",
      "Iteration 9, loss = 0.32734197\n",
      "Iteration 10, loss = 0.32708486\n",
      "Iteration 11, loss = 0.33322116\n",
      "Iteration 12, loss = 0.33588422\n",
      "Iteration 13, loss = 0.32734600\n",
      "Iteration 14, loss = 0.33247512\n",
      "Iteration 15, loss = 0.33074058\n",
      "Iteration 16, loss = 0.32310129\n",
      "Iteration 17, loss = 0.33012239\n",
      "Iteration 18, loss = 0.32682427\n",
      "Iteration 19, loss = 0.33030390\n",
      "Iteration 20, loss = 0.33935963\n",
      "Iteration 21, loss = 0.31988329\n",
      "Iteration 22, loss = 0.32127900\n",
      "Iteration 23, loss = 0.33554665\n",
      "Iteration 24, loss = 0.31899501\n",
      "Iteration 25, loss = 0.31819874\n",
      "Iteration 26, loss = 0.31795810\n",
      "Iteration 27, loss = 0.31862129\n",
      "Iteration 28, loss = 0.32843711\n",
      "Iteration 29, loss = 0.31959298\n",
      "Iteration 30, loss = 0.31952117\n",
      "Iteration 31, loss = 0.32002900\n",
      "Iteration 32, loss = 0.32279930\n",
      "Iteration 33, loss = 0.32108805\n",
      "Iteration 34, loss = 0.31901923\n",
      "Iteration 35, loss = 0.31754362\n",
      "Iteration 36, loss = 0.32158019\n",
      "Iteration 37, loss = 0.31431499\n",
      "Iteration 38, loss = 0.31255182\n",
      "Iteration 39, loss = 0.31438582\n",
      "Iteration 40, loss = 0.31795349\n",
      "Iteration 41, loss = 0.31400717\n",
      "Iteration 42, loss = 0.31272207\n",
      "Iteration 43, loss = 0.32118762\n",
      "Iteration 44, loss = 0.31467117\n",
      "Iteration 45, loss = 0.32601485\n",
      "Iteration 46, loss = 0.31321058\n",
      "Iteration 47, loss = 0.31142206\n",
      "Iteration 48, loss = 0.31010006\n",
      "Iteration 49, loss = 0.30605817\n",
      "Iteration 50, loss = 0.31647149\n",
      "Iteration 1, loss = 0.67211694\n",
      "Iteration 2, loss = 0.54074878\n",
      "Iteration 3, loss = 0.38330493\n",
      "Iteration 4, loss = 0.34602922\n",
      "Iteration 5, loss = 0.35880702\n",
      "Iteration 6, loss = 0.34055740\n",
      "Iteration 7, loss = 0.33093940\n",
      "Iteration 8, loss = 0.34051025\n",
      "Iteration 9, loss = 0.33747809\n",
      "Iteration 10, loss = 0.32939113\n",
      "Iteration 11, loss = 0.32585599\n",
      "Iteration 12, loss = 0.33071841\n",
      "Iteration 13, loss = 0.32893778\n",
      "Iteration 14, loss = 0.33373988\n",
      "Iteration 15, loss = 0.32811623\n",
      "Iteration 16, loss = 0.32892331\n",
      "Iteration 17, loss = 0.32908744\n",
      "Iteration 18, loss = 0.33408053\n",
      "Iteration 19, loss = 0.32177452\n",
      "Iteration 20, loss = 0.32262394\n",
      "Iteration 21, loss = 0.32902743\n",
      "Iteration 22, loss = 0.32848215\n",
      "Iteration 23, loss = 0.31862845\n",
      "Iteration 24, loss = 0.32446346\n",
      "Iteration 25, loss = 0.31779475\n",
      "Iteration 26, loss = 0.32011967\n",
      "Iteration 27, loss = 0.31611756\n",
      "Iteration 28, loss = 0.31905602\n",
      "Iteration 29, loss = 0.32372991\n",
      "Iteration 30, loss = 0.31725583\n",
      "Iteration 31, loss = 0.31829443\n",
      "Iteration 32, loss = 0.31835261\n",
      "Iteration 33, loss = 0.32077910\n",
      "Iteration 34, loss = 0.31892580\n",
      "Iteration 35, loss = 0.31161574\n",
      "Iteration 36, loss = 0.31471996\n",
      "Iteration 37, loss = 0.31166449\n",
      "Iteration 38, loss = 0.31508330\n",
      "Iteration 39, loss = 0.30794152\n",
      "Iteration 40, loss = 0.32422397\n",
      "Iteration 41, loss = 0.31233199\n",
      "Iteration 42, loss = 0.31162093\n",
      "Iteration 43, loss = 0.31246410\n",
      "Iteration 44, loss = 0.30873918\n",
      "Iteration 45, loss = 0.30834003\n",
      "Iteration 46, loss = 0.30966665\n",
      "Iteration 47, loss = 0.30900642\n",
      "Iteration 48, loss = 0.31246085\n",
      "Iteration 49, loss = 0.31262389\n",
      "Iteration 50, loss = 0.30938894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67313842\n",
      "Iteration 2, loss = 0.54173148\n",
      "Iteration 3, loss = 0.38832664\n",
      "Iteration 4, loss = 0.34798853\n",
      "Iteration 5, loss = 0.35657485\n",
      "Iteration 6, loss = 0.33704610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.34140132\n",
      "Iteration 8, loss = 0.33162819\n",
      "Iteration 9, loss = 0.33474917\n",
      "Iteration 10, loss = 0.33077411\n",
      "Iteration 11, loss = 0.33142604\n",
      "Iteration 12, loss = 0.33246425\n",
      "Iteration 13, loss = 0.33981751\n",
      "Iteration 14, loss = 0.32854340\n",
      "Iteration 15, loss = 0.32290267\n",
      "Iteration 16, loss = 0.33153061\n",
      "Iteration 17, loss = 0.33761651\n",
      "Iteration 18, loss = 0.32480846\n",
      "Iteration 19, loss = 0.32691245\n",
      "Iteration 20, loss = 0.32389017\n",
      "Iteration 21, loss = 0.33916076\n",
      "Iteration 22, loss = 0.33048939\n",
      "Iteration 23, loss = 0.31971350\n",
      "Iteration 24, loss = 0.32339529\n",
      "Iteration 25, loss = 0.32124142\n",
      "Iteration 26, loss = 0.33594357\n",
      "Iteration 27, loss = 0.31600643\n",
      "Iteration 28, loss = 0.32404264\n",
      "Iteration 29, loss = 0.31667264\n",
      "Iteration 30, loss = 0.31395486\n",
      "Iteration 31, loss = 0.31592598\n",
      "Iteration 32, loss = 0.32171983\n",
      "Iteration 33, loss = 0.32683851\n",
      "Iteration 34, loss = 0.32195183\n",
      "Iteration 35, loss = 0.31168967\n",
      "Iteration 36, loss = 0.32065939\n",
      "Iteration 37, loss = 0.31435769\n",
      "Iteration 38, loss = 0.31033973\n",
      "Iteration 39, loss = 0.31492328\n",
      "Iteration 40, loss = 0.31494200\n",
      "Iteration 41, loss = 0.30947987\n",
      "Iteration 42, loss = 0.32098821\n",
      "Iteration 43, loss = 0.31601249\n",
      "Iteration 44, loss = 0.31354861\n",
      "Iteration 45, loss = 0.30649606\n",
      "Iteration 46, loss = 0.32102875\n",
      "Iteration 47, loss = 0.31181208\n",
      "Iteration 48, loss = 0.31068289\n",
      "Iteration 49, loss = 0.31049139\n",
      "Iteration 50, loss = 0.30925476\n",
      "Iteration 1, loss = 0.67288062\n",
      "Iteration 2, loss = 0.53586434\n",
      "Iteration 3, loss = 0.38345200\n",
      "Iteration 4, loss = 0.34743712\n",
      "Iteration 5, loss = 0.33867528\n",
      "Iteration 6, loss = 0.33766204\n",
      "Iteration 7, loss = 0.34250352\n",
      "Iteration 8, loss = 0.33005758\n",
      "Iteration 9, loss = 0.33493879\n",
      "Iteration 10, loss = 0.33937129\n",
      "Iteration 11, loss = 0.33126760\n",
      "Iteration 12, loss = 0.33011136\n",
      "Iteration 13, loss = 0.33152514\n",
      "Iteration 14, loss = 0.32713508\n",
      "Iteration 15, loss = 0.33099155\n",
      "Iteration 16, loss = 0.32151443\n",
      "Iteration 17, loss = 0.33161111\n",
      "Iteration 18, loss = 0.32313947\n",
      "Iteration 19, loss = 0.32986498\n",
      "Iteration 20, loss = 0.32193055\n",
      "Iteration 21, loss = 0.32528606\n",
      "Iteration 22, loss = 0.32596140\n",
      "Iteration 23, loss = 0.32301698\n",
      "Iteration 24, loss = 0.32670852\n",
      "Iteration 25, loss = 0.32114980\n",
      "Iteration 26, loss = 0.31735738\n",
      "Iteration 27, loss = 0.31770302\n",
      "Iteration 28, loss = 0.31730662\n",
      "Iteration 29, loss = 0.32849013\n",
      "Iteration 30, loss = 0.32480227\n",
      "Iteration 31, loss = 0.31895083\n",
      "Iteration 32, loss = 0.31247727\n",
      "Iteration 33, loss = 0.31821939\n",
      "Iteration 34, loss = 0.32159817\n",
      "Iteration 35, loss = 0.32427191\n",
      "Iteration 36, loss = 0.31090899\n",
      "Iteration 37, loss = 0.31288440\n",
      "Iteration 38, loss = 0.31254355\n",
      "Iteration 39, loss = 0.31933178\n",
      "Iteration 40, loss = 0.31095871\n",
      "Iteration 41, loss = 0.31375917\n",
      "Iteration 42, loss = 0.31882430\n",
      "Iteration 43, loss = 0.30553844\n",
      "Iteration 44, loss = 0.31138344\n",
      "Iteration 45, loss = 0.31796187\n",
      "Iteration 46, loss = 0.30824730\n",
      "Iteration 47, loss = 0.31406607\n",
      "Iteration 48, loss = 0.31197067\n",
      "Iteration 49, loss = 0.30667907\n",
      "Iteration 50, loss = 0.30324916\n",
      "Iteration 1, loss = 0.67282502\n",
      "Iteration 2, loss = 0.53143095\n",
      "Iteration 3, loss = 0.37418568\n",
      "Iteration 4, loss = 0.34370476\n",
      "Iteration 5, loss = 0.34294934\n",
      "Iteration 6, loss = 0.32761980\n",
      "Iteration 7, loss = 0.32500954\n",
      "Iteration 8, loss = 0.34258218\n",
      "Iteration 9, loss = 0.32641377\n",
      "Iteration 10, loss = 0.32964898\n",
      "Iteration 11, loss = 0.32285043\n",
      "Iteration 12, loss = 0.32167284\n",
      "Iteration 13, loss = 0.32420193\n",
      "Iteration 14, loss = 0.32438079\n",
      "Iteration 15, loss = 0.32063259\n",
      "Iteration 16, loss = 0.32353601\n",
      "Iteration 17, loss = 0.32266597\n",
      "Iteration 18, loss = 0.32366998\n",
      "Iteration 19, loss = 0.32236107\n",
      "Iteration 20, loss = 0.31740286\n",
      "Iteration 21, loss = 0.32496125\n",
      "Iteration 22, loss = 0.32173721\n",
      "Iteration 23, loss = 0.32176715\n",
      "Iteration 24, loss = 0.31503611\n",
      "Iteration 25, loss = 0.31436393\n",
      "Iteration 26, loss = 0.32368211\n",
      "Iteration 27, loss = 0.31206837\n",
      "Iteration 28, loss = 0.30962731\n",
      "Iteration 29, loss = 0.32119503\n",
      "Iteration 30, loss = 0.31788474\n",
      "Iteration 31, loss = 0.30865766\n",
      "Iteration 32, loss = 0.31161829\n",
      "Iteration 33, loss = 0.31985984\n",
      "Iteration 34, loss = 0.31025798\n",
      "Iteration 35, loss = 0.31417293\n",
      "Iteration 36, loss = 0.30817040\n",
      "Iteration 37, loss = 0.31133476\n",
      "Iteration 38, loss = 0.30931724\n",
      "Iteration 39, loss = 0.31800616\n",
      "Iteration 40, loss = 0.31143978\n",
      "Iteration 41, loss = 0.31606253\n",
      "Iteration 42, loss = 0.31136204\n",
      "Iteration 43, loss = 0.31138343\n",
      "Iteration 44, loss = 0.30863962\n",
      "Iteration 45, loss = 0.30675788\n",
      "Iteration 46, loss = 0.30697888\n",
      "Iteration 47, loss = 0.30083880\n",
      "Iteration 48, loss = 0.30575559\n",
      "Iteration 49, loss = 0.30956863\n",
      "Iteration 50, loss = 0.30617772\n",
      "Iteration 1, loss = 0.67255929\n",
      "Iteration 2, loss = 0.53907409\n",
      "Iteration 3, loss = 0.38256872\n",
      "Iteration 4, loss = 0.34450638\n",
      "Iteration 5, loss = 0.35290504\n",
      "Iteration 6, loss = 0.34196845\n",
      "Iteration 7, loss = 0.34517922\n",
      "Iteration 8, loss = 0.35071075\n",
      "Iteration 9, loss = 0.32854653\n",
      "Iteration 10, loss = 0.32750527\n",
      "Iteration 11, loss = 0.33252404\n",
      "Iteration 12, loss = 0.33094078\n",
      "Iteration 13, loss = 0.32818935\n",
      "Iteration 14, loss = 0.32648099\n",
      "Iteration 15, loss = 0.32533311\n",
      "Iteration 16, loss = 0.33902908\n",
      "Iteration 17, loss = 0.32541138\n",
      "Iteration 18, loss = 0.32487065\n",
      "Iteration 19, loss = 0.32858860\n",
      "Iteration 20, loss = 0.32289470\n",
      "Iteration 21, loss = 0.32579426\n",
      "Iteration 22, loss = 0.32818755\n",
      "Iteration 23, loss = 0.32278011\n",
      "Iteration 24, loss = 0.31979218\n",
      "Iteration 25, loss = 0.32057739\n",
      "Iteration 26, loss = 0.31630363\n",
      "Iteration 27, loss = 0.33022503\n",
      "Iteration 28, loss = 0.32553809\n",
      "Iteration 29, loss = 0.31852651\n",
      "Iteration 30, loss = 0.32005440\n",
      "Iteration 31, loss = 0.31964467\n",
      "Iteration 32, loss = 0.32542445\n",
      "Iteration 33, loss = 0.31528767\n",
      "Iteration 34, loss = 0.32288195\n",
      "Iteration 35, loss = 0.31211177\n",
      "Iteration 36, loss = 0.31653920\n",
      "Iteration 37, loss = 0.31550786\n",
      "Iteration 38, loss = 0.31862662\n",
      "Iteration 39, loss = 0.31941008\n",
      "Iteration 40, loss = 0.31758140\n",
      "Iteration 41, loss = 0.31581961\n",
      "Iteration 42, loss = 0.32614010\n",
      "Iteration 43, loss = 0.31630177\n",
      "Iteration 44, loss = 0.31372199\n",
      "Iteration 45, loss = 0.31599806\n",
      "Iteration 46, loss = 0.31226409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67393438\n",
      "Iteration 2, loss = 0.53560446\n",
      "Iteration 3, loss = 0.37943576\n",
      "Iteration 4, loss = 0.34444000\n",
      "Iteration 5, loss = 0.34554267\n",
      "Iteration 6, loss = 0.33656709\n",
      "Iteration 7, loss = 0.34164941\n",
      "Iteration 8, loss = 0.33437403\n",
      "Iteration 9, loss = 0.33455165\n",
      "Iteration 10, loss = 0.32948387\n",
      "Iteration 11, loss = 0.33517175\n",
      "Iteration 12, loss = 0.32811462\n",
      "Iteration 13, loss = 0.33393847\n",
      "Iteration 14, loss = 0.32283278\n",
      "Iteration 15, loss = 0.33834688\n",
      "Iteration 16, loss = 0.32597171\n",
      "Iteration 17, loss = 0.32605946\n",
      "Iteration 18, loss = 0.33025074\n",
      "Iteration 19, loss = 0.32419662\n",
      "Iteration 20, loss = 0.32427461\n",
      "Iteration 21, loss = 0.32543081\n",
      "Iteration 22, loss = 0.32276640\n",
      "Iteration 23, loss = 0.32259104\n",
      "Iteration 24, loss = 0.31840603\n",
      "Iteration 25, loss = 0.32061991\n",
      "Iteration 26, loss = 0.33791500\n",
      "Iteration 27, loss = 0.32080282\n",
      "Iteration 28, loss = 0.31608126\n",
      "Iteration 29, loss = 0.32065578\n",
      "Iteration 30, loss = 0.32476902\n",
      "Iteration 31, loss = 0.32193944\n",
      "Iteration 32, loss = 0.31639467\n",
      "Iteration 33, loss = 0.31660871\n",
      "Iteration 34, loss = 0.31238478\n",
      "Iteration 35, loss = 0.31159099\n",
      "Iteration 36, loss = 0.32238665\n",
      "Iteration 37, loss = 0.33416969\n",
      "Iteration 38, loss = 0.31121959\n",
      "Iteration 39, loss = 0.32084444\n",
      "Iteration 40, loss = 0.32195949\n",
      "Iteration 41, loss = 0.31758328\n",
      "Iteration 42, loss = 0.31068787\n",
      "Iteration 43, loss = 0.31191486\n",
      "Iteration 44, loss = 0.30918698\n",
      "Iteration 45, loss = 0.31679456\n",
      "Iteration 46, loss = 0.30772097\n",
      "Iteration 47, loss = 0.30907807\n",
      "Iteration 48, loss = 0.30945257\n",
      "Iteration 49, loss = 0.31171602\n",
      "Iteration 50, loss = 0.30632331\n",
      "Iteration 1, loss = 0.67262300\n",
      "Iteration 2, loss = 0.53621305\n",
      "Iteration 3, loss = 0.38217553\n",
      "Iteration 4, loss = 0.34703713\n",
      "Iteration 5, loss = 0.33871654\n",
      "Iteration 6, loss = 0.33729369\n",
      "Iteration 7, loss = 0.35912513\n",
      "Iteration 8, loss = 0.33837864\n",
      "Iteration 9, loss = 0.33158643\n",
      "Iteration 10, loss = 0.34194148\n",
      "Iteration 11, loss = 0.32928188\n",
      "Iteration 12, loss = 0.32863640\n",
      "Iteration 13, loss = 0.33293119\n",
      "Iteration 14, loss = 0.33008812\n",
      "Iteration 15, loss = 0.33766439\n",
      "Iteration 16, loss = 0.33517576\n",
      "Iteration 17, loss = 0.32862430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.32727285\n",
      "Iteration 19, loss = 0.32193302\n",
      "Iteration 20, loss = 0.33619906\n",
      "Iteration 21, loss = 0.32195006\n",
      "Iteration 22, loss = 0.32574228\n",
      "Iteration 23, loss = 0.32700476\n",
      "Iteration 24, loss = 0.31878465\n",
      "Iteration 25, loss = 0.32054538\n",
      "Iteration 26, loss = 0.32344612\n",
      "Iteration 27, loss = 0.31660480\n",
      "Iteration 28, loss = 0.32464595\n",
      "Iteration 29, loss = 0.32924051\n",
      "Iteration 30, loss = 0.32215764\n",
      "Iteration 31, loss = 0.32066822\n",
      "Iteration 32, loss = 0.31599617\n",
      "Iteration 33, loss = 0.31936927\n",
      "Iteration 34, loss = 0.32305039\n",
      "Iteration 35, loss = 0.31879651\n",
      "Iteration 36, loss = 0.32062478\n",
      "Iteration 37, loss = 0.31809099\n",
      "Iteration 38, loss = 0.31877351\n",
      "Iteration 39, loss = 0.31671078\n",
      "Iteration 40, loss = 0.31488013\n",
      "Iteration 41, loss = 0.31235626\n",
      "Iteration 42, loss = 0.31188392\n",
      "Iteration 43, loss = 0.30827256\n",
      "Iteration 44, loss = 0.32178176\n",
      "Iteration 45, loss = 0.31607040\n",
      "Iteration 46, loss = 0.31979817\n",
      "Iteration 47, loss = 0.31427057\n",
      "Iteration 48, loss = 0.30959609\n",
      "Iteration 49, loss = 0.31404379\n",
      "Iteration 50, loss = 0.31515620\n",
      "Train Accuracy: 0.8054\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 22.38 Seconds\n",
      "Test  Accuracy: 0.8296\n",
      "Test  Presion:  0.8014\n",
      "Test  Recall:   0.9064\n",
      "Test  F1-score: 0.8507\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS3_tfidf_2, memory_MLP_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS3_tfidf_2, acc_MLP_DS3_tfidf_2, pre_MLP_DS3_tfidf_2, rec_MLP_DS3_tfidf_2, f1_MLP_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS3_tfidf_2, model_3, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67140359\n",
      "Iteration 2, loss = 0.57592159\n",
      "Iteration 3, loss = 0.48460558\n",
      "Iteration 4, loss = 0.46775481\n",
      "Iteration 5, loss = 0.46157181\n",
      "Iteration 6, loss = 0.46630119\n",
      "Iteration 7, loss = 0.45785900\n",
      "Iteration 8, loss = 0.45981970\n",
      "Iteration 9, loss = 0.47013022\n",
      "Iteration 10, loss = 0.46398419\n",
      "Iteration 11, loss = 0.46160683\n",
      "Iteration 12, loss = 0.46311443\n",
      "Iteration 13, loss = 0.45626107\n",
      "Iteration 14, loss = 0.46151038\n",
      "Iteration 15, loss = 0.45882927\n",
      "Iteration 16, loss = 0.46158709\n",
      "Iteration 17, loss = 0.45943072\n",
      "Iteration 18, loss = 0.45746954\n",
      "Iteration 19, loss = 0.45819247\n",
      "Iteration 20, loss = 0.45770070\n",
      "Iteration 21, loss = 0.45704243\n",
      "Iteration 22, loss = 0.45724407\n",
      "Iteration 23, loss = 0.46163105\n",
      "Iteration 24, loss = 0.45557685\n",
      "Iteration 25, loss = 0.45491807\n",
      "Iteration 26, loss = 0.45748232\n",
      "Iteration 27, loss = 0.46361302\n",
      "Iteration 28, loss = 0.46176814\n",
      "Iteration 29, loss = 0.45677430\n",
      "Iteration 30, loss = 0.45749509\n",
      "Iteration 31, loss = 0.45700657\n",
      "Iteration 32, loss = 0.45957931\n",
      "Iteration 33, loss = 0.45823154\n",
      "Iteration 34, loss = 0.45997049\n",
      "Iteration 35, loss = 0.45949664\n",
      "Iteration 36, loss = 0.45468430\n",
      "Iteration 37, loss = 0.45977844\n",
      "Iteration 38, loss = 0.45613265\n",
      "Iteration 39, loss = 0.45821264\n",
      "Iteration 40, loss = 0.45817056\n",
      "Iteration 41, loss = 0.45705945\n",
      "Iteration 42, loss = 0.45475617\n",
      "Iteration 43, loss = 0.46329781\n",
      "Iteration 44, loss = 0.45708854\n",
      "Iteration 45, loss = 0.45523363\n",
      "Iteration 46, loss = 0.45658947\n",
      "Iteration 47, loss = 0.45570143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67374325\n",
      "Iteration 2, loss = 0.58935965\n",
      "Iteration 3, loss = 0.49571831\n",
      "Iteration 4, loss = 0.46649701\n",
      "Iteration 5, loss = 0.46158520\n",
      "Iteration 6, loss = 0.47319235\n",
      "Iteration 7, loss = 0.46165797\n",
      "Iteration 8, loss = 0.46304943\n",
      "Iteration 9, loss = 0.46432625\n",
      "Iteration 10, loss = 0.46287835\n",
      "Iteration 11, loss = 0.46348671\n",
      "Iteration 12, loss = 0.46203378\n",
      "Iteration 13, loss = 0.46511056\n",
      "Iteration 14, loss = 0.45734382\n",
      "Iteration 15, loss = 0.46269459\n",
      "Iteration 16, loss = 0.46204788\n",
      "Iteration 17, loss = 0.46134699\n",
      "Iteration 18, loss = 0.45913127\n",
      "Iteration 19, loss = 0.45954507\n",
      "Iteration 20, loss = 0.46277481\n",
      "Iteration 21, loss = 0.46719283\n",
      "Iteration 22, loss = 0.45811893\n",
      "Iteration 23, loss = 0.45953116\n",
      "Iteration 24, loss = 0.46202551\n",
      "Iteration 25, loss = 0.46788567\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67324011\n",
      "Iteration 2, loss = 0.58818635\n",
      "Iteration 3, loss = 0.49260361\n",
      "Iteration 4, loss = 0.46509728\n",
      "Iteration 5, loss = 0.46658883\n",
      "Iteration 6, loss = 0.46016485\n",
      "Iteration 7, loss = 0.46201566\n",
      "Iteration 8, loss = 0.46698666\n",
      "Iteration 9, loss = 0.45938701\n",
      "Iteration 10, loss = 0.47254704\n",
      "Iteration 11, loss = 0.46067936\n",
      "Iteration 12, loss = 0.46507684\n",
      "Iteration 13, loss = 0.46036002\n",
      "Iteration 14, loss = 0.46240272\n",
      "Iteration 15, loss = 0.45657575\n",
      "Iteration 16, loss = 0.45959134\n",
      "Iteration 17, loss = 0.45913334\n",
      "Iteration 18, loss = 0.46312415\n",
      "Iteration 19, loss = 0.45949535\n",
      "Iteration 20, loss = 0.46175359\n",
      "Iteration 21, loss = 0.46019596\n",
      "Iteration 22, loss = 0.45813935\n",
      "Iteration 23, loss = 0.46358361\n",
      "Iteration 24, loss = 0.45772040\n",
      "Iteration 25, loss = 0.45763315\n",
      "Iteration 26, loss = 0.45980077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67380274\n",
      "Iteration 2, loss = 0.59044015\n",
      "Iteration 3, loss = 0.49201814\n",
      "Iteration 4, loss = 0.46584944\n",
      "Iteration 5, loss = 0.46201223\n",
      "Iteration 6, loss = 0.47408297\n",
      "Iteration 7, loss = 0.45835929\n",
      "Iteration 8, loss = 0.45599550\n",
      "Iteration 9, loss = 0.46018403\n",
      "Iteration 10, loss = 0.46022308\n",
      "Iteration 11, loss = 0.46492174\n",
      "Iteration 12, loss = 0.46908544\n",
      "Iteration 13, loss = 0.46294454\n",
      "Iteration 14, loss = 0.46148510\n",
      "Iteration 15, loss = 0.46057454\n",
      "Iteration 16, loss = 0.45913003\n",
      "Iteration 17, loss = 0.46224263\n",
      "Iteration 18, loss = 0.45785700\n",
      "Iteration 19, loss = 0.46277001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67407740\n",
      "Iteration 2, loss = 0.59134458\n",
      "Iteration 3, loss = 0.49715891\n",
      "Iteration 4, loss = 0.46764632\n",
      "Iteration 5, loss = 0.47796042\n",
      "Iteration 6, loss = 0.46595928\n",
      "Iteration 7, loss = 0.46210709\n",
      "Iteration 8, loss = 0.46147762\n",
      "Iteration 9, loss = 0.46262290\n",
      "Iteration 10, loss = 0.46105813\n",
      "Iteration 11, loss = 0.45808671\n",
      "Iteration 12, loss = 0.46047989\n",
      "Iteration 13, loss = 0.46304626\n",
      "Iteration 14, loss = 0.46505154\n",
      "Iteration 15, loss = 0.46332219\n",
      "Iteration 16, loss = 0.46371089\n",
      "Iteration 17, loss = 0.46535420\n",
      "Iteration 18, loss = 0.46469816\n",
      "Iteration 19, loss = 0.45726502\n",
      "Iteration 20, loss = 0.45956381\n",
      "Iteration 21, loss = 0.46957882\n",
      "Iteration 22, loss = 0.46299785\n",
      "Iteration 23, loss = 0.45994987\n",
      "Iteration 24, loss = 0.46457680\n",
      "Iteration 25, loss = 0.45923622\n",
      "Iteration 26, loss = 0.46306618\n",
      "Iteration 27, loss = 0.45895676\n",
      "Iteration 28, loss = 0.46393664\n",
      "Iteration 29, loss = 0.46123672\n",
      "Iteration 30, loss = 0.45960394\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67466076\n",
      "Iteration 2, loss = 0.59166964\n",
      "Iteration 3, loss = 0.49637950\n",
      "Iteration 4, loss = 0.47028474\n",
      "Iteration 5, loss = 0.46975734\n",
      "Iteration 6, loss = 0.46104459\n",
      "Iteration 7, loss = 0.45844329\n",
      "Iteration 8, loss = 0.46404640\n",
      "Iteration 9, loss = 0.46414497\n",
      "Iteration 10, loss = 0.45967878\n",
      "Iteration 11, loss = 0.46547809\n",
      "Iteration 12, loss = 0.46156884\n",
      "Iteration 13, loss = 0.46635367\n",
      "Iteration 14, loss = 0.45992664\n",
      "Iteration 15, loss = 0.45735013\n",
      "Iteration 16, loss = 0.46581229\n",
      "Iteration 17, loss = 0.46407466\n",
      "Iteration 18, loss = 0.46012943\n",
      "Iteration 19, loss = 0.46230715\n",
      "Iteration 20, loss = 0.45647135\n",
      "Iteration 21, loss = 0.46609243\n",
      "Iteration 22, loss = 0.46367382\n",
      "Iteration 23, loss = 0.45754427\n",
      "Iteration 24, loss = 0.45677947\n",
      "Iteration 25, loss = 0.45686690\n",
      "Iteration 26, loss = 0.46537117\n",
      "Iteration 27, loss = 0.45539233\n",
      "Iteration 28, loss = 0.46000104\n",
      "Iteration 29, loss = 0.45760362\n",
      "Iteration 30, loss = 0.45591009\n",
      "Iteration 31, loss = 0.45857956\n",
      "Iteration 32, loss = 0.45562413\n",
      "Iteration 33, loss = 0.46275281\n",
      "Iteration 34, loss = 0.45934594\n",
      "Iteration 35, loss = 0.45811956\n",
      "Iteration 36, loss = 0.46086082\n",
      "Iteration 37, loss = 0.45575832\n",
      "Iteration 38, loss = 0.45830134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67398692\n",
      "Iteration 2, loss = 0.58824952\n",
      "Iteration 3, loss = 0.49276931\n",
      "Iteration 4, loss = 0.46730349\n",
      "Iteration 5, loss = 0.45940525\n",
      "Iteration 6, loss = 0.46598412\n",
      "Iteration 7, loss = 0.46071132\n",
      "Iteration 8, loss = 0.45832488\n",
      "Iteration 9, loss = 0.46562911\n",
      "Iteration 10, loss = 0.45816511\n",
      "Iteration 11, loss = 0.45973371\n",
      "Iteration 12, loss = 0.45914849\n",
      "Iteration 13, loss = 0.45899289\n",
      "Iteration 14, loss = 0.45769166\n",
      "Iteration 15, loss = 0.46488074\n",
      "Iteration 16, loss = 0.45514825\n",
      "Iteration 17, loss = 0.45787829\n",
      "Iteration 18, loss = 0.46156940\n",
      "Iteration 19, loss = 0.45744437\n",
      "Iteration 20, loss = 0.45753341\n",
      "Iteration 21, loss = 0.46058466\n",
      "Iteration 22, loss = 0.45901200\n",
      "Iteration 23, loss = 0.45928490\n",
      "Iteration 24, loss = 0.46429939\n",
      "Iteration 25, loss = 0.45744872\n",
      "Iteration 26, loss = 0.45952192\n",
      "Iteration 27, loss = 0.45684006\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67442447\n",
      "Iteration 2, loss = 0.58590258\n",
      "Iteration 3, loss = 0.48910003\n",
      "Iteration 4, loss = 0.46567669\n",
      "Iteration 5, loss = 0.45907135\n",
      "Iteration 6, loss = 0.45469023\n",
      "Iteration 7, loss = 0.45292091\n",
      "Iteration 8, loss = 0.46662709\n",
      "Iteration 9, loss = 0.45717428\n",
      "Iteration 10, loss = 0.45840142\n",
      "Iteration 11, loss = 0.45339034\n",
      "Iteration 12, loss = 0.45259389\n",
      "Iteration 13, loss = 0.45409464\n",
      "Iteration 14, loss = 0.45526007\n",
      "Iteration 15, loss = 0.45297678\n",
      "Iteration 16, loss = 0.45202949\n",
      "Iteration 17, loss = 0.45575204\n",
      "Iteration 18, loss = 0.45451193\n",
      "Iteration 19, loss = 0.45501865\n",
      "Iteration 20, loss = 0.45495176\n",
      "Iteration 21, loss = 0.46181110\n",
      "Iteration 22, loss = 0.45991734\n",
      "Iteration 23, loss = 0.45264127\n",
      "Iteration 24, loss = 0.45447191\n",
      "Iteration 25, loss = 0.45209147\n",
      "Iteration 26, loss = 0.45381981\n",
      "Iteration 27, loss = 0.45109160\n",
      "Iteration 28, loss = 0.45073765\n",
      "Iteration 29, loss = 0.45851194\n",
      "Iteration 30, loss = 0.45414755\n",
      "Iteration 31, loss = 0.45331176\n",
      "Iteration 32, loss = 0.45160682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 33, loss = 0.45587781\n",
      "Iteration 34, loss = 0.45514012\n",
      "Iteration 35, loss = 0.45413343\n",
      "Iteration 36, loss = 0.45193486\n",
      "Iteration 37, loss = 0.45057410\n",
      "Iteration 38, loss = 0.45646250\n",
      "Iteration 39, loss = 0.45563439\n",
      "Iteration 40, loss = 0.45408338\n",
      "Iteration 41, loss = 0.45411517\n",
      "Iteration 42, loss = 0.45277804\n",
      "Iteration 43, loss = 0.45998604\n",
      "Iteration 44, loss = 0.45338102\n",
      "Iteration 45, loss = 0.45229501\n",
      "Iteration 46, loss = 0.44925991\n",
      "Iteration 47, loss = 0.45394230\n",
      "Iteration 48, loss = 0.45367793\n",
      "Iteration 49, loss = 0.45472447\n",
      "Iteration 50, loss = 0.45591122\n",
      "Iteration 1, loss = 0.67364563\n",
      "Iteration 2, loss = 0.58875406\n",
      "Iteration 3, loss = 0.49275946\n",
      "Iteration 4, loss = 0.46649762\n",
      "Iteration 5, loss = 0.47197240\n",
      "Iteration 6, loss = 0.46295475\n",
      "Iteration 7, loss = 0.46519653\n",
      "Iteration 8, loss = 0.46653215\n",
      "Iteration 9, loss = 0.45862581\n",
      "Iteration 10, loss = 0.45876205\n",
      "Iteration 11, loss = 0.46003331\n",
      "Iteration 12, loss = 0.46177102\n",
      "Iteration 13, loss = 0.46102675\n",
      "Iteration 14, loss = 0.45725598\n",
      "Iteration 15, loss = 0.45604653\n",
      "Iteration 16, loss = 0.46278233\n",
      "Iteration 17, loss = 0.46047216\n",
      "Iteration 18, loss = 0.45533078\n",
      "Iteration 19, loss = 0.46157554\n",
      "Iteration 20, loss = 0.45716474\n",
      "Iteration 21, loss = 0.46815280\n",
      "Iteration 22, loss = 0.45832209\n",
      "Iteration 23, loss = 0.45999149\n",
      "Iteration 24, loss = 0.45953617\n",
      "Iteration 25, loss = 0.45665152\n",
      "Iteration 26, loss = 0.45717701\n",
      "Iteration 27, loss = 0.45776670\n",
      "Iteration 28, loss = 0.45896708\n",
      "Iteration 29, loss = 0.45615073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67520151\n",
      "Iteration 2, loss = 0.58570323\n",
      "Iteration 3, loss = 0.48836932\n",
      "Iteration 4, loss = 0.46135289\n",
      "Iteration 5, loss = 0.46701610\n",
      "Iteration 6, loss = 0.45859064\n",
      "Iteration 7, loss = 0.45719773\n",
      "Iteration 8, loss = 0.46237050\n",
      "Iteration 9, loss = 0.46354861\n",
      "Iteration 10, loss = 0.45726099\n",
      "Iteration 11, loss = 0.45631294\n",
      "Iteration 12, loss = 0.45954987\n",
      "Iteration 13, loss = 0.46523825\n",
      "Iteration 14, loss = 0.45415580\n",
      "Iteration 15, loss = 0.46551013\n",
      "Iteration 16, loss = 0.45900051\n",
      "Iteration 17, loss = 0.45830472\n",
      "Iteration 18, loss = 0.46209583\n",
      "Iteration 19, loss = 0.45721988\n",
      "Iteration 20, loss = 0.45522006\n",
      "Iteration 21, loss = 0.45684894\n",
      "Iteration 22, loss = 0.45960971\n",
      "Iteration 23, loss = 0.45794930\n",
      "Iteration 24, loss = 0.45354231\n",
      "Iteration 25, loss = 0.46210385\n",
      "Iteration 26, loss = 0.45886181\n",
      "Iteration 27, loss = 0.45462151\n",
      "Iteration 28, loss = 0.45697655\n",
      "Iteration 29, loss = 0.45674736\n",
      "Iteration 30, loss = 0.46052151\n",
      "Iteration 31, loss = 0.45693027\n",
      "Iteration 32, loss = 0.45392047\n",
      "Iteration 33, loss = 0.45834004\n",
      "Iteration 34, loss = 0.45423644\n",
      "Iteration 35, loss = 0.45199435\n",
      "Iteration 36, loss = 0.46356027\n",
      "Iteration 37, loss = 0.45989525\n",
      "Iteration 38, loss = 0.45480774\n",
      "Iteration 39, loss = 0.45705532\n",
      "Iteration 40, loss = 0.45976347\n",
      "Iteration 41, loss = 0.45664971\n",
      "Iteration 42, loss = 0.45371449\n",
      "Iteration 43, loss = 0.45793985\n",
      "Iteration 44, loss = 0.45901762\n",
      "Iteration 45, loss = 0.45465071\n",
      "Iteration 46, loss = 0.45427794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67395247\n",
      "Iteration 2, loss = 0.58767293\n",
      "Iteration 3, loss = 0.49051754\n",
      "Iteration 4, loss = 0.46554346\n",
      "Iteration 5, loss = 0.46308465\n",
      "Iteration 6, loss = 0.45977138\n",
      "Iteration 7, loss = 0.47544118\n",
      "Iteration 8, loss = 0.46126292\n",
      "Iteration 9, loss = 0.45853828\n",
      "Iteration 10, loss = 0.46559199\n",
      "Iteration 11, loss = 0.45943653\n",
      "Iteration 12, loss = 0.45831666\n",
      "Iteration 13, loss = 0.46128228\n",
      "Iteration 14, loss = 0.45927114\n",
      "Iteration 15, loss = 0.45965407\n",
      "Iteration 16, loss = 0.46611426\n",
      "Iteration 17, loss = 0.45894789\n",
      "Iteration 18, loss = 0.45913021\n",
      "Iteration 19, loss = 0.45650470\n",
      "Iteration 20, loss = 0.46693729\n",
      "Iteration 21, loss = 0.45574410\n",
      "Iteration 22, loss = 0.45737365\n",
      "Iteration 23, loss = 0.45959994\n",
      "Iteration 24, loss = 0.45710804\n",
      "Iteration 25, loss = 0.45815314\n",
      "Iteration 26, loss = 0.45868087\n",
      "Iteration 27, loss = 0.45481800\n",
      "Iteration 28, loss = 0.45714575\n",
      "Iteration 29, loss = 0.45873190\n",
      "Iteration 30, loss = 0.45945310\n",
      "Iteration 31, loss = 0.45719932\n",
      "Iteration 32, loss = 0.45568366\n",
      "Iteration 33, loss = 0.45786606\n",
      "Iteration 34, loss = 0.45634412\n",
      "Iteration 35, loss = 0.45519914\n",
      "Iteration 36, loss = 0.45979332\n",
      "Iteration 37, loss = 0.45736898\n",
      "Iteration 38, loss = 0.45944706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Train Accuracy: 0.7033\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 14.41 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_MLP_DS3_tfidf_3, memory_MLP_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_MLP, SKF)\n",
    "cm_MLP_DS3_tfidf_3, acc_MLP_DS3_tfidf_3, pre_MLP_DS3_tfidf_3, rec_MLP_DS3_tfidf_3, f1_MLP_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_MLP)\n",
    "#show_confusion_maxtrix(cm_MLP_DS3_tfidf_3, model_3, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8578\n",
      "Running Memory: 1.82 MB\n",
      "Running Time: 0.67 Seconds\n",
      "Test  Accuracy: 0.8697\n",
      "Test  Presion:  0.8669\n",
      "Test  Recall:   0.8939\n",
      "Test  F1-score: 0.8802\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS3_bow_1, memory_KNN_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS3_bow_1, acc_KNN_DS3_bow_1, pre_KNN_DS3_bow_1, rec_KNN_DS3_bow_1, f1_KNN_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS3_bow_1, model_4, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7918\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.77 Seconds\n",
      "Test  Accuracy: 0.7310\n",
      "Test  Presion:  0.8862\n",
      "Test  Recall:   0.5710\n",
      "Test  F1-score: 0.6945\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS3_bow_2, memory_KNN_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS3_bow_2, acc_KNN_DS3_bow_2, pre_KNN_DS3_bow_2, rec_KNN_DS3_bow_2, f1_KNN_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS3_bow_2, model_4, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6775\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.73 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  0.9965\n",
      "Test  Recall:   0.4399\n",
      "Test  F1-score: 0.6104\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS3_bow_3, memory_KNN_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS3_bow_3, acc_KNN_DS3_bow_3, pre_KNN_DS3_bow_3, rec_KNN_DS3_bow_3, f1_KNN_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS3_bow_3, model_4, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8657\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.88 Seconds\n",
      "Test  Accuracy: 0.8789\n",
      "Test  Presion:  0.8746\n",
      "Test  Recall:   0.9033\n",
      "Test  F1-score: 0.8887\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS3_tfidf_1, memory_KNN_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS3_tfidf_1, acc_KNN_DS3_tfidf_1, pre_KNN_DS3_tfidf_1, rec_KNN_DS3_tfidf_1, f1_KNN_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS3_tfidf_1, model_4, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7926\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.74 Seconds\n",
      "Test  Accuracy: 0.7293\n",
      "Test  Presion:  0.8838\n",
      "Test  Recall:   0.5694\n",
      "Test  F1-score: 0.6926\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS3_tfidf_2, memory_KNN_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS3_tfidf_2, acc_KNN_DS3_tfidf_2, pre_KNN_DS3_tfidf_2, rec_KNN_DS3_tfidf_2, f1_KNN_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS3_tfidf_2, model_4, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6786\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.75 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_KNN_DS3_tfidf_3, memory_KNN_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_KNN, SKF)\n",
    "cm_KNN_DS3_tfidf_3, acc_KNN_DS3_tfidf_3, pre_KNN_DS3_tfidf_3, rec_KNN_DS3_tfidf_3, f1_KNN_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_KNN)\n",
    "#show_confusion_maxtrix(cm_KNN_DS3_tfidf_3, model_4, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9283\n",
      "Running Memory: 0.20 MB\n",
      "Running Time: 9.14 Seconds\n",
      "Test  Accuracy: 0.9165\n",
      "Test  Presion:  0.9260\n",
      "Test  Recall:   0.9173\n",
      "Test  F1-score: 0.9216\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS3_bow_1, memory_RF_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS3_bow_1, acc_RF_DS3_bow_1, pre_RF_DS3_bow_1, rec_RF_DS3_bow_1, f1_RF_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS3_bow_1, model_5, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8373\n",
      "Running Memory: 0.05 MB\n",
      "Running Time: 6.09 Seconds\n",
      "Test  Accuracy: 0.8379\n",
      "Test  Presion:  0.7922\n",
      "Test  Recall:   0.9454\n",
      "Test  F1-score: 0.8620\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS3_bow_2, memory_RF_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS3_bow_2, acc_RF_DS3_bow_2, pre_RF_DS3_bow_2, rec_RF_DS3_bow_2, f1_RF_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS3_bow_2, model_5, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7055\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.87 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  0.9965\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6089\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS3_bow_3, memory_RF_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS3_bow_3, acc_RF_DS3_bow_3, pre_RF_DS3_bow_3, rec_RF_DS3_bow_3, f1_RF_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS3_bow_3, model_5, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9266\n",
      "Running Memory: 0.18 MB\n",
      "Running Time: 12.39 Seconds\n",
      "Test  Accuracy: 0.9181\n",
      "Test  Presion:  0.9303\n",
      "Test  Recall:   0.9158\n",
      "Test  F1-score: 0.9230\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS3_tfidf_1, memory_RF_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS3_tfidf_1, acc_RF_DS3_tfidf_1, pre_RF_DS3_tfidf_1, rec_RF_DS3_tfidf_1, f1_RF_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS3_tfidf_1, model_5, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8355\n",
      "Running Memory: 0.09 MB\n",
      "Running Time: 6.90 Seconds\n",
      "Test  Accuracy: 0.8404\n",
      "Test  Presion:  0.8000\n",
      "Test  Recall:   0.9360\n",
      "Test  F1-score: 0.8627\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS3_tfidf_2, memory_RF_DS3_tfidf_2 = time_RF_DS3_tfidf_2, memory_RF_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS3_tfidf_2, acc_RF_DS3_tfidf_2, pre_RF_DS3_tfidf_2, rec_RF_DS3_tfidf_2, f1_RF_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS3_tfidf_2, model_5, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.86 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_RF_DS3_tfidf_3, memory_RF_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_RF, SKF)\n",
    "cm_RF_DS3_tfidf_3, acc_RF_DS3_tfidf_3, pre_RF_DS3_tfidf_3, rec_RF_DS3_tfidf_3, f1_RF_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_RF)\n",
    "#show_confusion_maxtrix(cm_RF_DS3_tfidf_3, model_5, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier (RF,LR,KNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9094\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 11.58 Seconds\n",
      "Test  Accuracy: 0.9064\n",
      "Test  Presion:  0.8989\n",
      "Test  Recall:   0.9298\n",
      "Test  F1-score: 0.9141\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS3_bow_1, memory_VOT_1_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS3_bow_1, acc_VOT_1_DS3_bow_1, pre_VOT_1_DS3_bow_1, rec_VOT_1_DS3_bow_1, f1_VOT_1_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS3_bow_1, model_6, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8305\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 10.42 Seconds\n",
      "Test  Accuracy: 0.8137\n",
      "Test  Presion:  0.7743\n",
      "Test  Recall:   0.9204\n",
      "Test  F1-score: 0.8411\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS3_bow_2, memory_VOT_1_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS3_bow_2, acc_VOT_1_DS3_bow_2, pre_VOT_1_DS3_bow_2, rec_VOT_1_DS3_bow_2, f1_VOT_1_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS3_bow_2, model_6, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7048\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 8.05 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  0.9965\n",
      "Test  Recall:   0.4399\n",
      "Test  F1-score: 0.6104\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS3_bow_3, memory_VOT_1_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS3_bow_3, acc_VOT_1_DS3_bow_3, pre_VOT_1_DS3_bow_3, rec_VOT_1_DS3_bow_3, f1_VOT_1_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS3_bow_3, model_6, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9004\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 16.36 Seconds\n",
      "Test  Accuracy: 0.9073\n",
      "Test  Presion:  0.9115\n",
      "Test  Recall:   0.9158\n",
      "Test  F1-score: 0.9136\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS3_tfidf_1, memory_VOT_1_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS3_tfidf_1, acc_VOT_1_DS3_tfidf_1, pre_VOT_1_DS3_tfidf_1, rec_VOT_1_DS3_tfidf_1, f1_VOT_1_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS3_tfidf_1, model_6, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8309\n",
      "Running Memory: 0.03 MB\n",
      "Running Time: 9.74 Seconds\n",
      "Test  Accuracy: 0.8221\n",
      "Test  Presion:  0.7932\n",
      "Test  Recall:   0.9033\n",
      "Test  F1-score: 0.8446\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS3_tfidf_2, memory_VOT_1_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS3_tfidf_2, acc_VOT_1_DS3_tfidf_2, pre_VOT_1_DS3_tfidf_2, rec_VOT_1_DS3_tfidf_2, f1_VOT_1_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS3_tfidf_2, model_6, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 9.04 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_VOT_1_DS3_tfidf_3, memory_VOT_1_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_VOT_1, SKF)\n",
    "cm_VOT_1_DS3_tfidf_3, acc_VOT_1_DS3_tfidf_3, pre_VOT_1_DS3_tfidf_3, rec_VOT_1_DS3_tfidf_3, f1_VOT_1_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_VOT_1)\n",
    "#show_confusion_maxtrix(cm_VOT_1_DS3_tfidf_3, model_6, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting classifier (LR, LSVM, CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8699\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.66 Seconds\n",
      "Test  Accuracy: 0.8638\n",
      "Test  Presion:  0.8319\n",
      "Test  Recall:   0.9345\n",
      "Test  F1-score: 0.8802\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS3_bow_1, memory_VOT_2_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS3_bow_1, acc_VOT_2_DS3_bow_1, pre_VOT_2_DS3_bow_1, rec_VOT_2_DS3_bow_1, f1_VOT_2_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS3_bow_1, model_7, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8244\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 3.19 Seconds\n",
      "Test  Accuracy: 0.8262\n",
      "Test  Presion:  0.7852\n",
      "Test  Recall:   0.9298\n",
      "Test  F1-score: 0.8514\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS3_bow_2, memory_VOT_2_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS3_bow_2, acc_VOT_2_DS3_bow_2, pre_VOT_2_DS3_bow_2, rec_VOT_2_DS3_bow_2, f1_VOT_2_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS3_bow_2, model_7, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 2.53 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS3_bow_3, memory_VOT_2_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS3_bow_3, acc_VOT_2_DS3_bow_3, pre_VOT_2_DS3_bow_3, rec_VOT_2_DS3_bow_3, f1_VOT_2_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS3_bow_3, model_7, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8474\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 2.56 Seconds\n",
      "Test  Accuracy: 0.8613\n",
      "Test  Presion:  0.8776\n",
      "Test  Recall:   0.8612\n",
      "Test  F1-score: 0.8693\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS3_tfidf_1, memory_VOT_2_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS3_tfidf_1, acc_VOT_2_DS3_tfidf_1, pre_VOT_2_DS3_tfidf_1, rec_VOT_2_DS3_tfidf_1, f1_VOT_2_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS3_tfidf_1, model_7, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8173\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 1.29 Seconds\n",
      "Test  Accuracy: 0.8087\n",
      "Test  Presion:  0.7784\n",
      "Test  Recall:   0.8986\n",
      "Test  F1-score: 0.8342\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS3_tfidf_2, memory_VOT_2_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS3_tfidf_2, acc_VOT_2_DS3_tfidf_2, pre_VOT_2_DS3_tfidf_2, rec_VOT_2_DS3_tfidf_2, f1_VOT_2_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS3_tfidf_2, model_7, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 0.96 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_VOT_2_DS3_tfidf_3, memory_VOT_2_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_VOT_2, SKF)\n",
    "cm_VOT_2_DS3_tfidf_3, acc_VOT_2_DS3_tfidf_3, pre_VOT_2_DS3_tfidf_3, rec_VOT_2_DS3_tfidf_3, f1_VOT_2_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_VOT_2)\n",
    "#show_confusion_maxtrix(cm_VOT_2_DS3_tfidf_3, model_7, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging classifier (decision trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9176\n",
      "Running Memory: 0.07 MB\n",
      "Running Time: 32.45 Seconds\n",
      "Test  Accuracy: 0.9190\n",
      "Test  Presion:  0.9172\n",
      "Test  Recall:   0.9329\n",
      "Test  F1-score: 0.9250\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS3_bow_1, memory_BAGG_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS3_bow_1, acc_BAGG_DS3_bow_1, pre_BAGG_DS3_bow_1, rec_BAGG_DS3_bow_1, f1_BAGG_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS3_bow_1, model_8, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8359\n",
      "Running Memory: 0.09 MB\n",
      "Running Time: 16.06 Seconds\n",
      "Test  Accuracy: 0.8371\n",
      "Test  Presion:  0.7904\n",
      "Test  Recall:   0.9470\n",
      "Test  F1-score: 0.8616\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS3_bow_2, memory_BAGG_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS3_bow_2, acc_BAGG_DS3_bow_2, pre_BAGG_DS3_bow_2, rec_BAGG_DS3_bow_2, f1_BAGG_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS3_bow_2, model_8, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7055\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 9.89 Seconds\n",
      "Test  Accuracy: 0.6976\n",
      "Test  Presion:  0.9964\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6074\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS3_bow_3, memory_BAGG_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS3_bow_3, acc_BAGG_DS3_bow_3, pre_BAGG_DS3_bow_3, rec_BAGG_DS3_bow_3, f1_BAGG_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS3_bow_3, model_8, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9169\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 78.75 Seconds\n",
      "Test  Accuracy: 0.9140\n",
      "Test  Presion:  0.9256\n",
      "Test  Recall:   0.9126\n",
      "Test  F1-score: 0.9191\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS3_tfidf_1, memory_BAGG_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS3_tfidf_1, acc_BAGG_DS3_tfidf_1, pre_BAGG_DS3_tfidf_1, rec_BAGG_DS3_tfidf_1, f1_BAGG_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS3_tfidf_1, model_8, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8341\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 18.44 Seconds\n",
      "Test  Accuracy: 0.8363\n",
      "Test  Presion:  0.7955\n",
      "Test  Recall:   0.9345\n",
      "Test  F1-score: 0.8594\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS3_tfidf_2, memory_BAGG_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS3_tfidf_2, acc_BAGG_DS3_tfidf_2, pre_BAGG_DS3_tfidf_2, rec_BAGG_DS3_tfidf_2, f1_BAGG_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS3_tfidf_2, model_8, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 9.68 Seconds\n",
      "Test  Accuracy: 0.6984\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4368\n",
      "Test  F1-score: 0.6080\n"
     ]
    }
   ],
   "source": [
    "time_BAGG_DS3_tfidf_3, memory_BAGG_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_BAGG, SKF)\n",
    "cm_BAGG_DS3_tfidf_3, acc_BAGG_DS3_tfidf_3, pre_BAGG_DS3_tfidf_3, rec_BAGG_DS3_tfidf_3, f1_BAGG_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_BAGG)\n",
    "#show_confusion_maxtrix(cm_BAGG_DS3_tfidf_3, model_8, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADABoost (Boosting classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8807\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 6.39 Seconds\n",
      "Test  Accuracy: 0.8864\n",
      "Test  Presion:  0.8686\n",
      "Test  Recall:   0.9282\n",
      "Test  F1-score: 0.8974\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS3_bow_1, memory_ADA_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS3_bow_1, acc_ADA_DS3_bow_1, pre_ADA_DS3_bow_1, rec_ADA_DS3_bow_1, f1_ADA_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS3_bow_1, model_9, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8287\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.43 Seconds\n",
      "Test  Accuracy: 0.8204\n",
      "Test  Presion:  0.7752\n",
      "Test  Recall:   0.9360\n",
      "Test  F1-score: 0.8481\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS3_bow_2, memory_ADA_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS3_bow_2, acc_ADA_DS3_bow_2, pre_ADA_DS3_bow_2, rec_ADA_DS3_bow_2, f1_ADA_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS3_bow_2, model_9, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7055\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.26 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  0.9965\n",
      "Test  Recall:   0.4399\n",
      "Test  F1-score: 0.6104\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS3_bow_3, memory_ADA_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS3_bow_3, acc_ADA_DS3_bow_3, pre_ADA_DS3_bow_3, rec_ADA_DS3_bow_3, f1_ADA_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS3_bow_3, model_9, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8721\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 9.84 Seconds\n",
      "Test  Accuracy: 0.8822\n",
      "Test  Presion:  0.8765\n",
      "Test  Recall:   0.9080\n",
      "Test  F1-score: 0.8920\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS3_tfidf_1, memory_ADA_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS3_tfidf_1, acc_ADA_DS3_tfidf_1, pre_ADA_DS3_tfidf_1, rec_ADA_DS3_tfidf_1, f1_ADA_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS3_tfidf_1, model_9, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8259\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 5.05 Seconds\n",
      "Test  Accuracy: 0.8212\n",
      "Test  Presion:  0.7784\n",
      "Test  Recall:   0.9314\n",
      "Test  F1-score: 0.8480\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS3_tfidf_2, memory_ADA_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS3_tfidf_2, acc_ADA_DS3_tfidf_2, pre_ADA_DS3_tfidf_2, rec_ADA_DS3_tfidf_2, f1_ADA_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS3_tfidf_2, model_9, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.19 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_ADA_DS3_tfidf_3, memory_ADA_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_ADA, SKF)\n",
    "cm_ADA_DS3_tfidf_3, acc_ADA_DS3_tfidf_3, pre_ADA_DS3_tfidf_3, rec_ADA_DS3_tfidf_3, f1_ADA_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_ADA)\n",
    "#show_confusion_maxtrix(cm_ADA_DS3_tfidf_3, model_9, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Boosting classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9273\n",
      "Running Memory: 0.12 MB\n",
      "Running Time: 5.87 Seconds\n",
      "Test  Accuracy: 0.9298\n",
      "Test  Presion:  0.9252\n",
      "Test  Recall:   0.9454\n",
      "Test  F1-score: 0.9352\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS3_bow_1, memory_XG_DS3_bow_1 = train(DS3_bow_1_x_train, DS3_bow_1_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS3_bow_1, acc_XG_DS3_bow_1, pre_XG_DS3_bow_1, rec_XG_DS3_bow_1, f1_XG_DS3_bow_1 = predict(DS3_bow_1_x_test, DS3_bow_1_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS3_bow_1, model_10, method_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8298\n",
      "Running Memory: 0.02 MB\n",
      "Running Time: 4.56 Seconds\n",
      "Test  Accuracy: 0.8304\n",
      "Test  Presion:  0.7905\n",
      "Test  Recall:   0.9298\n",
      "Test  F1-score: 0.8545\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS3_bow_2, memory_XG_DS3_bow_2 = train(DS3_bow_2_x_train, DS3_bow_2_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS3_bow_2, acc_XG_DS3_bow_2, pre_XG_DS3_bow_2, rec_XG_DS3_bow_2, f1_XG_DS3_bow_2 = predict(DS3_bow_2_x_test, DS3_bow_2_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS3_bow_2, model_10, method_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 3.64 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS3_bow_3, memory_XG_DS3_bow_3 = train(DS3_bow_3_x_train, DS3_bow_3_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS3_bow_3, acc_XG_DS3_bow_3, pre_XG_DS3_bow_3, rec_XG_DS3_bow_3, f1_XG_DS3_bow_3 = predict(DS3_bow_3_x_test, DS3_bow_3_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS3_bow_3, model_10, method_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9344\n",
      "Running Memory: 0.02 MB\n",
      "Running Time: 8.78 Seconds\n",
      "Test  Accuracy: 0.9248\n",
      "Test  Presion:  0.9245\n",
      "Test  Recall:   0.9360\n",
      "Test  F1-score: 0.9302\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS3_tfidf_1, memory_XG_DS3_tfidf_1 = train(DS3_tfidf_1_x_train, DS3_tfidf_1_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS3_tfidf_1, acc_XG_DS3_tfidf_1, pre_XG_DS3_tfidf_1, rec_XG_DS3_tfidf_1, f1_XG_DS3_tfidf_1 = predict(DS3_tfidf_1_x_test, DS3_tfidf_1_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS3_tfidf_1, model_10, method_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8341\n",
      "Running Memory: 0.00 MB\n",
      "Running Time: 4.91 Seconds\n",
      "Test  Accuracy: 0.8413\n",
      "Test  Presion:  0.7971\n",
      "Test  Recall:   0.9438\n",
      "Test  F1-score: 0.8643\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS3_tfidf_2, memory_XG_DS3_tfidf_2 = train(DS3_tfidf_2_x_train, DS3_tfidf_2_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS3_tfidf_2, acc_XG_DS3_tfidf_2, pre_XG_DS3_tfidf_2, rec_XG_DS3_tfidf_2, f1_XG_DS3_tfidf_2 = predict(DS3_tfidf_2_x_test, DS3_tfidf_2_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS3_tfidf_2, model_10, method_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7044\n",
      "Running Memory: 0.01 MB\n",
      "Running Time: 3.16 Seconds\n",
      "Test  Accuracy: 0.6992\n",
      "Test  Presion:  1.0000\n",
      "Test  Recall:   0.4384\n",
      "Test  F1-score: 0.6095\n"
     ]
    }
   ],
   "source": [
    "time_XG_DS3_tfidf_3, memory_XG_DS3_tfidf_3 = train(DS3_tfidf_3_x_train, DS3_tfidf_3_y_train, classifier_XG, SKF)\n",
    "cm_XG_DS3_tfidf_3, acc_XG_DS3_tfidf_3, pre_XG_DS3_tfidf_3, rec_XG_DS3_tfidf_3, f1_XG_DS3_tfidf_3 = predict(DS3_tfidf_3_x_test, DS3_tfidf_3_y_test, classifier_XG)\n",
    "#show_confusion_maxtrix(cm_XG_DS3_tfidf_3, model_10, method_6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_LR_Accuracy  = [ acc_LR_DS1_bow_1, acc_LR_DS1_bow_2, acc_LR_DS1_bow_3, acc_LR_DS1_tfidf_1, acc_LR_DS1_tfidf_2, acc_LR_DS1_tfidf_3 ]\n",
    "DS1_LR_Precision = [ pre_LR_DS1_bow_1, pre_LR_DS1_bow_2, pre_LR_DS1_bow_3, pre_LR_DS1_tfidf_1, pre_LR_DS1_tfidf_2, pre_LR_DS1_tfidf_3 ]\n",
    "DS1_LR_Recall    = [ rec_LR_DS1_bow_1, rec_LR_DS1_bow_2, rec_LR_DS1_bow_3, rec_LR_DS1_tfidf_1, rec_LR_DS1_tfidf_2, rec_LR_DS1_tfidf_3 ]\n",
    "DS1_LR_F1_score  = [ f1_LR_DS1_bow_1,  f1_LR_DS1_bow_2,  f1_LR_DS1_bow_3,  f1_LR_DS1_tfidf_1,  f1_LR_DS1_tfidf_2,  f1_LR_DS1_tfidf_3  ]\n",
    "DS1_LR_time      = [ time_LR_DS1_bow_1,time_LR_DS1_bow_2,time_LR_DS1_bow_3,time_LR_DS1_tfidf_1,time_LR_DS1_tfidf_2,time_LR_DS1_tfidf_3]\n",
    "DS1_LR_memory    = [ memory_LR_DS1_bow_1, memory_LR_DS1_bow_2, memory_LR_DS1_bow_3, memory_LR_DS1_tfidf_1, memory_LR_DS1_tfidf_2, memory_LR_DS1_tfidf_3]\n",
    "DS1_LR           = [ DS1_LR_Accuracy, DS1_LR_Precision, DS1_LR_Recall, DS1_LR_F1_score, DS1_LR_time, DS1_LR_memory]\n",
    "\n",
    "DS1_LSVM_Accuracy  = [ acc_LSVM_DS1_bow_1, acc_LSVM_DS1_bow_2, acc_LSVM_DS1_bow_3, acc_LSVM_DS1_tfidf_1, acc_LSVM_DS1_tfidf_2, acc_LSVM_DS1_tfidf_3 ]\n",
    "DS1_LSVM_Precision = [ pre_LSVM_DS1_bow_1, pre_LSVM_DS1_bow_2, pre_LSVM_DS1_bow_3, pre_LSVM_DS1_tfidf_1, pre_LSVM_DS1_tfidf_2, pre_LSVM_DS1_tfidf_3 ]\n",
    "DS1_LSVM_Recall    = [ rec_LSVM_DS1_bow_1, rec_LSVM_DS1_bow_2, rec_LSVM_DS1_bow_3, rec_LSVM_DS1_tfidf_1, rec_LSVM_DS1_tfidf_2, rec_LSVM_DS1_tfidf_3 ]\n",
    "DS1_LSVM_F1_score  = [ f1_LSVM_DS1_bow_1,  f1_LSVM_DS1_bow_2,  f1_LSVM_DS1_bow_3,  f1_LSVM_DS1_tfidf_1,  f1_LSVM_DS1_tfidf_2,  f1_LSVM_DS1_tfidf_3  ]\n",
    "DS1_LSVM_time      = [ time_LSVM_DS1_bow_1,time_LSVM_DS1_bow_2,time_LSVM_DS1_bow_3,time_LSVM_DS1_tfidf_1,time_LSVM_DS1_tfidf_2,time_LSVM_DS1_tfidf_3]\n",
    "DS1_LSVM_memory    = [ memory_LSVM_DS1_bow_1, memory_LSVM_DS1_bow_2, memory_LSVM_DS1_bow_3, memory_LSVM_DS1_tfidf_1, memory_LSVM_DS1_tfidf_2, memory_LSVM_DS1_tfidf_3]\n",
    "DS1_LSVM           = [ DS1_LSVM_Accuracy, DS1_LSVM_Precision, DS1_LSVM_Recall, DS1_LSVM_F1_score, DS1_LSVM_time, DS1_LSVM_memory]\n",
    "\n",
    "DS1_MLP_Accuracy  = [ acc_MLP_DS1_bow_1, acc_MLP_DS1_bow_2, acc_MLP_DS1_bow_3, acc_MLP_DS1_tfidf_1, acc_MLP_DS1_tfidf_2, acc_MLP_DS1_tfidf_3 ]\n",
    "DS1_MLP_Precision = [ pre_MLP_DS1_bow_1, pre_MLP_DS1_bow_2, pre_MLP_DS1_bow_3, pre_MLP_DS1_tfidf_1, pre_MLP_DS1_tfidf_2, pre_MLP_DS1_tfidf_3 ]\n",
    "DS1_MLP_Recall    = [ rec_MLP_DS1_bow_1, rec_MLP_DS1_bow_2, rec_MLP_DS1_bow_3, rec_MLP_DS1_tfidf_1, rec_MLP_DS1_tfidf_2, rec_MLP_DS1_tfidf_3 ]\n",
    "DS1_MLP_F1_score  = [ f1_MLP_DS1_bow_1,  f1_MLP_DS1_bow_2,  f1_MLP_DS1_bow_3,  f1_MLP_DS1_tfidf_1,  f1_MLP_DS1_tfidf_2,  f1_MLP_DS1_tfidf_3  ]\n",
    "DS1_MLP_time      = [ time_MLP_DS1_bow_1,time_MLP_DS1_bow_2,time_MLP_DS1_bow_3,time_MLP_DS1_tfidf_1,time_MLP_DS1_tfidf_2,time_MLP_DS1_tfidf_3]\n",
    "DS1_MLP_memory    = [ memory_MLP_DS1_bow_1, memory_MLP_DS1_bow_2, memory_MLP_DS1_bow_3, memory_MLP_DS1_tfidf_1, memory_MLP_DS1_tfidf_2, memory_MLP_DS1_tfidf_3]\n",
    "DS1_MLP           = [ DS1_MLP_Accuracy, DS1_MLP_Precision, DS1_MLP_Recall, DS1_MLP_F1_score, DS1_MLP_time, DS1_MLP_memory]\n",
    "\n",
    "DS1_KNN_Accuracy  = [ acc_KNN_DS1_bow_1, acc_KNN_DS1_bow_2, acc_KNN_DS1_bow_3, acc_KNN_DS1_tfidf_1, acc_KNN_DS1_tfidf_2, acc_KNN_DS1_tfidf_3 ]\n",
    "DS1_KNN_Precision = [ pre_KNN_DS1_bow_1, pre_KNN_DS1_bow_2, pre_KNN_DS1_bow_3, pre_KNN_DS1_tfidf_1, pre_KNN_DS1_tfidf_2, pre_KNN_DS1_tfidf_3 ]\n",
    "DS1_KNN_Recall    = [ rec_KNN_DS1_bow_1, rec_KNN_DS1_bow_2, rec_KNN_DS1_bow_3, rec_KNN_DS1_tfidf_1, rec_KNN_DS1_tfidf_2, rec_KNN_DS1_tfidf_3 ]\n",
    "DS1_KNN_F1_score  = [ f1_KNN_DS1_bow_1,  f1_KNN_DS1_bow_2,  f1_KNN_DS1_bow_3,  f1_KNN_DS1_tfidf_1,  f1_KNN_DS1_tfidf_2,  f1_KNN_DS1_tfidf_3  ]\n",
    "DS1_KNN_time      = [ time_KNN_DS1_bow_1,time_KNN_DS1_bow_2,time_KNN_DS1_bow_3,time_KNN_DS1_tfidf_1,time_KNN_DS1_tfidf_2,time_KNN_DS1_tfidf_3]\n",
    "DS1_KNN_memory    = [ memory_KNN_DS1_bow_1, memory_KNN_DS1_bow_2, memory_KNN_DS1_bow_3, memory_KNN_DS1_tfidf_1, memory_KNN_DS1_tfidf_2, memory_KNN_DS1_tfidf_3]\n",
    "DS1_KNN           = [ DS1_KNN_Accuracy, DS1_KNN_Precision, DS1_KNN_Recall, DS1_KNN_F1_score, DS1_KNN_time, DS1_KNN_memory]\n",
    "\n",
    "DS1_RF_Accuracy  = [ acc_RF_DS1_bow_1, acc_RF_DS1_bow_2, acc_RF_DS1_bow_3, acc_RF_DS1_tfidf_1, acc_RF_DS1_tfidf_2, acc_RF_DS1_tfidf_3 ]\n",
    "DS1_RF_Precision = [ pre_RF_DS1_bow_1, pre_RF_DS1_bow_2, pre_RF_DS1_bow_3, pre_RF_DS1_tfidf_1, pre_RF_DS1_tfidf_2, pre_RF_DS1_tfidf_3 ]\n",
    "DS1_RF_Recall    = [ rec_RF_DS1_bow_1, rec_RF_DS1_bow_2, rec_RF_DS1_bow_3, rec_RF_DS1_tfidf_1, rec_RF_DS1_tfidf_2, rec_RF_DS1_tfidf_3 ]\n",
    "DS1_RF_F1_score  = [ f1_RF_DS1_bow_1,  f1_RF_DS1_bow_2,  f1_RF_DS1_bow_3,  f1_RF_DS1_tfidf_1,  f1_RF_DS1_tfidf_2,  f1_RF_DS1_tfidf_3  ]\n",
    "DS1_RF_time      = [ time_RF_DS1_bow_1,time_RF_DS1_bow_2,time_RF_DS1_bow_3,time_RF_DS1_tfidf_1,time_RF_DS1_tfidf_2,time_RF_DS1_tfidf_3]\n",
    "DS1_RF_memory    = [ memory_RF_DS1_bow_1, memory_RF_DS1_bow_2, memory_RF_DS1_bow_3, memory_RF_DS1_tfidf_1, memory_RF_DS1_tfidf_2, memory_RF_DS1_tfidf_3]\n",
    "DS1_RF           = [ DS1_RF_Accuracy, DS1_RF_Precision, DS1_RF_Recall, DS1_RF_F1_score, DS1_RF_time, DS1_RF_memory]\n",
    "\n",
    "DS1_VOT_1_Accuracy  = [ acc_VOT_1_DS1_bow_1, acc_VOT_1_DS1_bow_2, acc_VOT_1_DS1_bow_3, acc_VOT_1_DS1_tfidf_1, acc_VOT_1_DS1_tfidf_2, acc_VOT_1_DS1_tfidf_3 ]\n",
    "DS1_VOT_1_Precision = [ pre_VOT_1_DS1_bow_1, pre_VOT_1_DS1_bow_2, pre_VOT_1_DS1_bow_3, pre_VOT_1_DS1_tfidf_1, pre_VOT_1_DS1_tfidf_2, pre_VOT_1_DS1_tfidf_3 ]\n",
    "DS1_VOT_1_Recall    = [ rec_VOT_1_DS1_bow_1, rec_VOT_1_DS1_bow_2, rec_VOT_1_DS1_bow_3, rec_VOT_1_DS1_tfidf_1, rec_VOT_1_DS1_tfidf_2, rec_VOT_1_DS1_tfidf_3 ]\n",
    "DS1_VOT_1_F1_score  = [ f1_VOT_1_DS1_bow_1,  f1_VOT_1_DS1_bow_2,  f1_VOT_1_DS1_bow_3,  f1_VOT_1_DS1_tfidf_1,  f1_VOT_1_DS1_tfidf_2,  f1_VOT_1_DS1_tfidf_3  ]\n",
    "DS1_VOT_1_time      = [ time_VOT_1_DS1_bow_1,time_VOT_1_DS1_bow_2,time_VOT_1_DS1_bow_3,time_VOT_1_DS1_tfidf_1,time_VOT_1_DS1_tfidf_2,time_VOT_1_DS1_tfidf_3]\n",
    "DS1_VOT_1_memory    = [ memory_VOT_1_DS1_bow_1, memory_VOT_1_DS1_bow_2, memory_VOT_1_DS1_bow_3, memory_VOT_1_DS1_tfidf_1, memory_VOT_1_DS1_tfidf_2, memory_VOT_1_DS1_tfidf_3]\n",
    "DS1_VOT_1           = [ DS1_VOT_1_Accuracy, DS1_VOT_1_Precision, DS1_VOT_1_Recall, DS1_VOT_1_F1_score, DS1_VOT_1_time, DS1_VOT_1_memory]\n",
    "\n",
    "DS1_VOT_2_Accuracy  = [ acc_VOT_2_DS1_bow_1, acc_VOT_2_DS1_bow_2, acc_VOT_2_DS1_bow_3, acc_VOT_2_DS1_tfidf_1, acc_VOT_2_DS1_tfidf_2, acc_VOT_2_DS1_tfidf_3 ]\n",
    "DS1_VOT_2_Precision = [ pre_VOT_2_DS1_bow_1, pre_VOT_2_DS1_bow_2, pre_VOT_2_DS1_bow_3, pre_VOT_2_DS1_tfidf_1, pre_VOT_2_DS1_tfidf_2, pre_VOT_2_DS1_tfidf_3 ]\n",
    "DS1_VOT_2_Recall    = [ rec_VOT_2_DS1_bow_1, rec_VOT_2_DS1_bow_2, rec_VOT_2_DS1_bow_3, rec_VOT_2_DS1_tfidf_1, rec_VOT_2_DS1_tfidf_2, rec_VOT_2_DS1_tfidf_3 ]\n",
    "DS1_VOT_2_F1_score  = [ f1_VOT_2_DS1_bow_1,  f1_VOT_2_DS1_bow_2,  f1_VOT_2_DS1_bow_3,  f1_VOT_2_DS1_tfidf_1,  f1_VOT_2_DS1_tfidf_2,  f1_VOT_2_DS1_tfidf_3  ]\n",
    "DS1_VOT_2_time      = [ time_VOT_2_DS1_bow_1,time_VOT_2_DS1_bow_2,time_VOT_2_DS1_bow_3,time_VOT_2_DS1_tfidf_1,time_VOT_2_DS1_tfidf_2,time_VOT_2_DS1_tfidf_3]\n",
    "DS1_VOT_2_memory    = [ memory_VOT_2_DS1_bow_1, memory_VOT_2_DS1_bow_2, memory_VOT_2_DS1_bow_3, memory_VOT_2_DS1_tfidf_1, memory_VOT_2_DS1_tfidf_2, memory_VOT_2_DS1_tfidf_3]\n",
    "DS1_VOT_2           = [ DS1_VOT_2_Accuracy, DS1_VOT_2_Precision, DS1_VOT_2_Recall, DS1_VOT_2_F1_score, DS1_VOT_2_time, DS1_VOT_2_memory]\n",
    "\n",
    "DS1_BAGG_Accuracy  = [ acc_BAGG_DS1_bow_1, acc_BAGG_DS1_bow_2, acc_BAGG_DS1_bow_3, acc_BAGG_DS1_tfidf_1, acc_BAGG_DS1_tfidf_2, acc_BAGG_DS1_tfidf_3 ]\n",
    "DS1_BAGG_Precision = [ pre_BAGG_DS1_bow_1, pre_BAGG_DS1_bow_2, pre_BAGG_DS1_bow_3, pre_BAGG_DS1_tfidf_1, pre_BAGG_DS1_tfidf_2, pre_BAGG_DS1_tfidf_3 ]\n",
    "DS1_BAGG_Recall    = [ rec_BAGG_DS1_bow_1, rec_BAGG_DS1_bow_2, rec_BAGG_DS1_bow_3, rec_BAGG_DS1_tfidf_1, rec_BAGG_DS1_tfidf_2, rec_BAGG_DS1_tfidf_3 ]\n",
    "DS1_BAGG_F1_score  = [ f1_BAGG_DS1_bow_1,  f1_BAGG_DS1_bow_2,  f1_BAGG_DS1_bow_3,  f1_BAGG_DS1_tfidf_1,  f1_BAGG_DS1_tfidf_2,  f1_BAGG_DS1_tfidf_3  ]\n",
    "DS1_BAGG_time      = [ time_BAGG_DS1_bow_1,time_BAGG_DS1_bow_2,time_BAGG_DS1_bow_3,time_BAGG_DS1_tfidf_1,time_BAGG_DS1_tfidf_2,time_BAGG_DS1_tfidf_3]\n",
    "DS1_BAGG_memory    = [ memory_BAGG_DS1_bow_1, memory_BAGG_DS1_bow_2, memory_BAGG_DS1_bow_3, memory_BAGG_DS1_tfidf_1, memory_BAGG_DS1_tfidf_2, memory_BAGG_DS1_tfidf_3]\n",
    "DS1_BAGG           = [ DS1_BAGG_Accuracy, DS1_BAGG_Precision, DS1_BAGG_Recall, DS1_BAGG_F1_score, DS1_BAGG_time, DS1_BAGG_memory]\n",
    "\n",
    "DS1_ADA_Accuracy  = [ acc_ADA_DS1_bow_1, acc_ADA_DS1_bow_2, acc_ADA_DS1_bow_3, acc_ADA_DS1_tfidf_1, acc_ADA_DS1_tfidf_2, acc_ADA_DS1_tfidf_3 ]\n",
    "DS1_ADA_Precision = [ pre_ADA_DS1_bow_1, pre_ADA_DS1_bow_2, pre_ADA_DS1_bow_3, pre_ADA_DS1_tfidf_1, pre_ADA_DS1_tfidf_2, pre_ADA_DS1_tfidf_3 ]\n",
    "DS1_ADA_Recall    = [ rec_ADA_DS1_bow_1, rec_ADA_DS1_bow_2, rec_ADA_DS1_bow_3, rec_ADA_DS1_tfidf_1, rec_ADA_DS1_tfidf_2, rec_ADA_DS1_tfidf_3 ]\n",
    "DS1_ADA_F1_score  = [ f1_ADA_DS1_bow_1,  f1_ADA_DS1_bow_2,  f1_ADA_DS1_bow_3,  f1_ADA_DS1_tfidf_1,  f1_ADA_DS1_tfidf_2,  f1_ADA_DS1_tfidf_3  ]\n",
    "DS1_ADA_time      = [ time_ADA_DS1_bow_1,time_ADA_DS1_bow_2,time_ADA_DS1_bow_3,time_ADA_DS1_tfidf_1,time_ADA_DS1_tfidf_2,time_ADA_DS1_tfidf_3]\n",
    "DS1_ADA_memory    = [ memory_ADA_DS1_bow_1, memory_ADA_DS1_bow_2, memory_ADA_DS1_bow_3, memory_ADA_DS1_tfidf_1, memory_ADA_DS1_tfidf_2, memory_ADA_DS1_tfidf_3]\n",
    "DS1_ADA           = [ DS1_ADA_Accuracy, DS1_ADA_Precision, DS1_ADA_Recall, DS1_ADA_F1_score, DS1_ADA_time, DS1_ADA_memory]\n",
    "\n",
    "DS1_XG_Accuracy  = [ acc_XG_DS1_bow_1, acc_XG_DS1_bow_2, acc_XG_DS1_bow_3, acc_XG_DS1_tfidf_1, acc_XG_DS1_tfidf_2, acc_XG_DS1_tfidf_3 ]\n",
    "DS1_XG_Precision = [ pre_XG_DS1_bow_1, pre_XG_DS1_bow_2, pre_XG_DS1_bow_3, pre_XG_DS1_tfidf_1, pre_XG_DS1_tfidf_2, pre_XG_DS1_tfidf_3 ]\n",
    "DS1_XG_Recall    = [ rec_XG_DS1_bow_1, rec_XG_DS1_bow_2, rec_XG_DS1_bow_3, rec_XG_DS1_tfidf_1, rec_XG_DS1_tfidf_2, rec_XG_DS1_tfidf_3 ]\n",
    "DS1_XG_F1_score  = [ f1_XG_DS1_bow_1,  f1_XG_DS1_bow_2,  f1_XG_DS1_bow_3,  f1_XG_DS1_tfidf_1,  f1_XG_DS1_tfidf_2,  f1_XG_DS1_tfidf_3  ]\n",
    "DS1_XG_time      = [ time_XG_DS1_bow_1,time_XG_DS1_bow_2,time_XG_DS1_bow_3,time_XG_DS1_tfidf_1,time_XG_DS1_tfidf_2,time_XG_DS1_tfidf_3]\n",
    "DS1_XG_memory    = [ memory_XG_DS1_bow_1, memory_XG_DS1_bow_2, memory_XG_DS1_bow_3, memory_XG_DS1_tfidf_1, memory_XG_DS1_tfidf_2, memory_XG_DS1_tfidf_3]\n",
    "DS1_XG           = [ DS1_XG_Accuracy, DS1_XG_Precision, DS1_XG_Recall, DS1_XG_F1_score, DS1_XG_time, DS1_XG_memory]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = ['Accuracy','Precision', 'Recall', 'F1-score', 'Run time (s)', 'Memory (MB)']\n",
    "Columns=[' '+' (BoW) (n=1)',' (BoW) (n=2)', ' (BoW) (n=3)',' (TF-IDF) (n=1)',' (TF-IDF) (n=2)', ' (TF-IDF) (n=3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_LR_df    = pd.DataFrame( DS1_LR,    index=Index, columns=[ 'LR '      + w for w in Columns] ).round(3).T\n",
    "DS1_LSVM_df  = pd.DataFrame( DS1_LSVM,  index=Index, columns=[ 'LSVM '    + w for w in Columns] ).round(3).T\n",
    "DS1_MLP_df   = pd.DataFrame( DS1_MLP,   index=Index, columns=[ 'MLP '     + w for w in Columns] ).round(3).T\n",
    "DS1_KNN_df   = pd.DataFrame( DS1_KNN,   index=Index, columns=[ 'KNN '     + w for w in Columns] ).round(3).T\n",
    "DS1_RF_df    = pd.DataFrame( DS1_RF,    index=Index, columns=[ 'RF '      + w for w in Columns] ).round(3).T\n",
    "DS1_VOT_1_df = pd.DataFrame( DS1_VOT_1, index=Index, columns=[ 'Voting 1' + w for w in Columns] ).round(3).T\n",
    "DS1_VOT_2_df = pd.DataFrame( DS1_VOT_2, index=Index, columns=[ 'Voting 2' + w for w in Columns] ).round(3).T\n",
    "DS1_BAGG_df  = pd.DataFrame( DS1_BAGG,  index=Index, columns=[ 'Bagging'     + w for w in Columns] ).round(3).T\n",
    "DS1_ADA_df   = pd.DataFrame( DS1_ADA,   index=Index, columns=[ 'AdaBoost '+ w for w in Columns] ).round(3).T\n",
    "DS1_XG_df    = pd.DataFrame( DS1_XG,    index=Index, columns=[ 'XGBoost ' + w for w in Columns] ).round(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS1_LR_df.to_csv('DS1_LR_df')\n",
    "DS1_LSVM_df.to_csv('DS1_LSVM_df')\n",
    "DS1_MLP_df.to_csv('DS1_MLP_df')\n",
    "DS1_KNN_df.to_csv('DS1_KNN_df')\n",
    "DS1_RF_df.to_csv('DS1_RF_df')\n",
    "DS1_VOT_1_df.to_csv('DS1_VOT_1_df')\n",
    "DS1_VOT_2_df.to_csv('DS1_VOT_2_df')\n",
    "DS1_BAGG_df.to_csv('DS1_BAGG_df')\n",
    "DS1_ADA_df.to_csv('DS1_ADA_df')\n",
    "DS1_XG_df .to_csv('DS1_XG_df ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_LR_Accuracy  = [ acc_LR_DS2_bow_1, acc_LR_DS2_bow_2, acc_LR_DS2_bow_3, acc_LR_DS2_tfidf_1, acc_LR_DS2_tfidf_2, acc_LR_DS2_tfidf_3 ]\n",
    "DS2_LR_Precision = [ pre_LR_DS2_bow_1, pre_LR_DS2_bow_2, pre_LR_DS2_bow_3, pre_LR_DS2_tfidf_1, pre_LR_DS2_tfidf_2, pre_LR_DS2_tfidf_3 ]\n",
    "DS2_LR_Recall    = [ rec_LR_DS2_bow_1, rec_LR_DS2_bow_2, rec_LR_DS2_bow_3, rec_LR_DS2_tfidf_1, rec_LR_DS2_tfidf_2, rec_LR_DS2_tfidf_3 ]\n",
    "DS2_LR_F1_score  = [ f1_LR_DS2_bow_1,  f1_LR_DS2_bow_2,  f1_LR_DS2_bow_3,  f1_LR_DS2_tfidf_1,  f1_LR_DS2_tfidf_2,  f1_LR_DS2_tfidf_3  ]\n",
    "DS2_LR_time      = [ time_LR_DS2_bow_1,time_LR_DS2_bow_2,time_LR_DS2_bow_3,time_LR_DS2_tfidf_1,time_LR_DS2_tfidf_2,time_LR_DS2_tfidf_3]\n",
    "DS2_LR_memory    = [ memory_LR_DS2_bow_1, memory_LR_DS2_bow_2, memory_LR_DS2_bow_3, memory_LR_DS2_tfidf_1, memory_LR_DS2_tfidf_2, memory_LR_DS2_tfidf_3]\n",
    "DS2_LR           = [ DS2_LR_Accuracy, DS2_LR_Precision, DS2_LR_Recall, DS2_LR_F1_score, DS2_LR_time, DS2_LR_memory]\n",
    "\n",
    "DS2_LSVM_Accuracy  = [ acc_LSVM_DS2_bow_1, acc_LSVM_DS2_bow_2, acc_LSVM_DS2_bow_3, acc_LSVM_DS2_tfidf_1, acc_LSVM_DS2_tfidf_2, acc_LSVM_DS2_tfidf_3 ]\n",
    "DS2_LSVM_Precision = [ pre_LSVM_DS2_bow_1, pre_LSVM_DS2_bow_2, pre_LSVM_DS2_bow_3, pre_LSVM_DS2_tfidf_1, pre_LSVM_DS2_tfidf_2, pre_LSVM_DS2_tfidf_3 ]\n",
    "DS2_LSVM_Recall    = [ rec_LSVM_DS2_bow_1, rec_LSVM_DS2_bow_2, rec_LSVM_DS2_bow_3, rec_LSVM_DS2_tfidf_1, rec_LSVM_DS2_tfidf_2, rec_LSVM_DS2_tfidf_3 ]\n",
    "DS2_LSVM_F1_score  = [ f1_LSVM_DS2_bow_1,  f1_LSVM_DS2_bow_2,  f1_LSVM_DS2_bow_3,  f1_LSVM_DS2_tfidf_1,  f1_LSVM_DS2_tfidf_2,  f1_LSVM_DS2_tfidf_3  ]\n",
    "DS2_LSVM_time      = [ time_LSVM_DS2_bow_1,time_LSVM_DS2_bow_2,time_LSVM_DS2_bow_3,time_LSVM_DS2_tfidf_1,time_LSVM_DS2_tfidf_2,time_LSVM_DS2_tfidf_3]\n",
    "DS2_LSVM_memory    = [ memory_LSVM_DS2_bow_1, memory_LSVM_DS2_bow_2, memory_LSVM_DS2_bow_3, memory_LSVM_DS2_tfidf_1, memory_LSVM_DS2_tfidf_2, memory_LSVM_DS2_tfidf_3]\n",
    "DS2_LSVM           = [ DS2_LSVM_Accuracy, DS2_LSVM_Precision, DS2_LSVM_Recall, DS2_LSVM_F1_score, DS2_LSVM_time, DS2_LSVM_memory]\n",
    "\n",
    "DS2_MLP_Accuracy  = [ acc_MLP_DS2_bow_1, acc_MLP_DS2_bow_2, acc_MLP_DS2_bow_3, acc_MLP_DS2_tfidf_1, acc_MLP_DS2_tfidf_2, acc_MLP_DS2_tfidf_3 ]\n",
    "DS2_MLP_Precision = [ pre_MLP_DS2_bow_1, pre_MLP_DS2_bow_2, pre_MLP_DS2_bow_3, pre_MLP_DS2_tfidf_1, pre_MLP_DS2_tfidf_2, pre_MLP_DS2_tfidf_3 ]\n",
    "DS2_MLP_Recall    = [ rec_MLP_DS2_bow_1, rec_MLP_DS2_bow_2, rec_MLP_DS2_bow_3, rec_MLP_DS2_tfidf_1, rec_MLP_DS2_tfidf_2, rec_MLP_DS2_tfidf_3 ]\n",
    "DS2_MLP_F1_score  = [ f1_MLP_DS2_bow_1,  f1_MLP_DS2_bow_2,  f1_MLP_DS2_bow_3,  f1_MLP_DS2_tfidf_1,  f1_MLP_DS2_tfidf_2,  f1_MLP_DS2_tfidf_3  ]\n",
    "DS2_MLP_time      = [ time_MLP_DS2_bow_1,time_MLP_DS2_bow_2,time_MLP_DS2_bow_3,time_MLP_DS2_tfidf_1,time_MLP_DS2_tfidf_2,time_MLP_DS2_tfidf_3]\n",
    "DS2_MLP_memory    = [ memory_MLP_DS2_bow_1, memory_MLP_DS2_bow_2, memory_MLP_DS2_bow_3, memory_MLP_DS2_tfidf_1, memory_MLP_DS2_tfidf_2, memory_MLP_DS2_tfidf_3]\n",
    "DS2_MLP           = [ DS2_MLP_Accuracy, DS2_MLP_Precision, DS2_MLP_Recall, DS2_MLP_F1_score, DS2_MLP_time, DS2_MLP_memory]\n",
    "\n",
    "DS2_KNN_Accuracy  = [ acc_KNN_DS2_bow_1, acc_KNN_DS2_bow_2, acc_KNN_DS2_bow_3, acc_KNN_DS2_tfidf_1, acc_KNN_DS2_tfidf_2, acc_KNN_DS2_tfidf_3 ]\n",
    "DS2_KNN_Precision = [ pre_KNN_DS2_bow_1, pre_KNN_DS2_bow_2, pre_KNN_DS2_bow_3, pre_KNN_DS2_tfidf_1, pre_KNN_DS2_tfidf_2, pre_KNN_DS2_tfidf_3 ]\n",
    "DS2_KNN_Recall    = [ rec_KNN_DS2_bow_1, rec_KNN_DS2_bow_2, rec_KNN_DS2_bow_3, rec_KNN_DS2_tfidf_1, rec_KNN_DS2_tfidf_2, rec_KNN_DS2_tfidf_3 ]\n",
    "DS2_KNN_F1_score  = [ f1_KNN_DS2_bow_1,  f1_KNN_DS2_bow_2,  f1_KNN_DS2_bow_3,  f1_KNN_DS2_tfidf_1,  f1_KNN_DS2_tfidf_2,  f1_KNN_DS2_tfidf_3  ]\n",
    "DS2_KNN_time      = [ time_KNN_DS2_bow_1,time_KNN_DS2_bow_2,time_KNN_DS2_bow_3,time_KNN_DS2_tfidf_1,time_KNN_DS2_tfidf_2,time_KNN_DS2_tfidf_3]\n",
    "DS2_KNN_memory    = [ memory_KNN_DS2_bow_1, memory_KNN_DS2_bow_2, memory_KNN_DS2_bow_3, memory_KNN_DS2_tfidf_1, memory_KNN_DS2_tfidf_2, memory_KNN_DS2_tfidf_3]\n",
    "DS2_KNN           = [ DS2_KNN_Accuracy, DS2_KNN_Precision, DS2_KNN_Recall, DS2_KNN_F1_score, DS2_KNN_time, DS2_KNN_memory]\n",
    "\n",
    "DS2_RF_Accuracy  = [ acc_RF_DS2_bow_1, acc_RF_DS2_bow_2, acc_RF_DS2_bow_3, acc_RF_DS2_tfidf_1, acc_RF_DS2_tfidf_2, acc_RF_DS2_tfidf_3 ]\n",
    "DS2_RF_Precision = [ pre_RF_DS2_bow_1, pre_RF_DS2_bow_2, pre_RF_DS2_bow_3, pre_RF_DS2_tfidf_1, pre_RF_DS2_tfidf_2, pre_RF_DS2_tfidf_3 ]\n",
    "DS2_RF_Recall    = [ rec_RF_DS2_bow_1, rec_RF_DS2_bow_2, rec_RF_DS2_bow_3, rec_RF_DS2_tfidf_1, rec_RF_DS2_tfidf_2, rec_RF_DS2_tfidf_3 ]\n",
    "DS2_RF_F1_score  = [ f1_RF_DS2_bow_1,  f1_RF_DS2_bow_2,  f1_RF_DS2_bow_3,  f1_RF_DS2_tfidf_1,  f1_RF_DS2_tfidf_2,  f1_RF_DS2_tfidf_3  ]\n",
    "DS2_RF_time      = [ time_RF_DS2_bow_1,time_RF_DS2_bow_2,time_RF_DS2_bow_3,time_RF_DS2_tfidf_1,time_RF_DS2_tfidf_2,time_RF_DS2_tfidf_3]\n",
    "DS2_RF_memory    = [ memory_RF_DS2_bow_1, memory_RF_DS2_bow_2, memory_RF_DS2_bow_3, memory_RF_DS2_tfidf_1, memory_RF_DS2_tfidf_2, memory_RF_DS2_tfidf_3]\n",
    "DS2_RF           = [ DS2_RF_Accuracy, DS2_RF_Precision, DS2_RF_Recall, DS2_RF_F1_score, DS2_RF_time, DS2_RF_memory]\n",
    "\n",
    "DS2_VOT_1_Accuracy  = [ acc_VOT_1_DS2_bow_1, acc_VOT_1_DS2_bow_2, acc_VOT_1_DS2_bow_3, acc_VOT_1_DS2_tfidf_1, acc_VOT_1_DS2_tfidf_2, acc_VOT_1_DS2_tfidf_3 ]\n",
    "DS2_VOT_1_Precision = [ pre_VOT_1_DS2_bow_1, pre_VOT_1_DS2_bow_2, pre_VOT_1_DS2_bow_3, pre_VOT_1_DS2_tfidf_1, pre_VOT_1_DS2_tfidf_2, pre_VOT_1_DS2_tfidf_3 ]\n",
    "DS2_VOT_1_Recall    = [ rec_VOT_1_DS2_bow_1, rec_VOT_1_DS2_bow_2, rec_VOT_1_DS2_bow_3, rec_VOT_1_DS2_tfidf_1, rec_VOT_1_DS2_tfidf_2, rec_VOT_1_DS2_tfidf_3 ]\n",
    "DS2_VOT_1_F1_score  = [ f1_VOT_1_DS2_bow_1,  f1_VOT_1_DS2_bow_2,  f1_VOT_1_DS2_bow_3,  f1_VOT_1_DS2_tfidf_1,  f1_VOT_1_DS2_tfidf_2,  f1_VOT_1_DS2_tfidf_3  ]\n",
    "DS2_VOT_1_time      = [ time_VOT_1_DS2_bow_1,time_VOT_1_DS2_bow_2,time_VOT_1_DS2_bow_3,time_VOT_1_DS2_tfidf_1,time_VOT_1_DS2_tfidf_2,time_VOT_1_DS2_tfidf_3]\n",
    "DS2_VOT_1_memory    = [ memory_VOT_1_DS2_bow_1, memory_VOT_1_DS2_bow_2, memory_VOT_1_DS2_bow_3, memory_VOT_1_DS2_tfidf_1, memory_VOT_1_DS2_tfidf_2, memory_VOT_1_DS2_tfidf_3]\n",
    "DS2_VOT_1           = [ DS2_VOT_1_Accuracy, DS2_VOT_1_Precision, DS2_VOT_1_Recall, DS2_VOT_1_F1_score, DS2_VOT_1_time, DS2_VOT_1_memory]\n",
    "\n",
    "DS2_VOT_2_Accuracy  = [ acc_VOT_2_DS2_bow_1, acc_VOT_2_DS2_bow_2, acc_VOT_2_DS2_bow_3, acc_VOT_2_DS2_tfidf_1, acc_VOT_2_DS2_tfidf_2, acc_VOT_2_DS2_tfidf_3 ]\n",
    "DS2_VOT_2_Precision = [ pre_VOT_2_DS2_bow_1, pre_VOT_2_DS2_bow_2, pre_VOT_2_DS2_bow_3, pre_VOT_2_DS2_tfidf_1, pre_VOT_2_DS2_tfidf_2, pre_VOT_2_DS2_tfidf_3 ]\n",
    "DS2_VOT_2_Recall    = [ rec_VOT_2_DS2_bow_1, rec_VOT_2_DS2_bow_2, rec_VOT_2_DS2_bow_3, rec_VOT_2_DS2_tfidf_1, rec_VOT_2_DS2_tfidf_2, rec_VOT_2_DS2_tfidf_3 ]\n",
    "DS2_VOT_2_F1_score  = [ f1_VOT_2_DS2_bow_1,  f1_VOT_2_DS2_bow_2,  f1_VOT_2_DS2_bow_3,  f1_VOT_2_DS2_tfidf_1,  f1_VOT_2_DS2_tfidf_2,  f1_VOT_2_DS2_tfidf_3  ]\n",
    "DS2_VOT_2_time      = [ time_VOT_2_DS2_bow_1,time_VOT_2_DS2_bow_2,time_VOT_2_DS2_bow_3,time_VOT_2_DS2_tfidf_1,time_VOT_2_DS2_tfidf_2,time_VOT_2_DS2_tfidf_3]\n",
    "DS2_VOT_2_memory    = [ memory_VOT_2_DS2_bow_1, memory_VOT_2_DS2_bow_2, memory_VOT_2_DS2_bow_3, memory_VOT_2_DS2_tfidf_1, memory_VOT_2_DS2_tfidf_2, memory_VOT_2_DS2_tfidf_3]\n",
    "DS2_VOT_2           = [ DS2_VOT_2_Accuracy, DS2_VOT_2_Precision, DS2_VOT_2_Recall, DS2_VOT_2_F1_score, DS2_VOT_2_time, DS2_VOT_2_memory]\n",
    "\n",
    "DS2_BAGG_Accuracy  = [ acc_BAGG_DS2_bow_1, acc_BAGG_DS2_bow_2, acc_BAGG_DS2_bow_3, acc_BAGG_DS2_tfidf_1, acc_BAGG_DS2_tfidf_2, acc_BAGG_DS2_tfidf_3 ]\n",
    "DS2_BAGG_Precision = [ pre_BAGG_DS2_bow_1, pre_BAGG_DS2_bow_2, pre_BAGG_DS2_bow_3, pre_BAGG_DS2_tfidf_1, pre_BAGG_DS2_tfidf_2, pre_BAGG_DS2_tfidf_3 ]\n",
    "DS2_BAGG_Recall    = [ rec_BAGG_DS2_bow_1, rec_BAGG_DS2_bow_2, rec_BAGG_DS2_bow_3, rec_BAGG_DS2_tfidf_1, rec_BAGG_DS2_tfidf_2, rec_BAGG_DS2_tfidf_3 ]\n",
    "DS2_BAGG_F1_score  = [ f1_BAGG_DS2_bow_1,  f1_BAGG_DS2_bow_2,  f1_BAGG_DS2_bow_3,  f1_BAGG_DS2_tfidf_1,  f1_BAGG_DS2_tfidf_2,  f1_BAGG_DS2_tfidf_3  ]\n",
    "DS2_BAGG_time      = [ time_BAGG_DS2_bow_1,time_BAGG_DS2_bow_2,time_BAGG_DS2_bow_3,time_BAGG_DS2_tfidf_1,time_BAGG_DS2_tfidf_2,time_BAGG_DS2_tfidf_3]\n",
    "DS2_BAGG_memory    = [ memory_BAGG_DS2_bow_1, memory_BAGG_DS2_bow_2, memory_BAGG_DS2_bow_3, memory_BAGG_DS2_tfidf_1, memory_BAGG_DS2_tfidf_2, memory_BAGG_DS2_tfidf_3]\n",
    "DS2_BAGG           = [ DS2_BAGG_Accuracy, DS2_BAGG_Precision, DS2_BAGG_Recall, DS2_BAGG_F1_score, DS2_BAGG_time, DS2_BAGG_memory]\n",
    "\n",
    "DS2_ADA_Accuracy  = [ acc_ADA_DS2_bow_1, acc_ADA_DS2_bow_2, acc_ADA_DS2_bow_3, acc_ADA_DS2_tfidf_1, acc_ADA_DS2_tfidf_2, acc_ADA_DS2_tfidf_3 ]\n",
    "DS2_ADA_Precision = [ pre_ADA_DS2_bow_1, pre_ADA_DS2_bow_2, pre_ADA_DS2_bow_3, pre_ADA_DS2_tfidf_1, pre_ADA_DS2_tfidf_2, pre_ADA_DS2_tfidf_3 ]\n",
    "DS2_ADA_Recall    = [ rec_ADA_DS2_bow_1, rec_ADA_DS2_bow_2, rec_ADA_DS2_bow_3, rec_ADA_DS2_tfidf_1, rec_ADA_DS2_tfidf_2, rec_ADA_DS2_tfidf_3 ]\n",
    "DS2_ADA_F1_score  = [ f1_ADA_DS2_bow_1,  f1_ADA_DS2_bow_2,  f1_ADA_DS2_bow_3,  f1_ADA_DS2_tfidf_1,  f1_ADA_DS2_tfidf_2,  f1_ADA_DS2_tfidf_3  ]\n",
    "DS2_ADA_time      = [ time_ADA_DS2_bow_1,time_ADA_DS2_bow_2,time_ADA_DS2_bow_3,time_ADA_DS2_tfidf_1,time_ADA_DS2_tfidf_2,time_ADA_DS2_tfidf_3]\n",
    "DS2_ADA_memory    = [ memory_ADA_DS2_bow_1, memory_ADA_DS2_bow_2, memory_ADA_DS2_bow_3, memory_ADA_DS2_tfidf_1, memory_ADA_DS2_tfidf_2, memory_ADA_DS2_tfidf_3]\n",
    "DS2_ADA           = [ DS2_ADA_Accuracy, DS2_ADA_Precision, DS2_ADA_Recall, DS2_ADA_F1_score, DS2_ADA_time, DS2_ADA_memory]\n",
    "\n",
    "DS2_XG_Accuracy  = [ acc_XG_DS2_bow_1, acc_XG_DS2_bow_2, acc_XG_DS2_bow_3, acc_XG_DS2_tfidf_1, acc_XG_DS2_tfidf_2, acc_XG_DS2_tfidf_3 ]\n",
    "DS2_XG_Precision = [ pre_XG_DS2_bow_1, pre_XG_DS2_bow_2, pre_XG_DS2_bow_3, pre_XG_DS2_tfidf_1, pre_XG_DS2_tfidf_2, pre_XG_DS2_tfidf_3 ]\n",
    "DS2_XG_Recall    = [ rec_XG_DS2_bow_1, rec_XG_DS2_bow_2, rec_XG_DS2_bow_3, rec_XG_DS2_tfidf_1, rec_XG_DS2_tfidf_2, rec_XG_DS2_tfidf_3 ]\n",
    "DS2_XG_F1_score  = [ f1_XG_DS2_bow_1,  f1_XG_DS2_bow_2,  f1_XG_DS2_bow_3,  f1_XG_DS2_tfidf_1,  f1_XG_DS2_tfidf_2,  f1_XG_DS2_tfidf_3  ]\n",
    "DS2_XG_time      = [ time_XG_DS2_bow_1,time_XG_DS2_bow_2,time_XG_DS2_bow_3,time_XG_DS2_tfidf_1,time_XG_DS2_tfidf_2,time_XG_DS2_tfidf_3]\n",
    "DS2_XG_memory    = [ memory_XG_DS2_bow_1, memory_XG_DS2_bow_2, memory_XG_DS2_bow_3, memory_XG_DS2_tfidf_1, memory_XG_DS2_tfidf_2, memory_XG_DS2_tfidf_3]\n",
    "DS2_XG           = [ DS2_XG_Accuracy, DS2_XG_Precision, DS2_XG_Recall, DS2_XG_F1_score, DS2_XG_time, DS2_XG_memory]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = ['Accuracy','Precision', 'Recall', 'F1-score', 'Run time (s)', 'Memory (MB)']\n",
    "Columns=[' '+' (BoW) (n=1)',' (BoW) (n=2)', ' (BoW) (n=3)',' (TF-IDF) (n=1)',' (TF-IDF) (n=2)', ' (TF-IDF) (n=3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_LR_df    = pd.DataFrame( DS2_LR,    index=Index, columns=[ 'LR '      + w for w in Columns] ).round(3).T\n",
    "DS2_LSVM_df  = pd.DataFrame( DS2_LSVM,  index=Index, columns=[ 'LSVM '    + w for w in Columns] ).round(3).T\n",
    "DS2_MLP_df   = pd.DataFrame( DS2_MLP,   index=Index, columns=[ 'MLP '     + w for w in Columns] ).round(3).T\n",
    "DS2_KNN_df   = pd.DataFrame( DS2_KNN,   index=Index, columns=[ 'KNN '     + w for w in Columns] ).round(3).T\n",
    "DS2_RF_df    = pd.DataFrame( DS2_RF,    index=Index, columns=[ 'RF '      + w for w in Columns] ).round(3).T\n",
    "DS2_VOT_1_df = pd.DataFrame( DS2_VOT_1, index=Index, columns=[ 'Voting 1' + w for w in Columns] ).round(3).T\n",
    "DS2_VOT_2_df = pd.DataFrame( DS2_VOT_2, index=Index, columns=[ 'Voting 2' + w for w in Columns] ).round(3).T\n",
    "DS2_BAGG_df  = pd.DataFrame( DS2_BAGG,  index=Index, columns=[ 'Bagging'     + w for w in Columns] ).round(3).T\n",
    "DS2_ADA_df   = pd.DataFrame( DS2_ADA,   index=Index, columns=[ 'AdaBoost '+ w for w in Columns] ).round(3).T\n",
    "DS2_XG_df    = pd.DataFrame( DS2_XG,    index=Index, columns=[ 'XGBoost ' + w for w in Columns] ).round(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS2_LR_df.to_csv('DS2_LR_df')\n",
    "DS2_LSVM_df.to_csv('DS2_LSVM_df')\n",
    "DS2_MLP_df.to_csv('DS2_MLP_df')\n",
    "DS2_KNN_df.to_csv('DS2_KNN_df')\n",
    "DS2_RF_df.to_csv('DS2_RF_df')\n",
    "DS2_VOT_1_df.to_csv('DS2_VOT_1_df')\n",
    "DS2_VOT_2_df.to_csv('DS2_VOT_2_df')\n",
    "DS2_BAGG_df.to_csv('DS2_BAGG_df')\n",
    "DS2_ADA_df.to_csv('DS2_ADA_df')\n",
    "DS2_XG_df .to_csv('DS2_XG_df ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS3_LR_Accuracy  = [ acc_LR_DS3_bow_1, acc_LR_DS3_bow_2, acc_LR_DS3_bow_3, acc_LR_DS3_tfidf_1, acc_LR_DS3_tfidf_2, acc_LR_DS3_tfidf_3 ]\n",
    "DS3_LR_Precision = [ pre_LR_DS3_bow_1, pre_LR_DS3_bow_2, pre_LR_DS3_bow_3, pre_LR_DS3_tfidf_1, pre_LR_DS3_tfidf_2, pre_LR_DS3_tfidf_3 ]\n",
    "DS3_LR_Recall    = [ rec_LR_DS3_bow_1, rec_LR_DS3_bow_2, rec_LR_DS3_bow_3, rec_LR_DS3_tfidf_1, rec_LR_DS3_tfidf_2, rec_LR_DS3_tfidf_3 ]\n",
    "DS3_LR_F1_score  = [ f1_LR_DS3_bow_1,  f1_LR_DS3_bow_2,  f1_LR_DS3_bow_3,  f1_LR_DS3_tfidf_1,  f1_LR_DS3_tfidf_2,  f1_LR_DS3_tfidf_3  ]\n",
    "DS3_LR_time      = [ time_LR_DS3_bow_1,time_LR_DS3_bow_2,time_LR_DS3_bow_3,time_LR_DS3_tfidf_1,time_LR_DS3_tfidf_2,time_LR_DS3_tfidf_3]\n",
    "DS3_LR_memory    = [ memory_LR_DS3_bow_1, memory_LR_DS3_bow_2, memory_LR_DS3_bow_3, memory_LR_DS3_tfidf_1, memory_LR_DS3_tfidf_2, memory_LR_DS3_tfidf_3]\n",
    "DS3_LR           = [ DS3_LR_Accuracy, DS3_LR_Precision, DS3_LR_Recall, DS3_LR_F1_score, DS3_LR_time, DS3_LR_memory]\n",
    "\n",
    "DS3_LSVM_Accuracy  = [ acc_LSVM_DS3_bow_1, acc_LSVM_DS3_bow_2, acc_LSVM_DS3_bow_3, acc_LSVM_DS3_tfidf_1, acc_LSVM_DS3_tfidf_2, acc_LSVM_DS3_tfidf_3 ]\n",
    "DS3_LSVM_Precision = [ pre_LSVM_DS3_bow_1, pre_LSVM_DS3_bow_2, pre_LSVM_DS3_bow_3, pre_LSVM_DS3_tfidf_1, pre_LSVM_DS3_tfidf_2, pre_LSVM_DS3_tfidf_3 ]\n",
    "DS3_LSVM_Recall    = [ rec_LSVM_DS3_bow_1, rec_LSVM_DS3_bow_2, rec_LSVM_DS3_bow_3, rec_LSVM_DS3_tfidf_1, rec_LSVM_DS3_tfidf_2, rec_LSVM_DS3_tfidf_3 ]\n",
    "DS3_LSVM_F1_score  = [ f1_LSVM_DS3_bow_1,  f1_LSVM_DS3_bow_2,  f1_LSVM_DS3_bow_3,  f1_LSVM_DS3_tfidf_1,  f1_LSVM_DS3_tfidf_2,  f1_LSVM_DS3_tfidf_3  ]\n",
    "DS3_LSVM_time      = [ time_LSVM_DS3_bow_1,time_LSVM_DS3_bow_2,time_LSVM_DS3_bow_3,time_LSVM_DS3_tfidf_1,time_LSVM_DS3_tfidf_2,time_LSVM_DS3_tfidf_3]\n",
    "DS3_LSVM_memory    = [ memory_LSVM_DS3_bow_1, memory_LSVM_DS3_bow_2, memory_LSVM_DS3_bow_3, memory_LSVM_DS3_tfidf_1, memory_LSVM_DS3_tfidf_2, memory_LSVM_DS3_tfidf_3]\n",
    "DS3_LSVM           = [ DS3_LSVM_Accuracy, DS3_LSVM_Precision, DS3_LSVM_Recall, DS3_LSVM_F1_score, DS3_LSVM_time, DS3_LSVM_memory]\n",
    "\n",
    "DS3_MLP_Accuracy  = [ acc_MLP_DS3_bow_1, acc_MLP_DS3_bow_2, acc_MLP_DS3_bow_3, acc_MLP_DS3_tfidf_1, acc_MLP_DS3_tfidf_2, acc_MLP_DS3_tfidf_3 ]\n",
    "DS3_MLP_Precision = [ pre_MLP_DS3_bow_1, pre_MLP_DS3_bow_2, pre_MLP_DS3_bow_3, pre_MLP_DS3_tfidf_1, pre_MLP_DS3_tfidf_2, pre_MLP_DS3_tfidf_3 ]\n",
    "DS3_MLP_Recall    = [ rec_MLP_DS3_bow_1, rec_MLP_DS3_bow_2, rec_MLP_DS3_bow_3, rec_MLP_DS3_tfidf_1, rec_MLP_DS3_tfidf_2, rec_MLP_DS3_tfidf_3 ]\n",
    "DS3_MLP_F1_score  = [ f1_MLP_DS3_bow_1,  f1_MLP_DS3_bow_2,  f1_MLP_DS3_bow_3,  f1_MLP_DS3_tfidf_1,  f1_MLP_DS3_tfidf_2,  f1_MLP_DS3_tfidf_3  ]\n",
    "DS3_MLP_time      = [ time_MLP_DS3_bow_1,time_MLP_DS3_bow_2,time_MLP_DS3_bow_3,time_MLP_DS3_tfidf_1,time_MLP_DS3_tfidf_2,time_MLP_DS3_tfidf_3]\n",
    "DS3_MLP_memory    = [ memory_MLP_DS3_bow_1, memory_MLP_DS3_bow_2, memory_MLP_DS3_bow_3, memory_MLP_DS3_tfidf_1, memory_MLP_DS3_tfidf_2, memory_MLP_DS3_tfidf_3]\n",
    "DS3_MLP           = [ DS3_MLP_Accuracy, DS3_MLP_Precision, DS3_MLP_Recall, DS3_MLP_F1_score, DS3_MLP_time, DS3_MLP_memory]\n",
    "\n",
    "DS3_KNN_Accuracy  = [ acc_KNN_DS3_bow_1, acc_KNN_DS3_bow_2, acc_KNN_DS3_bow_3, acc_KNN_DS3_tfidf_1, acc_KNN_DS3_tfidf_2, acc_KNN_DS3_tfidf_3 ]\n",
    "DS3_KNN_Precision = [ pre_KNN_DS3_bow_1, pre_KNN_DS3_bow_2, pre_KNN_DS3_bow_3, pre_KNN_DS3_tfidf_1, pre_KNN_DS3_tfidf_2, pre_KNN_DS3_tfidf_3 ]\n",
    "DS3_KNN_Recall    = [ rec_KNN_DS3_bow_1, rec_KNN_DS3_bow_2, rec_KNN_DS3_bow_3, rec_KNN_DS3_tfidf_1, rec_KNN_DS3_tfidf_2, rec_KNN_DS3_tfidf_3 ]\n",
    "DS3_KNN_F1_score  = [ f1_KNN_DS3_bow_1,  f1_KNN_DS3_bow_2,  f1_KNN_DS3_bow_3,  f1_KNN_DS3_tfidf_1,  f1_KNN_DS3_tfidf_2,  f1_KNN_DS3_tfidf_3  ]\n",
    "DS3_KNN_time      = [ time_KNN_DS3_bow_1,time_KNN_DS3_bow_2,time_KNN_DS3_bow_3,time_KNN_DS3_tfidf_1,time_KNN_DS3_tfidf_2,time_KNN_DS3_tfidf_3]\n",
    "DS3_KNN_memory    = [ memory_KNN_DS3_bow_1, memory_KNN_DS3_bow_2, memory_KNN_DS3_bow_3, memory_KNN_DS3_tfidf_1, memory_KNN_DS3_tfidf_2, memory_KNN_DS3_tfidf_3]\n",
    "DS3_KNN           = [ DS3_KNN_Accuracy, DS3_KNN_Precision, DS3_KNN_Recall, DS3_KNN_F1_score, DS3_KNN_time, DS3_KNN_memory]\n",
    "\n",
    "DS3_RF_Accuracy  = [ acc_RF_DS3_bow_1, acc_RF_DS3_bow_2, acc_RF_DS3_bow_3, acc_RF_DS3_tfidf_1, acc_RF_DS3_tfidf_2, acc_RF_DS3_tfidf_3 ]\n",
    "DS3_RF_Precision = [ pre_RF_DS3_bow_1, pre_RF_DS3_bow_2, pre_RF_DS3_bow_3, pre_RF_DS3_tfidf_1, pre_RF_DS3_tfidf_2, pre_RF_DS3_tfidf_3 ]\n",
    "DS3_RF_Recall    = [ rec_RF_DS3_bow_1, rec_RF_DS3_bow_2, rec_RF_DS3_bow_3, rec_RF_DS3_tfidf_1, rec_RF_DS3_tfidf_2, rec_RF_DS3_tfidf_3 ]\n",
    "DS3_RF_F1_score  = [ f1_RF_DS3_bow_1,  f1_RF_DS3_bow_2,  f1_RF_DS3_bow_3,  f1_RF_DS3_tfidf_1,  f1_RF_DS3_tfidf_2,  f1_RF_DS3_tfidf_3  ]\n",
    "DS3_RF_time      = [ time_RF_DS3_bow_1,time_RF_DS3_bow_2,time_RF_DS3_bow_3,time_RF_DS3_tfidf_1,time_RF_DS3_tfidf_2,time_RF_DS3_tfidf_3]\n",
    "DS3_RF_memory    = [ memory_RF_DS3_bow_1, memory_RF_DS3_bow_2, memory_RF_DS3_bow_3, memory_RF_DS3_tfidf_1, memory_RF_DS3_tfidf_2, memory_RF_DS3_tfidf_3]\n",
    "DS3_RF           = [ DS3_RF_Accuracy, DS3_RF_Precision, DS3_RF_Recall, DS3_RF_F1_score, DS3_RF_time, DS3_RF_memory]\n",
    "\n",
    "DS3_VOT_1_Accuracy  = [ acc_VOT_1_DS3_bow_1, acc_VOT_1_DS3_bow_2, acc_VOT_1_DS3_bow_3, acc_VOT_1_DS3_tfidf_1, acc_VOT_1_DS3_tfidf_2, acc_VOT_1_DS3_tfidf_3 ]\n",
    "DS3_VOT_1_Precision = [ pre_VOT_1_DS3_bow_1, pre_VOT_1_DS3_bow_2, pre_VOT_1_DS3_bow_3, pre_VOT_1_DS3_tfidf_1, pre_VOT_1_DS3_tfidf_2, pre_VOT_1_DS3_tfidf_3 ]\n",
    "DS3_VOT_1_Recall    = [ rec_VOT_1_DS3_bow_1, rec_VOT_1_DS3_bow_2, rec_VOT_1_DS3_bow_3, rec_VOT_1_DS3_tfidf_1, rec_VOT_1_DS3_tfidf_2, rec_VOT_1_DS3_tfidf_3 ]\n",
    "DS3_VOT_1_F1_score  = [ f1_VOT_1_DS3_bow_1,  f1_VOT_1_DS3_bow_2,  f1_VOT_1_DS3_bow_3,  f1_VOT_1_DS3_tfidf_1,  f1_VOT_1_DS3_tfidf_2,  f1_VOT_1_DS3_tfidf_3  ]\n",
    "DS3_VOT_1_time      = [ time_VOT_1_DS3_bow_1,time_VOT_1_DS3_bow_2,time_VOT_1_DS3_bow_3,time_VOT_1_DS3_tfidf_1,time_VOT_1_DS3_tfidf_2,time_VOT_1_DS3_tfidf_3]\n",
    "DS3_VOT_1_memory    = [ memory_VOT_1_DS3_bow_1, memory_VOT_1_DS3_bow_2, memory_VOT_1_DS3_bow_3, memory_VOT_1_DS3_tfidf_1, memory_VOT_1_DS3_tfidf_2, memory_VOT_1_DS3_tfidf_3]\n",
    "DS3_VOT_1           = [ DS3_VOT_1_Accuracy, DS3_VOT_1_Precision, DS3_VOT_1_Recall, DS3_VOT_1_F1_score, DS3_VOT_1_time, DS3_VOT_1_memory]\n",
    "\n",
    "DS3_VOT_2_Accuracy  = [ acc_VOT_2_DS3_bow_1, acc_VOT_2_DS3_bow_2, acc_VOT_2_DS3_bow_3, acc_VOT_2_DS3_tfidf_1, acc_VOT_2_DS3_tfidf_2, acc_VOT_2_DS3_tfidf_3 ]\n",
    "DS3_VOT_2_Precision = [ pre_VOT_2_DS3_bow_1, pre_VOT_2_DS3_bow_2, pre_VOT_2_DS3_bow_3, pre_VOT_2_DS3_tfidf_1, pre_VOT_2_DS3_tfidf_2, pre_VOT_2_DS3_tfidf_3 ]\n",
    "DS3_VOT_2_Recall    = [ rec_VOT_2_DS3_bow_1, rec_VOT_2_DS3_bow_2, rec_VOT_2_DS3_bow_3, rec_VOT_2_DS3_tfidf_1, rec_VOT_2_DS3_tfidf_2, rec_VOT_2_DS3_tfidf_3 ]\n",
    "DS3_VOT_2_F1_score  = [ f1_VOT_2_DS3_bow_1,  f1_VOT_2_DS3_bow_2,  f1_VOT_2_DS3_bow_3,  f1_VOT_2_DS3_tfidf_1,  f1_VOT_2_DS3_tfidf_2,  f1_VOT_2_DS3_tfidf_3  ]\n",
    "DS3_VOT_2_time      = [ time_VOT_2_DS3_bow_1,time_VOT_2_DS3_bow_2,time_VOT_2_DS3_bow_3,time_VOT_2_DS3_tfidf_1,time_VOT_2_DS3_tfidf_2,time_VOT_2_DS3_tfidf_3]\n",
    "DS3_VOT_2_memory    = [ memory_VOT_2_DS3_bow_1, memory_VOT_2_DS3_bow_2, memory_VOT_2_DS3_bow_3, memory_VOT_2_DS3_tfidf_1, memory_VOT_2_DS3_tfidf_2, memory_VOT_2_DS3_tfidf_3]\n",
    "DS3_VOT_2           = [ DS3_VOT_2_Accuracy, DS3_VOT_2_Precision, DS3_VOT_2_Recall, DS3_VOT_2_F1_score, DS3_VOT_2_time, DS3_VOT_2_memory]\n",
    "\n",
    "DS3_BAGG_Accuracy  = [ acc_BAGG_DS3_bow_1, acc_BAGG_DS3_bow_2, acc_BAGG_DS3_bow_3, acc_BAGG_DS3_tfidf_1, acc_BAGG_DS3_tfidf_2, acc_BAGG_DS3_tfidf_3 ]\n",
    "DS3_BAGG_Precision = [ pre_BAGG_DS3_bow_1, pre_BAGG_DS3_bow_2, pre_BAGG_DS3_bow_3, pre_BAGG_DS3_tfidf_1, pre_BAGG_DS3_tfidf_2, pre_BAGG_DS3_tfidf_3 ]\n",
    "DS3_BAGG_Recall    = [ rec_BAGG_DS3_bow_1, rec_BAGG_DS3_bow_2, rec_BAGG_DS3_bow_3, rec_BAGG_DS3_tfidf_1, rec_BAGG_DS3_tfidf_2, rec_BAGG_DS3_tfidf_3 ]\n",
    "DS3_BAGG_F1_score  = [ f1_BAGG_DS3_bow_1,  f1_BAGG_DS3_bow_2,  f1_BAGG_DS3_bow_3,  f1_BAGG_DS3_tfidf_1,  f1_BAGG_DS3_tfidf_2,  f1_BAGG_DS3_tfidf_3  ]\n",
    "DS3_BAGG_time      = [ time_BAGG_DS3_bow_1,time_BAGG_DS3_bow_2,time_BAGG_DS3_bow_3,time_BAGG_DS3_tfidf_1,time_BAGG_DS3_tfidf_2,time_BAGG_DS3_tfidf_3]\n",
    "DS3_BAGG_memory    = [ memory_BAGG_DS3_bow_1, memory_BAGG_DS3_bow_2, memory_BAGG_DS3_bow_3, memory_BAGG_DS3_tfidf_1, memory_BAGG_DS3_tfidf_2, memory_BAGG_DS3_tfidf_3]\n",
    "DS3_BAGG           = [ DS3_BAGG_Accuracy, DS3_BAGG_Precision, DS3_BAGG_Recall, DS3_BAGG_F1_score, DS3_BAGG_time, DS3_BAGG_memory]\n",
    "\n",
    "DS3_ADA_Accuracy  = [ acc_ADA_DS3_bow_1, acc_ADA_DS3_bow_2, acc_ADA_DS3_bow_3, acc_ADA_DS3_tfidf_1, acc_ADA_DS3_tfidf_2, acc_ADA_DS3_tfidf_3 ]\n",
    "DS3_ADA_Precision = [ pre_ADA_DS3_bow_1, pre_ADA_DS3_bow_2, pre_ADA_DS3_bow_3, pre_ADA_DS3_tfidf_1, pre_ADA_DS3_tfidf_2, pre_ADA_DS3_tfidf_3 ]\n",
    "DS3_ADA_Recall    = [ rec_ADA_DS3_bow_1, rec_ADA_DS3_bow_2, rec_ADA_DS3_bow_3, rec_ADA_DS3_tfidf_1, rec_ADA_DS3_tfidf_2, rec_ADA_DS3_tfidf_3 ]\n",
    "DS3_ADA_F1_score  = [ f1_ADA_DS3_bow_1,  f1_ADA_DS3_bow_2,  f1_ADA_DS3_bow_3,  f1_ADA_DS3_tfidf_1,  f1_ADA_DS3_tfidf_2,  f1_ADA_DS3_tfidf_3  ]\n",
    "DS3_ADA_time      = [ time_ADA_DS3_bow_1,time_ADA_DS3_bow_2,time_ADA_DS3_bow_3,time_ADA_DS3_tfidf_1,time_ADA_DS3_tfidf_2,time_ADA_DS3_tfidf_3]\n",
    "DS3_ADA_memory    = [ memory_ADA_DS3_bow_1, memory_ADA_DS3_bow_2, memory_ADA_DS3_bow_3, memory_ADA_DS3_tfidf_1, memory_ADA_DS3_tfidf_2, memory_ADA_DS3_tfidf_3]\n",
    "DS3_ADA           = [ DS3_ADA_Accuracy, DS3_ADA_Precision, DS3_ADA_Recall, DS3_ADA_F1_score, DS3_ADA_time, DS3_ADA_memory]\n",
    "\n",
    "DS3_XG_Accuracy  = [ acc_XG_DS3_bow_1, acc_XG_DS3_bow_2, acc_XG_DS3_bow_3, acc_XG_DS3_tfidf_1, acc_XG_DS3_tfidf_2, acc_XG_DS3_tfidf_3 ]\n",
    "DS3_XG_Precision = [ pre_XG_DS3_bow_1, pre_XG_DS3_bow_2, pre_XG_DS3_bow_3, pre_XG_DS3_tfidf_1, pre_XG_DS3_tfidf_2, pre_XG_DS3_tfidf_3 ]\n",
    "DS3_XG_Recall    = [ rec_XG_DS3_bow_1, rec_XG_DS3_bow_2, rec_XG_DS3_bow_3, rec_XG_DS3_tfidf_1, rec_XG_DS3_tfidf_2, rec_XG_DS3_tfidf_3 ]\n",
    "DS3_XG_F1_score  = [ f1_XG_DS3_bow_1,  f1_XG_DS3_bow_2,  f1_XG_DS3_bow_3,  f1_XG_DS3_tfidf_1,  f1_XG_DS3_tfidf_2,  f1_XG_DS3_tfidf_3  ]\n",
    "DS3_XG_time      = [ time_XG_DS3_bow_1,time_XG_DS3_bow_2,time_XG_DS3_bow_3,time_XG_DS3_tfidf_1,time_XG_DS3_tfidf_2,time_XG_DS3_tfidf_3]\n",
    "DS3_XG_memory    = [ memory_XG_DS3_bow_1, memory_XG_DS3_bow_2, memory_XG_DS3_bow_3, memory_XG_DS3_tfidf_1, memory_XG_DS3_tfidf_2, memory_XG_DS3_tfidf_3]\n",
    "DS3_XG           = [ DS3_XG_Accuracy, DS3_XG_Precision, DS3_XG_Recall, DS3_XG_F1_score, DS3_XG_time, DS3_XG_memory]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Index = ['Accuracy','Precision', 'Recall', 'F1-score', 'Run time (s)', 'Memory (MB)']\n",
    "Columns=[' '+' (BoW) (n=1)',' (BoW) (n=2)', ' (BoW) (n=3)',' (TF-IDF) (n=1)',' (TF-IDF) (n=2)', ' (TF-IDF) (n=3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS3_LR_df    = pd.DataFrame( DS3_LR,    index=Index, columns=[ 'LR '      + w for w in Columns] ).round(2).T\n",
    "DS3_LSVM_df  = pd.DataFrame( DS3_LSVM,  index=Index, columns=[ 'LSVM '    + w for w in Columns] ).round(2).T\n",
    "DS3_MLP_df   = pd.DataFrame( DS3_MLP,   index=Index, columns=[ 'MLP '     + w for w in Columns] ).round(2).T\n",
    "DS3_KNN_df   = pd.DataFrame( DS3_KNN,   index=Index, columns=[ 'KNN '     + w for w in Columns] ).round(2).T\n",
    "DS3_RF_df    = pd.DataFrame( DS3_RF,    index=Index, columns=[ 'RF '      + w for w in Columns] ).round(2).T\n",
    "DS3_VOT_1_df = pd.DataFrame( DS3_VOT_1, index=Index, columns=[ 'Voting 1' + w for w in Columns] ).round(2).T\n",
    "DS3_VOT_2_df = pd.DataFrame( DS3_VOT_2, index=Index, columns=[ 'Voting 2' + w for w in Columns] ).round(2).T\n",
    "DS3_BAGG_df  = pd.DataFrame( DS3_BAGG,  index=Index, columns=[ 'Bagging'     + w for w in Columns] ).round(2).T\n",
    "DS3_ADA_df   = pd.DataFrame( DS3_ADA,   index=Index, columns=[ 'AdaBoost '+ w for w in Columns] ).round(2).T\n",
    "DS3_XG_df    = pd.DataFrame( DS3_XG,    index=Index, columns=[ 'XGBoost ' + w for w in Columns] ).round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS3_LR_df.to_csv('DS3_LR_df')\n",
    "DS3_LSVM_df.to_csv('DS3_LSVM_df')\n",
    "DS3_MLP_df.to_csv('DS3_MLP_df')\n",
    "DS3_KNN_df.to_csv('DS3_KNN_df')\n",
    "DS3_RF_df.to_csv('DS3_RF_df')\n",
    "DS3_VOT_1_df.to_csv('DS3_VOT_1_df')\n",
    "DS3_VOT_2_df.to_csv('DS3_VOT_2_df')\n",
    "DS3_BAGG_df.to_csv('DS3_BAGG_df')\n",
    "DS3_ADA_df.to_csv('DS3_ADA_df')\n",
    "DS3_XG_df .to_csv('DS3_XG_df ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
